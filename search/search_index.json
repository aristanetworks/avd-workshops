{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Arista CI Workshops","text":"<p>The Arista CI Workshops are intended for engineers looking to learn the fundamentals of automation tools and get hands-on experience deploying network-wide configurations with Arista Validated Designs (AVD). The workshops are split into two in-person sessions, allowing time to grasp the basic automation concepts before moving into building Data Models to deploy AVD. The content on this site is an overview of the topics we will cover in person with full details and examples.</p> <p>Note</p> <p>The workshops are meant to be leveraged within an Arista Test Drive (ATD) lab. You may follow along using a personal environment; additional setup may apply.</p> <ul> <li>Workshop #1 - Automation Fundamentals 101</li> <li>Workshop #2 (L2LS) - Arista CI / AVD - L2LS</li> <li>Workshop #2 (L3LS) - Arista CI / AVD - L3LS EVPN/VXLAN</li> <li>Workshop #3 - Arista CI / AVD with CI/CD</li> <li>Workshop #4 - Automated Network Validation</li> </ul>"},{"location":"#fundamentals","title":"Fundamentals","text":"<ul> <li>Git</li> <li>VS Code</li> <li>Jinja/YAML</li> <li>Ansible</li> </ul>"},{"location":"#arista-ci","title":"Arista CI","text":"<ul> <li>AVD L2LS<ul> <li>Overview</li> <li>Lab Guide</li> </ul> </li> <li>AVD L3LS EVPN/VXLAN<ul> <li>Overview</li> <li>Lab Guide</li> </ul> </li> <li>CI/CD Basics</li> <li>Automated Network Validation<ul> <li>Overview</li> <li>ANTA Lab Guide</li> <li>AVD Lab Guide</li> </ul> </li> </ul>"},{"location":"ansible/","title":"Welcome to Ansible","text":""},{"location":"ansible/#what-is-ansible","title":"What is Ansible","text":"<p>Ansible is a Python-based automation framework. Today, the term \"Ansible Automation Platform\" can refer to multiple applications, including:</p> <ul> <li>Ansible Core</li> <li>Ansible Galaxy</li> <li>Ansible Automation Controller (Previously known as Tower)</li> <li>Red Hat Insights</li> </ul> <p>We will be focusing on the first two items during this workshop.</p>"},{"location":"ansible/#why-use-ansible","title":"Why use Ansible","text":"<p>Aside from being \"agentless\", meaning that Ansible does not require any specialized software on the target hosts, Ansible is also straightforward to get started with. While prior coding or experience in automation is helpful, it is optional to get up and running with Ansible. Playbooks are written in YAML, a language that we'll cover in detail in the YAML section. For now, rest assured that YAML is a human-readable language, which is why it's accessible to start our Ansible journey.</p> <p>There is an extensive and very active user and development community with Ansible. The project itself is open source, with the GitHub repository available here. The popularity of Ansible has led to broad vendor support, spanning multiple technology silos. Network, Compute, Storage, Cloud, Security, and more can all be automated via Ansible.</p> <p>Finally, all that is required to get started is a Linux host with Python installed. A single Ansible Control Node (ACN) can manage hundreds or thousands of endpoints.</p> Important Note Before Getting Started <p>This section will make use of the fork of the Workshops GitHub repository that was created during the Git section. If you have not made a fork of this repository and cloned it into the <code>/home/coder/project/labfiles/</code> directory of your lab environment's VS Code IDE, please do so before moving forward.</p>"},{"location":"ansible/#ansible-initial-setup","title":"Ansible Initial Setup","text":"<p>There are multiple methods of installing Ansible on the Ansible Control Node. The most popular way is to leverage <code>pip</code>, and is covered in detail here.</p> <p>We will be using <code>ansible-core</code>, which is a lightweight minimalist installation of Ansible which does not include extra modules, plugins, etc. With this approach, we can use Ansible Galaxy (covered later in this section) to install collections containing the modules, plugins, and roles we need. For those familiar with Python, think of Ansible Galaxy as pypi.org, and Ansible Collections as Python modules.</p> Note <p>Ansible is already installed in the Arista Test Drive lab topology, so we won't need to perform any installation-related tasks.</p> <p>Below is an example of installing <code>ansible-core</code> via pip on Ubuntu 20.04:</p> <pre><code>#If necessary, install Python3\nsudo apt install update &amp;&amp; upgrade\nsudo apt install python3 python3-pip --yes\n\n# Install Ansible Core\npip3 install ansible-core\n</code></pre> <p>It is that easy to get started!</p> <p>Before running any commands, let's first ensure that we're in the <code>/home/coder/project/labfiles/ci-workshops-fundamentals/ansible</code> directory.</p> <pre><code>cd ~/project/labfiles/ci-workshops-fundamentals/ansible\n</code></pre> <p>Next, we'll confirm that Ansible is installed by running the <code>ansible --version</code> in the terminal. This should yield output similar to below:</p> <pre><code>ansible [core 2.12.10]\n  config file = /home/coder/project/labfiles/ci-workshops-fundamentals/ansible/ansible.cfg\n  configured module search path = ['/home/coder/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /home/coder/.local/lib/python3.9/site-packages/ansible\n  ansible collection location = /home/coder/.ansible/collections\n  executable location = /home/coder/.local/bin/ansible\n  python version = 3.9.2 (default, Feb 28 2021, 17:03:44) [GCC 10.2.1 20210110]\n  jinja version = 3.1.2\n  libyaml = True\n</code></pre> <p>All of the information displayed above is important. It can help troubleshoot why something may not work as expected when working with Ansible.</p>"},{"location":"ansible/#control-node-and-managed-nodes","title":"Control Node and Managed Nodes","text":"<p>Two important initial terms are the Ansible <code>Control Node</code> and <code>Managed Node</code>.</p> <p>The Control Node is where our playbooks are executed from. This Node then connects to the Managed Nodes to interact with them to perform the desired tasks. How the Control Node interacts with the Managed Node depends on the type of operating system running on the Managed Node. For example, suppose the Managed Node is a Linux server. In that case, the Control Node will \"ship\" the Python code associated with the tasks to the Managed Node. Then, the Managed Node will locally execute that code to complete the tasks.</p> What about the lab environment? <p>In our ATD lab environment, the <code>Control Node</code> is our JumpHost from which we're running our VS Code IDE. The <code>Managed Nodes</code> are switches that make up our lab's network topology.</p> <p>Suppose the Managed Node is a network device. In that case, the Control Node will locally execute the Python code associated with the tasks and then interact with the network devices via SSH or API to complete the tasks.</p> <p>The Control Node must be a Linux host (Ubuntu, CentOS, Rocky, Debian, etc.) with Ansible installed. That's it! Really! This is part of what makes Ansible easy and efficient to get started with. It does not require a software suite to be installed to get started. A single Control Node can manage hundreds or thousands of Managed Nodes.</p> <p>A Managed Node does not need any specialized software installed. In other words, Ansible is <code>agentless</code>. If a Managed Node is a Linux server, it must have Python3 installed. However, no prerequisites are required for Managed Nodes that are network devices, not even Python. This is because the Control Node will locally execute the Python code necessary to complete the tasks on the network device and will then interact as needed with the network device via SSH/API.</p>"},{"location":"ansible/#ansible-components","title":"Ansible Components","text":"<p>Like any other framework, Ansible is made up of a group of components that come together to make the magic happen. The components we'll focus on during this workshop are shown below.</p> <p></p> <p>In the next sections, we'll review these components individually to understand of how each piece fits together.</p>"},{"location":"ansible/#config-file","title":"Config File","text":"<p>The Ansible configuration file is where we set environment variables for our Ansible projects. Many variables can be set in this file, and the most common ones are documented here.</p> <p>When running an ad-hoc command, or playbook, Ansible will look for the configuration file in the locations listed below. These locations are defined in order of precedence:</p> <ol> <li> <p>ANSIBLE_CONFIG (environment variable if set)</p> </li> <li> <p>ansible.cfg (in the current directory)</p> </li> <li> <p>~/.ansible.cfg (in the home directory)</p> </li> <li> <p>/etc/ansible/ansible.cfg</p> </li> </ol> <p>This is illustrated in the image below:</p> <p></p> <p>Once Ansible finds an <code>ansible.cfg</code> file, it will only use the configuration options defined in that file. If, for example, an <code>ansible.cfg</code> file exists in the current directory and in <code>/etc/ansible/ansible.cfg</code>, then only the settings found in the <code>ansible.cfg</code> file in the current directory will be used.</p> Cows? <p>Yes! First, install cowsay: <code>sudo apt-get update &amp;&amp; sudo apt-get install cowsay -y</code> Next, in the <code>ansible.cfg</code> file, set it to what should be the only acceptable setting, <code>nocows = False</code>. This feature is udderly ridiculous, and as much as I'd love to milk it for all the Dad jokes possible, I don't want to start any beef by delaying the workshop...Moo.</p> <p>Below is an example of the <code>ansible.cfg</code> located in our fork of the Workshops repo:</p> Example Ansible Configuration File (~/project/labfiles/ci-workshops-fundamentals/ansible/ansible.cfg) <pre><code>[defaults]\n\n# Disable host key checking by the underlying tools Ansible uses to connect to target hosts\nhost_key_checking = False\n\n# Location of inventory file containing target hosts\ninventory = ./inventory/inventory.yml\n\n# Only gather Ansible facts if explicity directed to in a given play\ngathering = explicit\n\n# Disable the creation of .retry files if a playbook fails\nretry_files_enabled = False\n\n# Path(s) to search for installed Ansible Galaxy Collections\ncollections_paths = ~/.ansible/collections\n\n# Enable additional Jinja2 Extensions (https://jinja.palletsprojects.com/en/3.1.x/extensions/)\njinja2_extensions =  jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n\n\n# Enable the YAML callback plugin, providing much easier to read terminal output. (https://docs.ansible.com/ansible/latest/plugins/callback.html#callback-plugins)\nstdout_callback = yaml\n\n# Permit the use of callback plugins when running ad-hoc commands\nbin_ansible_callbacks = True\n\n# List of enabled callbacks. Many callbacks shipped with Ansible are not enabled by default\ncallback_whitelist = profile_roles, profile_tasks, timer\n\n# Maximum number of forks that Ansible will use to execute tasks on target hosts\nforks = 15\n\n# Disable cowsay (Why?)\nnocows = True\n\n[paramiko_connection]\n# Automatically add the keys of target hosts to known hosts\nhost_key_auto_add = True\n\n[persistent_connection]\n# Set the amount of time, in seconds, to wait for response from remote device before timing out persistent connection.\ncommand_timeout = 60\n\n# Set the amount of time, in seconds, that a persistent connection will remain idle before it is destroyed.\nconnect_timeout = 60\n</code></pre> <p>One of the most common settings in the ansible.cfg file is the location of the <code>inventory</code> file, which we will discuss next.</p>"},{"location":"ansible/#inventory","title":"Inventory","text":"<p>In the inventory file, we define the hosts and groups we'll target with our playbooks. The inventory file supports many formats, but the most common are <code>ini</code> and <code>yaml</code>. For our workshop, we'll be using the <code>yaml</code> format.</p> <p>An example of our inventory file can be seen below:</p> Example Inventory File (~project/labfiles/ci-workshops-fundamentals/ansible/inventory/inventory.yml) <pre><code>WORKSHOP_FABRIC:\n  children:\n    S1:\n      children:\n        S1_SPINES:\n          hosts:\n            s1-spine1:\n            s1-spine2:\n        S1_LEAFS:\n          hosts:\n            s1-leaf1:\n            s1-leaf2:\n            s1-leaf3:\n            s1-leaf4:\n            s1-brdr1:\n            s1-brdr2:\n            s1-core1:\n            s1-core2:\n    S2:\n      children:\n        S2_SPINES:\n          hosts:\n            s2-spine1:\n            s2-spine2:\n        S2_LEAFS:\n          hosts:\n            s2-leaf1:\n            s2-leaf2:\n            s2-leaf3:\n            s2-leaf4:\n            s2-brdr1:\n            s2-brdr2:\n            s2-core1:\n            s2-core2:\n</code></pre> <p>This inventory file defines the hosts and groupings, represented in the image below:</p> <p></p> <p>We can validate our inventory by using the <code>ansible-inventory</code> command, which is documented here.</p> <p>The below command will list the entire inventory, consisting of all hosts/groups and their respective variable values.</p> <p>Reminder</p> <p>Ensure all commands are run from the <code>/home/coder/project/labfiles/ci-workshops-fundamentals/ansible</code> directory in the terminal on the ATD VS Code IDE instance.</p> <pre><code>ansible-inventory --list --yaml\n</code></pre> <p>If we'd like to get more specific, we can filter the output down to a single host by using the command shown below:</p> <pre><code>ansible-inventory --host s1-leaf1 --yaml\n</code></pre> Example inventory output for s1-leaf1 <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: arista1c7z\nansible_user: arista\nbanner_text: This banner came from host_vars/s1-leaf1.YML\nmlag:\n  enabled: true\n  peer_link_int_1: 1\n  peer_link_int_2: 6\n  side_a: true\nmlag_config:\n  domain_id: 1000\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n</code></pre> <p>Notice the variables associated with <code>s1-leaf1</code>. Where did these come from? We'll be exploring that next...</p>"},{"location":"ansible/#variables","title":"Variables","text":"<p>We can define variables in many locations with Ansible. For example, we can explicitly define a variable when running a playbook by using the <code>extra-vars</code> flag.</p> <pre><code>ansible-playbook playbooks/hello_world.yml -e 'name=Mitch'\n</code></pre> <p>The contents of the playbook we just ran can be seen below. We will look more into the anatomy of a playbook in our next section. For now, note that the <code>name</code> variable is also set in the <code>hello-world.yml</code> playbook using the <code>vars</code> parameter. When we ran our playbook with <code>extra_vars</code>, this took precedence over the variable defined inside the playbook.</p> hello_world.yml Playbook (~/project/labfiles/ci-workshops-fundamentals/ansible/playbooks/hello_world.yml) <pre><code>---\n\n- name: A simple playbook\n  hosts: localhost\n  gather_facts: false\n  vars:\n    name: Mr.T\n\n  tasks:\n\n    - name: Say Hello\n      debug:\n        msg: \"Hello {{ name | default('you!') }}\"\n</code></pre> <p>Other valid locations for variables, and their respective precedence, are shown in the diagram below:</p> <p></p> <p>As can be expected, there are a LOT of places where we can define variables. Each option has its use case, but the general recommendation is to use host_vars and group_vars as much as possible.</p> <p>Inside our <code>~/project/labfiles/ci-workshops-fundamentals/ansible/inventory</code> directory, is a <code>host_vars</code> and <code>group_vars</code> directory. These are special directories that Ansible will use to establish a hierarchy of variables that maps directly to our inventory hosts and group structure. Each group can have a dedicated <code>yaml</code> file, as can each host. A visual representation of this can be seen below:</p> <p></p> <p>In the example above, if we were to define a variable in the <code>~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/WORKSHOP_FABRIC.yml</code> file, then all Managed Nodes contained within that group in our inventory file would inherit that variable. The contents of our <code>WORKSHOP_FABRIC.yml</code> file can be seen below:</p> WORKSHOP_FABRIC.yml (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/WORKSHOP_FABRIC.yml) <pre><code># eAPI connectivity via HTTPS (as opposed to CLI via SSH)\nansible_connection: ansible.netcommon.httpapi\n\n# Specifies that we are using Arista EOS\nansible_network_os: arista.eos.eos\n\n# Use SSL (HTTPS)\nansible_httpapi_use_ssl: true\n\n# Disable SSL certificate validation\nansible_httpapi_validate_certs: false\n\n# Credentials\nansible_user: arista\nansible_ssh_pass: arista1c7z\n\n# Global login banner for all switches in topology\nbanner_text: \"This banner came from group_vars/WORKSHOP_FABRIC.YML\"\n</code></pre> <p>These values will all be inherited by the nodes in the <code>WORKSHOP_FABRIC</code> group defined in our inventory file.</p> <p>We can verify this by running <code>ansible-inventory --host s2-spine1 --yaml</code>.</p> <pre><code>ansible-inventory --host s2-spine1 --yaml\n</code></pre> Output of 'ansible-inventory --host s2-spine1 --yaml' <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: YourLabPasswordHere\nansible_user: arista\nbanner_text: This banner came from group_vars/WORKSHOP_FABRIC.YML\n</code></pre> <p>And there they are! All of the variables we had defined in <code>WORKSHOP_FABRIC.yml</code> are present.</p> <p>If we run this same command but specify <code>s1-leaf3</code>, we'll see some additional and slightly different variables:</p> <pre><code>ansible-inventory --host s1-leaf3 --yaml\n</code></pre> Output of 'ansible-inventory --host s1-leaf3 --yaml' <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: arista1c7z\nansible_user: arista\nbanner_text: This banner came from group_vars/S1.YML\nmlag_config:\n  domain_id: 1000\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n</code></pre> <p>Whoa! There is certainly more there...and looking at <code>banner_text</code>, we can see that it's different. With <code>group_vars</code>, the closer to the host we get, the higher the precedence of the variable. So, in the case of <code>banner_text</code>, it is defined in <code>WORKSHOP_FABRIC.yml</code> and <code>S1.yml</code>. Because the <code>S1</code> group is closer to the host (<code>s1-leaf3</code>) in this case, the <code>banner_text</code> variable defined in <code>S1.yml</code> take priority.</p> S1.yml (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/S1.yml) <pre><code>mlag_config:\n  domain_id: 1000\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n\nbanner_text: \"This banner came from group_vars/S1.YML\"\n</code></pre> <p>We'll use the <code>mlag_config</code> stuff later. Let's focus on the <code>banner_text</code> variable for now.</p> <p>Finally, let's take a look at the effective variables on <code>s1-leaf1</code>.</p> <pre><code>ansible-inventory --host s1-leaf1 --yaml\n</code></pre> Output of 'ansible-inventory --host s1-leaf1 --yaml' <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: YourLabPasswordHere\nansible_user: arista\nbanner_text: This banner came from host_vars/s1-leaf1.YML\nmlag:\n  enabled: true\n  peer_link_int_1: 1\n  peer_link_int_2: 6\n  side_a: true\nmlag_config:\n  domain_id: 1000\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n</code></pre> <p>Notice we now have a few more variables, namely the <code>mlag</code> dictionary. We also can see that the <code>banner_text</code> has changed yet again. This time due to the fact that this variable is defined in <code>~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/host_vars/s1-leaf1.yml</code>. This demonstrates that a variable defined in <code>host_vars</code> will take priority over the same variable defined in <code>group_vars</code>.</p> s1-leaf1.yml (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/host_vars/s1-leaf1.yml) <pre><code>mlag_config:\n  domain_id: 1000\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n\nbanner_text: \"This banner came from group_vars/S1.YML\"\n</code></pre> <p>Next, let's use a playbook to deploy our \"Message of the Day\" banner to all of the switches in our lab!</p>"},{"location":"ansible/#playbooks","title":"Playbooks","text":"<p>Ansible Playbooks are written in YAML and typically consist of four main components:</p> <ul> <li>Plays</li> <li>Tasks</li> <li>Modules</li> <li>Module Parameters</li> </ul> <p>Other items such as variables, conditionals, tags, comments, and more are also available tools for us. But, we will focus on the four main components for our discussion.</p> <p>To do this, we'll review a playbook together. Specifically, the <code>~/project/labfiles/ci-workshops-fundamentals/ansible/playbooks/deploy_banner.yml</code> in our lab environment.</p> <p></p> <p>At the start of our playbook, we have the Play which is the very root of the playbook. At the beginning of the play is where we define the Managed Nodes we'd like to target with this play, as well as the list of tasks we'd like to run on these target hosts.</p> <p>Next, we have the task itself, leveraging the eos_facts module to gather information about the Managed Nodes (devices running Arista's EOS in our topology). In a minute, we'll unpack what a module is behind the scenes.</p> <p>Finally, we have any parameters associated with a module. Some of these parameters may be required, while others may be optional. The best way to learn about a module is to find its documentation out on Ansible Galaxy by searching for it. For example, when we search for <code>eos_</code> on Ansible Galaxy, we get the output below:</p> <p></p> <p>34 Modules, one of which is the <code>eos_banner</code> module, with all its associated documentation!</p> <p>More on Ansible Galaxy in a bit...</p> Configure Your Lab Password <p>Update <code>WORKSHOP_FABRIC.yml</code> at the line <code>ansible_ssh_pass</code> with your specific lab password otherwise Ansible playbook runs will error out. (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/WORKSHOP_FABRIC.yml)</p> <p>Let's go ahead and run our playbook! Bonus points if cowsay is enabled.</p> <pre><code>ansible-playbook playbooks/deploy_banner.yml\n</code></pre> <p>Alright! Let's hop into our switches and see what happened...</p>"},{"location":"ansible/#s2-spine1","title":"s2-spine1","text":"<pre><code>\"This banner came from group_vars/WORKSHOP_FABRIC.yml\"\ns2-spine1#\n</code></pre>"},{"location":"ansible/#s1-spine1","title":"s1-spine1","text":"<pre><code>\"This banner came from group_vars/S1.yml\"\ns1-spine1#\n</code></pre>"},{"location":"ansible/#s1-leaf1","title":"s1-leaf1","text":"<pre><code>\"This banner came from host_vars/s1-leaf1.yml\"\ns1-leaf1#\n</code></pre> <p>As expected, each device used whichever <code>banner_text</code> variable was closest to it in the group hierarchy. Or in the case of s1-leaf1, <code>banner_text</code> was applied via its <code>host_vars</code> file.</p>"},{"location":"ansible/#modules","title":"Modules","text":"<p>When we ran our playbook, one of the key components was the module. In our case, this was the eos_banner module.</p> <p>But what is this module doing behind the scenes? Why are modules such a key piece of what makes Ansible much easier to start with than other Automation Frameworks?</p> <p>Modules are an abstraction of the Python code necessary to complete a given task, or tasks, associated with the module. In other words, behind the scenes, modules are just Python code!</p> <p>An example of this can be seen below, specifically for the eos_banner module:</p> <p></p> <p>It's a lot easier to call <code>eos_banner</code> in a playbook than it is to write, in a reusable fashion, all of that Python code! The abstraction of the detailed \"behind the scenes\" code is why modules are such as big reason Ansible is easy to get started with.</p> <p>Now, if we feel compelled to dive in and write our own modules, or our own roles/collections/plugins, we can certainly do this. Ansible is very extensible in this manner. When we're finished developing our content, if we want to share it with the world, then we can publish it as a Collection on Ansible Galaxy!.</p>"},{"location":"ansible/#ansible-galaxy","title":"Ansible Galaxy","text":"<p>Earlier, we specified that we were using <code>ansible-core</code> for this workshop. This approach, instead of installing <code>ansible</code>, is becoming increasingly preferred. The reason for this is the efficiency of <code>ansible-core</code>; it is a lightweight minimalist installation of Ansible without any extra modules, roles, plugins, etc., natively included.</p> <p>With <code>ansible-core</code>, we can only grab the modules, roles, plugins, etc. that we need from Ansible Galaxy!.</p> <p></p> <p>For example, if we want to install the Arista EOS Ansible Collection, we can enter the following command in our terminal</p> <pre><code>ansible-galaxy collection install arista.eos\n</code></pre> <p>Once completed, we can validate that the collection has been installed by running the command below:</p> <pre><code>ansible-galaxy collection list\n</code></pre> Info <p>The <code>ansible-galaxy collection list</code> command is supported in Ansible 2.10+</p> <p>This will yield output similar to below:</p> Ansible Galaxy Collection List <pre><code># /home/coder/.ansible/collections/ansible_collections\nCollection        Version\n----------------- -------\nansible.netcommon 7.0.0\nansible.posix     1.5.4\nansible.utils     5.0.0\narista.avd        4.10.0\narista.cvp        3.10.1\narista.eos        10.0.0\ncommunity.general 6.5.0\n</code></pre> <p>Note the line in the output <code>/home/coder/.ansible/collections/ansible_collections</code>...how did Ansible know to look here? The answer: The <code>ansible.cfg</code> file!</p> <p>Specifically, this parameter:</p> <pre><code># Path(s) to search for installed Ansible Galaxy Collections\ncollections_paths = ~/.ansible/collections\n</code></pre> <p>Galaxy has thousands of modules, plugins, roles, and more from many community members.</p> <p>For those familiar with Python, we can think of Ansible Galaxy as PyPi, and our Collections as Python modules.</p> <p>Go forth and explore!</p>"},{"location":"ansible/#ansible-roles","title":"Ansible Roles","text":"<p>Ansible roles allow us to neatly pack all of the tasks, templates, files, etc., that we use to accomplish a task into a nice reusable format. An example of this could be a standardized method for installing NGINX or Apache on a server. Or, as we'll see in our labs, deploying an MLAG domain configuration on Arista switches.</p> <p>Ultimately, Ansible Roles can be seen as a blueprint to automate and standardize our repeatable tasks.</p>"},{"location":"cicd-basics/","title":"CI/CD","text":"<p>This section walks you through an example CI/CD pipeline leveraging GitHub Actions, Arista Validated Designs (AVD), and the Arista CloudVision Platform (CVP). In addition, the lab leverages the Arista Test Drive (ATD) solution to give you a pre-built environment to get started quickly. This section assumes readers have completed the AVD L2LS workshop within their ATD environment.</p> <p>Readers should be familiar with the following concepts.</p> <ul> <li>Git</li> <li>VS Code</li> <li>Jinja &amp; YAML</li> <li>Ansible</li> </ul>"},{"location":"cicd-basics/#the-topology","title":"The topology","text":"<p>Throughout this section, we will use the following dual data center topology. Click on the image to zoom in for details.</p> <p></p>"},{"location":"cicd-basics/#getting-started","title":"Getting started","text":"<p>This repository leverages the dual data center (DC) ATD. If you are not leveraging the ATD, you may still leverage this repository for a similar deployment. Please note that some updates may have to be made for the reachability of nodes and CloudVision (CVP) instances. This example was created with Ansible AVD version <code>4.10</code>.</p>"},{"location":"cicd-basics/#installation-external-to-the-atd-environment-optional","title":"Installation external to the ATD environment (optional)","text":"<p>Note</p> <p>If running outside of the ATD interactive developer environment (IDE), you must install the base requirements.</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npip3 install \"pyavd[ansible]==4.10.0\"\nansible-galaxy collection install -r requirements.yml\n</code></pre>"},{"location":"cicd-basics/#step-1-fork-and-clone-the-repository","title":"Step 1 - Fork and clone the repository","text":"<p>You will be creating your own CI/CD pipeline in this workflow. Log in to your GitHub account and fork the <code>ci-workshops-avd</code> repository to get started.</p> <p>Warning</p> <p>You can skip these steps if you are continuing from the AVD workshop.</p> <p></p> <p></p> <ol> <li>On the IDE terminal, run the following commands:</li> </ol> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <pre><code>git clone &lt;your copied URL&gt;\n</code></pre> <pre><code>cd ci-workshops-avd\n</code></pre> <ol> <li>Configure your global Git settings.</li> </ol> <pre><code>git config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code>git config --global user.email \"name@example.com\"\n</code></pre>"},{"location":"cicd-basics/#step-2-atd-programmability-ide-installation","title":"Step 2 - ATD programmability IDE installation","text":"<p>You can check the current AVD version by running the following command:</p> <pre><code>ansible-galaxy collection list\n</code></pre> <pre><code>\u279c  ci-workshops-avd git:(main) ansible-galaxy collection list\n\n# /home/coder/.ansible/collections/ansible_collections\nCollection        Version\n----------------- -------\nansible.netcommon 7.0.0\nansible.posix     1.5.4\nansible.utils     5.0.0\narista.avd        4.10.0\narista.cvp        3.10.1\narista.eos        10.0.0\ncommunity.general 6.5.0\n\u279c  ci-workshops-avd git:(main)\n</code></pre> <p>Run the following commands to install the required packages within the ATD environment.</p> <pre><code>pip3 config set global.break-system-packages true\npip3 config set global.disable-pip-version-check true\npip3 install \"pyavd[ansible-collection]==4.10.0\"\nansible-galaxy collection install -r requirements.yml\n</code></pre>"},{"location":"cicd-basics/#step-3-fast-forward-the-main-brach","title":"Step 3 - Fast-forward the main brach","text":"<p>On the programmability IDE, merge the <code>cicd-ff</code> branch into the <code>main</code> branch.</p> <p>Warning</p> <p>You can skip this step if you are continuing from the AVD workshop.</p> <pre><code>git merge origin/cicd-ff\n</code></pre> Note <p>You may get a note to edit the commit message, enter windows Ctrl + X or mac Cmd + X to save the message and exit out of the text editor.</p> <p>If you got the dreaded <code>merge: origin/cicd-ff - not something we can merge</code> error, you may have missed unchecking the <code>Copy the main branch only</code> option when forking. You can continue by running the following commands within the workshops directory on the IDE terminal.</p> <pre><code>git remote add upstream https://github.com/aristanetworks/ci-workshops-avd.git\ngit fetch upstream\ngit merge upstream/cicd-ff\n</code></pre>"},{"location":"cicd-basics/#step-4-setup-lab-password-environment-variable","title":"Step 4 - Setup lab password environment variable","text":"<p>Each lab comes with a unique password. We set an environment variable called <code>LABPASSPHRASE</code> with the following command. The variable is later used to generate local user passwords and connect to our switches to push configs.</p> <p>Warning</p> <p>You can skip this step if you are continuing from the AVD workshop.</p> <pre><code>export LABPASSPHRASE=`awk '/password:/{print $2}' /home/coder/.config/code-server/config.yaml`\n</code></pre>"},{"location":"cicd-basics/#step-5-configure-the-ip-network","title":"Step 5 - Configure the IP Network","text":"<p>The nodes that connect the two sites are out of scope for this workshop. We can get the hosts and EOS nodes in the IP network configured by running the <code>make preplab</code> command.</p> <pre><code>make preplab\n</code></pre> <p>The host and IP Network nodes will now be configured.</p>"},{"location":"cicd-basics/#step-6-enable-github-actions","title":"Step 6 - Enable GitHub actions","text":"<ol> <li>Go to Actions</li> <li>Click <code>I understand my workflows, go ahead and enable them</code></li> </ol>"},{"location":"cicd-basics/#set-github-secret","title":"Set GitHub secret","text":"<p>You will need to set one secret in your newly forked GitHub repository.</p> <ol> <li>Go to <code>Settings</code></li> <li>Click <code>Secrets and variables</code></li> <li>Click <code>Actions</code></li> <li>Click <code>New repository secret</code></li> </ol> <p></p> <ol> <li> <p>Enter the secret as follows</p> </li> <li> <p>Name: <code>LABPASSPHRASE</code></p> </li> <li> <p>Secret: Listed in ATD lab topology</p> <p> </p> </li> <li> <p>Click <code>Add secret</code></p> </li> </ol> <p>Note</p> <p>Our workflow uses this secret to authenticate with our CVP instance.</p>"},{"location":"cicd-basics/#step-7-update-local-cvp-variables","title":"Step 7 - Update local CVP variables","text":"<p>Every user will get a unique CVP instance deployed. There are two updates required.</p> <ol> <li> <p>Add the <code>ansible_host</code> variable under the <code>cvp</code> host in the <code>/home/coder/project/labfiles/ci-workshops-avd/sites/site_1/inventory.yml</code> file. The domain name can be located at the top of your ATD lab environment.</p> <pre><code>---\nSITE1:\n  children:\n    CVP:\n      hosts:\n          cvp:\n            ansible_host: &lt;atd-topo12345.topo.testdrive.arista.com&gt;\n   ...\n</code></pre> </li> <li> <p>Add the <code>ansible_host</code> variable under the <code>cvp</code> host in the <code>/home/coder/project/labfiles/ci-workshops-avd/sites/site_2/inventory.yml</code> file.</p> <pre><code>---\nSITE2:\n  children:\n    CVP:\n      hosts:\n          cvp:\n            ansible_host: &lt;atd-topo12345.topo.testdrive.arista.com&gt;\n   ...\n</code></pre> </li> </ol> <p>Note</p> <p>These will be the same value. Make sure to remove any prefix like <code>https://</code> or anything after <code>.com</code></p>"},{"location":"cicd-basics/#step-8-sync-with-remote-repository","title":"Step 8 - Sync with remote repository","text":"<ol> <li> <p>From the IDE terminal, run the following:</p> <pre><code>git add .\ngit commit -m \"Syncing with remote\"\ngit push\n</code></pre> </li> </ol> <p>Note</p> <p>If the Git <code>user.name</code> and <code>user.email</code> are set, they may be skipped. You can check this by running the <code>git config --list</code> command. You will get a notification to sign in to GitHub. Follow the prompts.</p>"},{"location":"cicd-basics/#step-9-create-a-new-branch","title":"Step 9 - Create a new branch","text":"<p>In a moment, we will be deploying changes to our environment. In reality, updates to a code repository would be done from a development or feature branch. We will follow this same workflow.</p> <p>Note</p> <p>This example will use the branch name <code>dc-updates</code>. If you use a different branch name, update the upcoming examples appropriately.</p> <pre><code>git checkout -b dc-updates\n</code></pre>"},{"location":"cicd-basics/#step-10-github-actions","title":"Step 10 - GitHub Actions","text":"<p>GitHub Actions is a CI/CD platform within GitHub. We can leverage GitHub Actions to create automated workflows within our repository. These workflows can be as simple as notifying appropriate reviewers of a change and automating the entire release of an application or network infrastructure.</p>"},{"location":"cicd-basics/#workflow-files","title":"Workflow files","text":"<p>GitHub actions are defined by separate files (<code>dev.yml</code> and <code>prod.yml</code>) within our code repository's <code>.github/workflows</code> directory.</p> <p>At the highest level of our workflow file, we set the <code>name</code> of the workflow. This version of our workflow file represents any pushes that do not go to the main branch. For example, we would like our test or development workflow to start whenever we push or change any branches not named main. We can control this by setting the <code>on.push.branches-ignore</code> variable to main.</p> <pre><code># dev.yml\nname: Test the upcoming changes\n\non:\n  push:\n    branches-ignore:\n      - main\n...\n</code></pre> <p>In the next portion of the workflow file, we define a dictionary of <code>jobs</code>. For this example, we will only use one job with multiple steps. We set the ATD credential as an environment variable that will be available for our future steps. The <code>timeout-minutes</code> variable is optional and only included to ensure we remove any long-running workflows. This workflow should come nowhere near the 15-minute mark. Any more than that, and it should signal to us that there is a problem in the workflow. We can see the <code>runs-on</code> key at the end of this code block. This workflow uses the <code>ubuntu-latest</code> flavor, but other options are available. For example, we can use a Windows, Ubuntu, or macOS runner (machines that execute jobs in a GitHub Actions workflow).</p> <pre><code>...\non:\n  push:\n    branches-ignore:\n      - main\n\njobs:\n  dev:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n...\n</code></pre> <p>Now that we have defined our <code>dev</code> job, we must define what <code>steps</code> will run within this workflow. For this portion, we have the first and second steps in the workflow. The initial step, \"Hi\" is only used to validate an operational workflow and is not required. Next, the <code>actions/checkout</code> action will check out your repository to make the repository accessible in the workflow. Future workflow steps will then be able to use the relevant repository information to run tasks like building a new application or deploying the latest state of a network.</p> <pre><code>...\njobs:\n  dev:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v4\n...\n</code></pre> <p>Having trouble setting up your GitHub actions?</p> <p>Check out this great actions linter from GitHub user <code>rhysd</code>. https://rhysd.github.io/actionlint/</p>"},{"location":"cicd-basics/#pre-commit","title":"pre-commit","text":"<p>To get started with pre-commit, run the following commands in your ATD IDE terminal.</p> <pre><code>pip3 install pre-commit\npre-commit install\n</code></pre> <p>We will leverage pre-commit in our local development workflow and within the pipeline. pre-commit works by running automated checks on Git repositories manually or whenever a Git commit is run. For example, if we wanted all of our YAML files to have a similar structure or follow specific guidelines, we could use a pre-commit \"check-yaml\" hook. Please note this is just a sample of what pre-commit can do. For a list of hooks, check out their official list. The code block below references the pre-commit configuration file used in our repository.</p> <p>In pre-commit, we define our jobs under a <code>repos</code> key. This first repo step points to the built-in hooks provided by the pre-commit team. Please note you can use hooks from other organizations. In our case, the checks are fairly simplistic. The first hook checks to ensure our files have no trailing whitespace. The next hook, <code>end-of-file-fixer</code>, ensures every file is empty or ends with one new line. Next, the check YAML hook validates any YAML file in our repository can be loaded as valid YAML syntax. Below is our workflow example leveraging the pre-commit action. This action will read the <code>.pre-commit-config.yaml</code> file in the root of our repository. The <code>files</code> key only checks files within specific directories.</p> <pre><code># .pre-commit-config.yaml\n# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n        files: sites/site_1/group_vars/|sites/site_2/group_vars/\n\n      - id: end-of-file-fixer\n        exclude_types: [svg, json]\n        files: sites/site_1/group_vars/|sites/site_2/group_vars/\n\n      - id: check-yaml\n        files: sites/site_1/group_vars/|sites/site_2/group_vars/\n</code></pre> <p>Finally, the setup Python and install requirements action above the pre-commit step installs Python dependencies in this workflow.</p> <pre><code>...\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n\n      - name: Install Python requirements\n        run: pip3 install requirements.txt\n\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n...\n</code></pre>"},{"location":"cicd-basics/#pre-commit-example","title":"pre-commit example","text":"<p>We can look at the benefits of pre-commit by introducing three errors in a group_vars file. This example will use the <code>sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml</code> file. Under VLAN 20, we can add extra whitespace after any entry, extra newlines, and move the <code>s1-spine2</code> key under the <code>s1-spine1</code> key.</p> <pre><code>          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n                - node: s1-spine2 # &lt;- Should not be nested under s1-spine1\n                  ip_address: 10.20.20.3/24\n# &lt;- Newline\n# &lt;- Newline\n</code></pre> <p>We can run pre-commit manually by running the following command:</p> <pre><code>pre-commit run -a\n</code></pre> Output<pre><code>trim trailing whitespace.................................................Passed\nfix end of files.........................................................Failed\n- hook id: end-of-file-fixer\n- exit code: 1\n- files were modified by this hook\n\nFixing sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml\n\ncheck yaml...............................................................Failed\n- hook id: check-yaml\n- exit code: 1\n\nwhile parsing a block mapping\n  in \"sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml\", line 25, column 17\ndid not find expected key\n  in \"sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml\", line 27, column 17\n\n\u279c  ci-workshops-avd git:(main) \u2717\n</code></pre> <p>We can see the two failures. pre-commit hooks will try and fix errors. However, pre-commit does not assume our intent with the YAML file; that fix is up to us. If you correct the indentation in the file and rerun pre-commit, you will see all passes.</p> <pre><code>          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2 # &lt;- Indentation fixed\n                ip_address: 10.20.20.3/24\n# &lt;- One newline\n</code></pre> <pre><code>\u279c  ci-workshops-avd git:(main) \u2717 pre-commit run -a\ntrim trailing whitespace.................................................Passed\nfix end of files.........................................................Passed\ncheck yaml...............................................................Passed\n\u279c  ci-workshops-avd git:(main)\n</code></pre>"},{"location":"cicd-basics/#filter-changes-to-the-pipeline","title":"Filter changes to the Pipeline","text":"<p>Currently, our workflow will build and deploy configurations for both sites. This is true even if we only have changes relevant to one site. We can use a path filter to check if changes within specific directories have been modified, signaling that a new build and deployment are required. Please take note of the <code>id</code> key. This will be referenced in our upcoming workflow steps.</p> <pre><code>...\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n\n      - name: Check paths for sites/site_1\n        uses: dorny/paths-filter@v3\n        id: filter-site1\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_1/**'\n\n      - name: Check paths for sites/site_2\n        uses: dorny/paths-filter@v3\n        id: filter-site2\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_2/**'\n...\n</code></pre>"},{"location":"cicd-basics/#conditionals-to-control-flow","title":"Conditionals to control flow","text":"<p>The Ansible collection install and test configuration steps have the conditional key of <code>if</code>. This maps to each path filter check step we used earlier. For example, the first path check has an <code>id</code> of <code>filter-site1</code>. We can reference the <code>id</code> in our workflow as <code>steps.filter-site1.outputs.workflows</code>. If this is set to <code>true</code>, a change will register in our check, and the test build step for site 1 will run. One difference is the Ansible collection install uses the <code>||</code> (or) operator. The \"or\" operator allows us to control when Ansible collections are installed. The collections will be installed if a change is registered in either <code>filter-site1</code> or <code>filter-site2</code>.</p> <pre><code>...\n      - name: Install collections\n        run: ansible-galaxy collection install -r requirements.yml\n        if: steps.filter-site1.outputs.workflows == 'true' || steps.filter-site2.outputs.workflows == 'true'\n\n      - name: Test configuration for site1\n        run: make build-site-1\n        if: steps.filter-site1.outputs.workflows == 'true'\n\n      - name: Test configuration for site2\n        run: make build-site-2\n        if: steps.filter-site2.outputs.workflows == 'true'\n</code></pre> <p>At this point, make sure both workflow files (<code>dev.yml</code> and <code>prod.yml</code>) within the <code>.github/workflows</code> directory are not commented out. An example of the <code>dev.yml</code> file is below.</p> .github/workflows/dev.yml <pre><code>name: Test the upcoming changes\n\non:\n  push:\n    branches-ignore:\n      - main\n\njobs:\n  dev:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n\n      - name: Install Python requirements\n        run: pip3 install \"pyavd[ansible]==4.10.0\"\n\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n\n      - name: Check paths for sites/site_1\n        uses: dorny/paths-filter@v3\n        id: filter-site1\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_1/**'\n\n      - name: Check paths for sites/site_2\n        uses: dorny/paths-filter@v3\n        id: filter-site2\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_2/**'\n\n      - name: Install collections\n        run: ansible-galaxy collection install -r requirements.yml\n        if: steps.filter-site1.outputs.workflows == 'true' || steps.filter-site2.outputs.workflows == 'true'\n\n      - name: Test configuration for site1\n        run: make build-site-1\n        if: steps.filter-site1.outputs.workflows == 'true'\n\n      - name: Test configuration for site2\n        run: make build-site-2\n        if: steps.filter-site2.outputs.workflows == 'true'\n</code></pre>"},{"location":"cicd-basics/#step-11-day-2-operations-new-service-vlan","title":"Step 11 - Day-2 Operations - New service (VLAN)","text":"<p>This example workflow will add two new VLANs to our sites. Site 1 will add VLAN 25, and site 2 will add VLAN 45. An example of the updated group_vars is below. The previous workshop modified the configuration of our devices directly through eAPI. This example will leverage GitHub actions with CloudVision to update our nodes. The provisioning with CVP will also create a new container topology and configlet assignment per device. For starters, we can update site 1.</p> sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml <pre><code>---\ntenants:\n  - name: MY_FABRIC\n    vrfs:\n      - name: default\n        svis:\n          - id: 10\n            name: 'Ten'\n            tags: [ \"Web\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.10.10.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.10.10.2/24\n              - node: s1-spine2\n                ip_address: 10.10.10.3/24\n          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2\n                ip_address: 10.20.20.3/24\n          - id: 25\n            name: 'Twenty-five'\n            tags: [ \"Wifi\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.25.25.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.25.25.2/24\n              - node: s1-spine2\n                ip_address: 10.25.25.3/24\n</code></pre>"},{"location":"cicd-basics/#build-the-updates-locally-optional","title":"Build the updates locally (optional)","text":"<p>The pipeline will run the build and deploy steps for us with these relevant changes. We can also run the build steps locally to see all our pending updates.</p> <pre><code>make build-site-1\n</code></pre> <p>Feel free to check out the changes made to your local files. Please make sure the GitHub workflows are uncommented. We can now push all of our changes and submit a pull request.</p> <p>Note</p> <p>The GitHub workflows are located in the <code>atd-cicd/.github/workflows</code> directory.</p> <pre><code>git add .\ngit commit -m \"updating VLANs\"\ngit push --set-upstream origin dc-updates\n</code></pre>"},{"location":"cicd-basics/#viewing-actions","title":"Viewing actions","text":"<p>If you navigate back to your GitHub repository, you should see an action executing.</p> <ol> <li>Click <code>Actions</code></li> <li>Click on the latest action</li> </ol> <p></p> <p>Since this is a development branch, we are only testing for valid variable files so that AVD can successfully build our configurations. We can run one more example before deploying to production. You may notice the test configuration step was only initiated for site 1 and was skipped for site 2 (no changes). You can finish this example by updating the site 2 fabric services file.</p> sites/site_2/group_vars/SITE2_FABRIC_SERVICES.yml <pre><code>---\ntenants:\n  - name: MY_FABRIC\n    vrfs:\n      - name: default\n        svis:\n          - id: 30\n            name: 'Thirty'\n            tags: [ \"DB\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.30.30.1\n            nodes:\n              - node: s2-spine1\n                ip_address: 10.30.30.2/24\n              - node: s2-spine2\n                ip_address: 10.30.30.3/24\n          - id: 40\n            name: 'Forty'\n            tags: [ \"DMZ\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.40.40.1\n            nodes:\n              - node: s2-spine1\n                ip_address: 10.40.40.2/24\n              - node: s2-spine2\n                ip_address: 10.40.40.3/24\n          - id: 45\n            name: 'Forty-five'\n            tags: [ \"Guest\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.45.45.1\n            nodes:\n              - node: s2-spine1\n                ip_address: 10.45.45.2/24\n              - node: s2-spine2\n                ip_address: 10.45.45.3/24\n</code></pre> <pre><code>make build-site-2\ngit add .\ngit commit -m \"updating VLANs for site 2\"\ngit push\n</code></pre> <p>Once complete, the GitHub actions will show changes on sites 1 and 2.</p> <p></p>"},{"location":"cicd-basics/#step-12-creating-a-pull-request-to-deploy-updates-main-branch","title":"Step 12 -  Creating a pull request to deploy updates (main branch)","text":"<p>We have activated our GitHub workflows and tested our configurations. We are now ready to create a pull request.</p> <p>In your GitHub repository, you should see a tab for Pull requests.</p> <ol> <li>Click on <code>Pull requests</code></li> <li>Click on <code>New pull request</code></li> <li>Change the base repository to be your fork</li> <li>Change the compare repository to <code>dc-updates</code></li> <li>Click <code>Create pull request</code></li> </ol> <p></p> <p></p> <p></p> <p>Add a title and enough of a summary to get the point across to other team members.</p> <p></p> <p>Once this is complete, click <code>Create pull request</code>. Since all checks have passed, we can merge our new pull request. If you have multiple options on the type of merge, select <code>squash and merge</code>.</p> <p></p> <p></p> <p>At this point, this will kick off our production workflow (<code>prod.yml</code>) against the <code>main</code> branch. The <code>prod.yml</code> workflow will build and deploy our updates with CVP. If you go to the \"Provisioning\" tab of CVP, you should be able to see tasks and pending changes. This workflow will automatically run any pending tasks for us. We can optionally connect to one of the spines at either site to see the new VLANs.</p> <p></p> <pre><code>s1-spine1#show vlan\nVLAN  Name                             Status    Ports\n----- -------------------------------- --------- -------------------------------\n1     default                          active\n10    Ten                              active    Cpu, Po1, Po2\n20    Twenty                           active    Cpu, Po1, Po4\n25    Twenty-five                      active    Cpu, Po1\n4093  LEAF_PEER_L3                     active    Cpu, Po1\n4094  MLAG_PEER                        active    Cpu, Po1\n\ns1-spine1#\n################################################################################\ns2-spine1#show vlan\nVLAN  Name                             Status    Ports\n----- -------------------------------- --------- -------------------------------\n1     default                          active\n30    Thirty                           active    Cpu, Po1, Po2\n40    Forty                            active    Cpu, Po1, Po4\n45    Forty-five                       active    Cpu, Po1\n4093  LEAF_PEER_L3                     active    Cpu, Po1\n4094  MLAG_PEER                        active    Cpu, Po1\n\ns2-spine1#\n</code></pre>"},{"location":"cicd-basics/#the-subtle-difference-between-prod-and-dev","title":"The subtle difference between prod and dev","text":"<p>You may recall that at the start of this lab, the <code>dev.yml</code> actions file ignored the branch <code>main</code>. This is intentional, as we do not want our development workflows to run against the production branch and possibly make changes to the live network. Since we merged our changes into the main branch, the run of the <code>prod.yml</code> actions file will be triggered. Below is a snippet of the production actions file, which is now set to run against any pushes to the main branch.</p> <pre><code>name: Deploy updates\n\non:\n  push:\n    branches:\n      - main\n</code></pre> <p>The rest of the workflow file is the exact same as <code>dev.yml</code>. The other subtle difference is that now, depending on the files changed, we will run the build playbooks as well as the playbooks responsible for deploying with CloudVision.</p> <pre><code>jobs:\n  deploy-prod:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n\n      - name: Install Python requirements\n        run: pip3 install \"pyavd[ansible]==4.10.0\"\n\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n\n      - name: Check paths for sites/site_1\n        uses: dorny/paths-filter@v3\n        id: filter-site1\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_1/**'\n\n      - name: Check paths for sites/site_2\n        uses: dorny/paths-filter@v3\n        id: filter-site2\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_2/**'\n\n      - name: Install collections\n        run: ansible-galaxy collection install -r requirements.yml\n        if: steps.filter-site1.outputs.workflows == 'true' || steps.filter-site2.outputs.workflows == 'true'\n\n      - name: Build and deploy site1\n        run: make build-site-1 cvp-site-1\n        if: steps.filter-site1.outputs.workflows == 'true'\n\n      - name: Build and deploy site2\n        run: make build-site-2 cvp-site-2\n        if: steps.filter-site2.outputs.workflows == 'true'\n</code></pre> <p>Below is an example of an Ansible playbook that leverages the <code>arista.avd.eos_config_deploy_cvp</code> role to push new configlets to CVP, assign configlets to devices, create tasks, and optionally approve the change controls to push updates to our network. The playbook also targets the <code>cvp</code> host. If you recall, we updated the host value to its public URL earlier. Therefore, this GitHub actions workflow can communicate with our lab CloudVision instance and deploy our changes.</p> <pre><code>---\n- name: Build Switch configuration\n  hosts: cvp\n  gather_facts: false\n\n  tasks:\n\n    - name: Generate Intended Config and Documentation\n      ansible.builtin.import_role:\n        name: arista.avd.eos_config_deploy_cvp\n      vars:\n        container_root: 'SITE1_FABRIC'\n        configlets_prefix: 'AVD'\n        state: present\n</code></pre>"},{"location":"cicd-basics/#summary","title":"Summary","text":"<p>Congratulations, you have successfully deployed a CI/CD pipeline with GitHub Actions. Feel free to make additional site changes or extend the testing pieces.</p> <p>Note</p> <p>If your topology shuts down or time elapses, you must install the requirements, Git configuration, and GitHub authentication.</p> <p>You must also set the <code>LABPASSPHRASE</code> environment variable in the IDE terminal.</p> <pre><code>export LABPASSPHRASE=`awk '/password:/{print $2}' /home/coder/.config/code-server/config.yaml`\n</code></pre>"},{"location":"git/","title":"Getting started with Git","text":""},{"location":"git/#introduction","title":"Introduction","text":"<p>In this section, we will explore a brief introduction of Git. We will cover the installation and basic commands used with Git. Git is the most commonly used version control system. Git tracks changes to files allowing you to revert to specific versions. File changes are tracked by storing snapshots (commits) of the files over time. In the image below, the content of files A, B, and C change over time. Git allows you to roll back to any previous commit.</p> <p></p> <p>Git makes collaboration effortless by allowing multiple people to merge their changes into one source. Whether you work solo or as part of a team, Git will be useful for you.</p> <p>Basic Git commands we will be working with:</p> <ul> <li>git config</li> <li>git status</li> <li>git init</li> <li>git add</li> <li>git commit</li> <li>git log</li> <li>git branch</li> <li>git clone</li> <li>git merge</li> <li>git switch</li> <li>git diff</li> <li>git restore</li> </ul>"},{"location":"git/#installation-setup","title":"Installation &amp; setup","text":""},{"location":"git/#installation","title":"Installation","text":"Note <p>Git has been pre-installed in your ATD Lab environment. If you need to install and configure Git on another system, follow the instructions at the links above.</p> <p>Download Git - https://git-scm.com/downloads</p> <p>Configuration - https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</p>"},{"location":"git/#setup","title":"Setup","text":"<p>When setting up Git for the first time, you must configure your Identity with a name and email address. This is used to add your signature to commits. Additionally, set the default branch name to <code>main</code>.</p> <p>Run the following commands from the Terminal in your ATD Lab Programmability IDE.</p> <pre><code># Set your username:\ngit config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code># Set your email address:\ngit config --global user.email \"name@example.com\"\n</code></pre> <pre><code># Set default branch name to `main`\ngit config --global init.defaultbranch main\n</code></pre>"},{"location":"git/#programmability-ide-vs-code","title":"Programmability IDE (VS Code)","text":"<p>Verify your configuration:</p> <pre><code>git config --global --list\n</code></pre>"},{"location":"git/#download-sample-files","title":"Download Sample Files","text":"<p>We have provided some sample configuration files to begin working with Git. From the Programmability IDE, run the following two commands to download sample files and change your working directory.</p> <pre><code>bash -c \"$(curl https://raw.githubusercontent.com/aristanetworks/ci-workshops-fundamentals/main/get-sample-files.sh)\"\ncd /home/coder/project/labfiles/samplefiles\n</code></pre>"},{"location":"git/#git","title":"Git","text":""},{"location":"git/#git-command-line-basics","title":"Git - command line basics","text":""},{"location":"git/#initialize-the-directory-as-a-git-repository","title":"Initialize the directory as a Git repository","text":"<p>Next, we initialize the current directory <code>/home/coder/project/labfiles/samplefiles/</code> as a repository (repo).</p> <pre><code>git init\n</code></pre> <p>Notice your CLI prompt changed.</p> <p>The directory is now initialized as a Git repository, and the following hidden sub-directory <code>/home/coder/project/labfiles/samplefiles/.git/</code> was created. It holds version control information for your repository.</p> <p> Congratulations!!! You have created your first repository.</p>"},{"location":"git/#git-repository-status","title":"Git Repository Status","text":"<p>Check the current status of your repo.</p> <pre><code>git status\n</code></pre> <p>Since this is a brand new repo, you should see output similar to the following, indicating there are untracked files.</p> <pre><code>On branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        leaf1.cfg\n        leaf2.cfg\n        leaf3.cfg\n        leaf4.cfg\n        spine1.cfg\n        spine2.cfg\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre>"},{"location":"git/#stage-your-changes","title":"Stage your changes","text":"<p>When you want to track files, you first need to stage them. The above output gives you a clue about the command needed to stage the changes. You can specify individual files or add all files with a wildcard period <code>.</code></p> <p>To stage all file changes:</p> <pre><code>git add .\n</code></pre> <p>Then recheck the status to see what is staged and ready to be committed.</p> <pre><code>git status\n</code></pre> <p>Output:</p> <pre><code>On branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   leaf1.cfg\n        new file:   leaf2.cfg\n        new file:   leaf3.cfg\n        new file:   leaf4.cfg\n        new file:   spine1.cfg\n        new file:   spine2.cfg\n</code></pre> <p>All the files are staged and ready to be committed to the <code>main</code> branch.</p>"},{"location":"git/#commit-your-changes","title":"Commit your changes","text":"<p>Now you can commit your staged changes with a comment:</p> Note <p>Use a comment reflecting the changes made so you can reference this commit in the future. Commit messages will show up in the log.</p> <pre><code>git commit -m \"Initial Commit\"\n</code></pre> <p>Output:</p> <pre><code>[main (root-commit) 45eeb6d] Initial Commit\n 6 files changed, 832 insertions(+)\n create mode 100644 leaf1.cfg\n create mode 100644 leaf2.cfg\n create mode 100644 leaf3.cfg\n create mode 100644 leaf4.cfg\n create mode 100644 spine1.cfg\n create mode 100644 spine2.cfg\n</code></pre> <p>Now these files are committed to your local repository.</p> <p>Check the status one more time.</p> <pre><code>On branch main\nnothing to commit, working tree clean\n</code></pre> <p>You have successfully stamped history in your repo. Check the log to see what is there.</p> <pre><code>git log\n</code></pre> Note <p>Press <code>q</code> to quit viewing the log.</p>"},{"location":"git/#create-a-branch","title":"Create a branch","text":"<p>Creating a branch allows you to make a new copy of your files without affecting the files in the <code>main</code> branch. For example, if you wanted to update the hostnames on your switches, you might create a new branch called <code>update-hostnames</code>.</p> <p></p> <p>Verify the current branch:</p> <pre><code>git branch\n</code></pre> <p>Create a new branch:</p> <pre><code>git branch update-hostnames\n</code></pre> <p>Switch to this new branch:</p> <pre><code>git switch update-hostnames\n</code></pre> <p>Using the IDE, open each switch config file and update the hostname by removing the prefix <code>s1-</code>. Changes are auto-saved.</p> <p>Example: spine1.cfg - change hostname from <code>s1-spine1</code> to <code>spine1</code>.</p> <p>Let's verify the changes (diffs) we are about to commit to ensure they are correct.</p> <pre><code>git diff\n</code></pre> <p>Stage and commit the changes to the new branch <code>update-hostnames</code>.</p> <pre><code>git add .\ngit commit -m \"updated hostname on each switch\"\n</code></pre>"},{"location":"git/#merge-branch","title":"Merge branch","text":"<p>Now that we are satisfied with our hostname changes, we can merge the <code>update-hostnames</code> branch into <code>main</code>.</p> <p></p> <p>First, switch back to the <code>main</code> branch and notice the hostnames return to the original name. Why did that happen? Remember, we never modified the original copy <code>main</code> branch. This is a different version of the file. Once we merge the <code>update-hostnames</code> branch into <code>main</code>, both copies will be identical.</p> <pre><code>git switch main\n</code></pre> <p>Execute the merge operation:</p> <pre><code>git merge update-hostnames\n</code></pre> <p>Verify the updated hostnames in each file.</p> <p>Now that your changes are merged, you can safely delete the <code>update-hostnames</code> branch.</p> <pre><code>git branch -d update-hostnames\n</code></pre>"},{"location":"git/#github","title":"GitHub","text":"<p>Before proceeding further, make sure you are logged into your GitHub account.</p> <p>If you do not have a GitHub account, you can create one here.</p> Note <p>In the ATD Lab, you will authenticate to GitHub using an eight-digit access code. On other systems, you will need a Personal Access Token. You may skip the next step if you work in the Arista-provided ATD Lab IDE. Detailed instructions for creating a Personal Access Token can be found here.</p>"},{"location":"git/#create-a-github-personal-access-token","title":"Create a GitHub personal access token","text":"When not using the ATD lab IDE, generate a GitHub personal access token <p>Under normal circumstances (does not apply to the ATD lab IDE) to push your local repo to GitHub, you will need a Personal Access Token. From your GitHub account, click through the following path to generate a new personal access token.  Profile \u2192 Settings \u2192 Developer Settings \u2192 Personal Access Tokens \u2192 Tokens (classic) \u2192 Generate new token (classic)</p> <ul> <li>Give the token a meaningful name by setting the Note: <code>MyNewToken</code></li> <li>Set the Expiration: 30 days (default)</li> </ul> <p>Select the scopes to grant to this token. To use your token to access repositories from the command line, select <code>repo</code>. A token with no assigned scopes can only access public information.</p> <p>Click <code>Generate token</code> at the bottom of the page.  Copy and save the token in a secure place. YOU WILL NOT BE ABLE TO SEE THE TOKEN AGAIN.</p> <p></p>"},{"location":"git/#fork-a-repository","title":"Fork a repository","text":"<p>A fork is a copy of another repository that you can manage. Forks let you make changes to a project without affecting the original repository. You can fetch updates from or submit changes to the original repository with a pull request.</p> <p>Fork the example Arista CI Fundamentals repository to make your copy.</p>"},{"location":"git/#steps-to-fork-the-example-repository","title":"Steps to Fork the example repository","text":"<ol> <li>From GitHub.com, navigate to the Arista CI Fundamentals repository.</li> <li>In the top-right corner of the page, click Fork. </li> <li>Select an owner.</li> <li>Set repository name. By default, forks are named the same as their upstream repository.</li> <li>Optionally, add a description of your fork.</li> <li>Click <code>Create fork</code> button at the bottom</li> </ol> <p>You should now see your repository <code>username/ci-workshops-fundamentals</code> forked from <code>aristanetworks/ci-workshops-fundamentals</code>.</p> <p>Next up... Clone this forked repository to your local host machine.</p>"},{"location":"git/#clone-forked-repo-to-local-host","title":"Clone forked repo to local host","text":"<p>Cloning a repository allows us to make a local copy of a project on GitHub. In the previous step, you forked a repository to your local GitHub account. Navigate to your forked repo in GitHub. From there, click on the green code button and copy the URL of the forked repository.</p> <p></p> <p>Clone this repository to your local machine.</p> <p>Before cloning, change your current directory to <code>/home/coder/project/labfiles/</code>.</p> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <p>Clone the repo.</p> <pre><code># replace this URL with your forked repo\ngit clone https://github.com/xxxxxxx/ci-workshops-fundamentals.git\n</code></pre> <p>Now change into the new cloned directory.</p> <pre><code>cd ci-workshops-fundamentals\n</code></pre> <p>Verify the location of the remote copy. This should be your local GitHub account.</p> <pre><code>git remote -v\n</code></pre> <p>Output:</p> <pre><code>origin  https://github.com/xxxxxxx/ci-workshops-fundamentals.git (fetch)\norigin  https://github.com/xxxxxxx/ci-workshops-fundamentals.git (push)\n</code></pre> <p>In the next step, let's add VLAN 40 to the data model in <code>avd/vlans.yml</code>. First, create a new branch called <code>add-vlan-40</code>.</p>"},{"location":"git/#create-and-switch-to-a-new-branch","title":"Create and switch to a new branch","text":"<pre><code>git branch add-vlan-40\ngit switch add-vlan-40\n</code></pre> <p>Using the Programmability IDE, update the file <code>ci-workshops-fundamentals/avd/vlans.yml</code> with VLAN 40 information.</p> Updated vlans.yml <pre><code>---\nvlans:\n  - 10:\n    name: Ten\n  - 20:\n    name: Twenty\n  - 30:\n    name: Thirty\n  - 40:\n    name: Forty\n</code></pre> <p>Now, stage and commit these changes to the new branch.</p> <pre><code>git add .\ngit commit -m \"added vlan 40\"\n</code></pre>"},{"location":"git/#push-changes-to-github","title":"Push Changes to GitHub","text":"<p>Now push the updated branch to your remote fork on GitHub.</p> <pre><code># push new branch to remote repo on GitHub\ngit push --set-upstream origin add-vlan-40\n</code></pre> Note <p>If this is your first push from the Lab environment, you will be prompted to authenticate to GitHub. Follow the <code>Copy &amp; Continue to GitHub</code> prompts by entering the eight-digit authentication code. Additional pushes to GitHub will cache your credentials.</p> <p>Once authenticated, your new branch and updated file will exist on GitHub.</p> <p>Output:</p> <pre><code>Enumerating objects: 7, done.\nCounting objects: 100% (7/7), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (4/4), 352 bytes | 352.00 KiB/s, done.\nTotal 4 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: Create a pull request for 'add-vlan-40' on GitHub by visiting:\nremote:      https://github.com/xxxxxxxx/ci-workshops-fundamentals/pull/new/add-vlan-40\nremote:\nTo https://github.com/xxxxxxxx/ci-workshops-fundamentals.git\n * [new branch]      add-vlan-40 -&gt; add-vlan-40\nBranch 'add-vlan-40' set up to track remote branch 'add-vlan-40' from 'origin'.\n</code></pre> <p>You should now see the new branch <code>add-vlan-40</code> and commit messages in GitHub.</p> <p>The next step is to merge the <code>add-vlan-40</code> branch into the <code>main</code> branch. A Pull Request is used to do this.</p>"},{"location":"git/#pull-request","title":"Pull request","text":"<p>A Pull Request in Git allows a contributor (you) to ask a maintainer (owner) of the origin repository to review code changes you wish to merge into a project. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the <code>main</code> branch.</p> <p>Once all changes have been agreed upon, the maintainer of the original repo will merge your changes. At this point, your code changes are visible in the origin project repo.</p>"},{"location":"git/#steps-to-initiate-a-pull-request","title":"Steps to Initiate a Pull Request","text":"<ol> <li>On GitHub.com, navigate to the main page of your forked repository.</li> <li>In the \"Branch\" menu, choose the branch that contains your commits.</li> <li>Above the list of files, click the Contribute drop-down and click Open pull request. </li> <li>Verify the <code>base repository:</code> is set to <code>aristanetworks/ci-workshops-fundamentals</code> and <code>base:</code> is set to <code>main</code>. Set the <code>head repository:</code> to your forked repo and <code>compare:</code> to the <code>add-vlan-40</code> branch. </li> <li>Add a title and description for your pull request.</li> <li>Click Create pull request.</li> </ol> <p>This will generate a Pull Request on the main project repository <code>aristanetworks/ci-workshops-fundamentals</code>. The owner/maintainer can merge the pull request once all changes are satisfied.</p> Note <p>During the workshop the PR is not normally merged to prevent having to reset the repo before the next workshop. The <code>Cleanup</code> section below is the normal course of action.</p>"},{"location":"git/#cleanup-optional","title":"Cleanup (optional)","text":"<p>After your Pull Request is merged, you may cleanup your old branch and sync your fork.</p> <ol> <li>Delete your branch on GitHub and your local host.</li> <li>Sync your Forked repo (below)</li> <li>Pull the updates into <code>main</code> branch on your local host. <code>git pull</code></li> </ol> <p></p>"},{"location":"jinja-yaml/","title":"Welcome to YAML &amp; Jinja","text":"<p>This section will cover both Jinja and YAML, which are two interdependent pieces of basic configuration automation framework. While both YAML and Jinja can get relatively complex with what they can accomplish and what can be done with them, we will only cover what is necessary to utilize these tools for network automation and DevOps. At the end of the YAML and Jinja sections, we will tie everything together with a final output.</p>"},{"location":"jinja-yaml/#what-is-yaml","title":"What is YAML?","text":"<p>YAML is what's officially referred to as a data serialization language. While that sounds complex, data serialization is the process of converting objects within a data model into a byte stream for the purpose of storing or transferring it. Breaking this down, we know when trying to automate configuration management or deployment, we need to specify what it is we want to configure, that is, what should our desired end state configuration look like. ( intent based networking )</p> <p>An important note is about why we are talking about and using YAML in the first place. As mentioned in the beginning, YAML is a data serialization language, however, it is not the only one. Some other common, data serialization languages are XML, JSON, and CSV. The reason we are particular towards YAML is that not only are there libraries available in most programming languages, but also because as the following table shows, it is very human readable.</p> XML JSON YAML <pre><code>&lt;ntp&gt;\n  &lt;local_interface&gt;\n    &lt;name&gt;Management1&lt;/name&gt;\n    &lt;vrf&gt;mgmt&lt;/vrf&gt;\n  &lt;/local_interface&gt;\n  &lt;servers&gt;\n    &lt;name&gt;time-a-g.nist.gov&lt;/name&gt;\n    &lt;preferred&gt;true&lt;/preferred&gt;\n    &lt;vrf&gt;mgmt&lt;/vrf&gt;\n  &lt;/servers&gt;\n  &lt;servers&gt;\n    &lt;name&gt;utcnist.colorado.edu&lt;/name&gt;\n    &lt;vrf&gt;mgmt&lt;/vrf&gt;\n  &lt;/servers&gt;\n&lt;/ntp&gt;\n</code></pre> <pre><code>{\n  \"ntp\": {\n    \"local_interface\": {\n      \"name\": \"Management1\",\n      \"vrf\": \"mgmt\"\n    },\n    \"servers\": [\n      {\n        \"name\": \"time-a-g.nist.gov\",\n        \"preferred\": true,\n        \"vrf\": \"mgmt\"\n      },\n      {\n        \"name\": \"utcnist.colorado.edu\",\n        \"vrf\": \"mgmt\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>---\nntp:\n  local_interface:\n    name: Management1\n    vrf: mgmt\n  servers:\n    - name: time-a-g.nist.gov\n      preferred: true\n      vrf: mgmt\n    - name: utcnist.colorado.edu\n      vrf: mgmt\n</code></pre>"},{"location":"jinja-yaml/#common-yaml-syntax","title":"Common YAML Syntax","text":"<p>Now that we know the idea of what YAML is and what it's used for, we can cover the various constructs within a YAML file, and how to use those expressions to create files that are useful to us as engineers trying to manage configurations.</p>"},{"location":"jinja-yaml/#comments","title":"Comments","text":"<p>Comments are an important part of any documentation or configuration you are doing. When creating or modifying any network configurations, while you don't realize it, you are commenting something that is important. When configuring an interface, you may add a description, such as the name of a downstream device or server it's connected to. Or if it's a provider handoff you may add the NOC telephone number and the circuit ID. If you are configuring BGP peers, you may add a description to each peer so it's clear what the neighbor is.</p> <p>In YAML, you can enter comments anywhere you'd like, and ideally, where it makes sense to help explain what your data model is representing. Comments in YAML are represented with the pound symbol (as shown below).</p> <pre><code>---\n# This is my YAML vars file for global configuration settings.\nsome config:  here\n\n# Management Interface Configuration\nmgmt_gateway: 192.168.0.1\nmgmt_interface: Management0\nmgmt_interface_vrf: default\n</code></pre> Tip <p> In VS Code, you can auto-comment any text you want by selecting the text and pressing windows Ctrl + / or mac Cmd + /.</p>"},{"location":"jinja-yaml/#mappings","title":"Mappings","text":"<p>The first, and basic YAML construct is a mapping, which comprises of what is technically called a <code>scalar</code>. This will probably be the only time you hear or reference the word scalar, ever.</p> <pre><code>---\n# These are some examples of scalars\n\ninteger: 10\nboolean: true\nstring: \"Welcome to the Automation Workshop\"\n</code></pre> <p>Sometimes, a scalar is referred to as only the initial part of the line of text such as a list of names or locations (as shown below).</p> <pre><code>---\n# Another scalar example\n\n- White Rim Trail\n- Hells Revenge\n- Long Canyon Road\n- Black Bears Pass\n- Ophir Pass\n</code></pre> <p>In normal day-to-day use, you will see scalars referred to as key-value pair mappings, or just mappings for short. A mapping is the data label (key), followed by its value, separated by a <code>:</code> colon.</p> <p>Here are some network config specific mappings:</p> <pre><code>---\n# Network Config Mappings\n\nmgmt_interface: Management1\nmgmt_interface_vrf: MGMT\n\nspanning_tree_mode: rapid-pvst\nspanning_tree_priority: 4096\n</code></pre>"},{"location":"jinja-yaml/#boolean","title":"Boolean","text":"<p>Another type of mapping in YAML are booleans. Booleans are simply a <code>true</code> or <code>false</code> value assigned to its data label. While the actual true or false does not have to be in a certain case, if you want the value to be compatible with lint options, you should use all lowercase.</p> <p>Here are some network config boolean mappings.</p> <pre><code>---\n# Network Config Mappings\n\nevpn_import_pruning: true\n\nevpn_gateway:\n        evpn_l2:\n          enabled: true\n\nmlag: false\n</code></pre>"},{"location":"jinja-yaml/#lists","title":"Lists","text":"<p>Lists, which are sometimes also called sequences, are similar to arrays in development. A list has a top level label, or what I like to refer to as a key, which represent the elements in the list. ( The key nomenclature will come in handy when thinking through loops in Jinja templates )  In a standard list, you will have singular entries below the top level label.</p> <p>Here are some examples of lists:</p> <pre><code>---\n# Some lists of BGP specific configurations\n\nbgp-peers:\n    - 192.168.103.1\n    - 192.168.103.3\n    - 192.168.103.5\n\nbgp_defaults:\n    - 'no bgp default ipv4-unicast'\n    - 'distance bgp 20 200 200'\n    - 'neighbor default send-community'\n    - 'graceful-restart restart-time 300'\n    - 'graceful-restart'\n</code></pre>"},{"location":"jinja-yaml/#dictionary","title":"Dictionary","text":"<p>While we just covered lists, which allow you to specify an item that falls inline below a parent label or key, what happens when you want to include more specific attributes to that parent label, or if you want to introduce more details such as key-value pair mappings to your list? In that occasion, what is used, and what you will see regularly in just about every network vars file, is a dictionary. As previously described, a dictionary is a list of key-value mappings.</p> <p>Here is an example of a dictionary:</p> <pre><code>---\nl3spine:\n  defaults:\n    platform: 720XP\n    spanning_tree_mode: rapid-pvst\n    spanning_tree_priority: 4096\n    loopback_ipv4_pool: 10.0.0.0/24\n    mlag_peer_ipv4_pool: 192.0.0.0/24\n    mlag_peer_l3_vlan: false\n    virtual_router_mac_address: aa:aa:bb:bb:cc:cc\n    mlag_interfaces: [Ethernet53, Ethernet54]\n    mlag_port_channel_id: 2000\n</code></pre>"},{"location":"jinja-yaml/#nested-data-structures","title":"Nested Data Structures","text":"<p>One of the double-edged parts of YAML is that it can be quite complex. This is good because it is very flexible for how we can build our data model, but it also means that model can get out of control. YAML syntax is hierarchical, and at the same time, just about every construct we covered previous to this can be nested within each other. Let's take a look at some nested data structures that illustrate this.</p> <p>In this first example we will have a list of dictionaries:</p> Reminder <p> Indentation is key.</p> <pre><code>---\n# local users\n\nlocal_users:\n  admin:\n    privilege: 15\n    role: network-admin\n    sha512_password: encrypted_pass\n  cvpadmin:\n    privilege: 15\n    role: network-admin\n    sha512_password: encrypted_pass\n  ansible_user:\n    privilege: 15\n    role: network-admin\n    sha512_password: encrypted_pass\n  noc:\n    privilege: 1\n    role: network-operator\n    sha512_password: encrypted_pass\n</code></pre> <p>The following example will show another network device specific nested data structure which mixes dictionaries and lists in different places:</p> <pre><code>---\nntp:\n  local_interface:\n    name: Management1\n    vrf: MGMT\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      preferred: true\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n\nsvis:\n  10:\n    name: 'DATA'\n    tags: ['Web']\n    enabled: true\n    ip_virtual_router_addresses:\n      - 10.10.10.1\n    mtu: 9214\n    ip_helpers:\n      10.100.100.20:\n        source_interface: Management1\n        source_vrf: MGMT\n      127.0.0.1:\n    nodes:\n      SW-CORE-A:\n        ip_address: 10.10.10.2/23\n      SW-CORE-B:\n        ip_address: 10.10.10.3/23\n</code></pre>"},{"location":"jinja-yaml/#yaml-file-examples","title":"YAML File Examples","text":"<p>Now that we have gone through all the most common constructs within a YAML file used for variables for network device configs, let's see what some complete files would look like for a real world environment.</p>"},{"location":"jinja-yaml/#network-config-example","title":"Network Config Example","text":"<p>These example YAML files could be used to build a base config for a series of devices. The devices would be based on your inventory file, however, for this example, we will assume there is a layer2 leaf/spine topology, with two spines and two leafs. For this example we will use two files, a <code>global.yml</code> and a <code>interface.yml</code> file.</p> <p>The global.yml file that follows includes the data model used for the base configuration. These items apply to all four devices in the fabric. ( This could be imported in the playbook, or put in the <code>group_vars</code> directory and named after the level of your hierarchy that contains the devices you want this to apply to. )</p> <p><code>global.yml</code></p> <pre><code>---\n# local users\nlocal_users:\n  admin:\n    privilege: 15\n    role: network-admin\n    secret: aristaadmin\n  noc:\n    privilege: 1\n    role: network-operator\n    secret: aristaops\n\n# aaa authentication and authorization\naaa_authentication:\n  login:\n    default: group radius local\n\naaa_authorization:\n  exec:\n    default: group radius local\n\n# radius servers\nradius_servers:\n  - host: 192.168.1.10\n    vrf: MGMT\n    key: 055A0A5D311E1E5B4944\n\n# HTTP Client source interface and VRF\nip_http_client_source_interfaces:\n    - name: Management1\n      vrf: MGMT\n\n# RADIUS source interface and VRF\nip_radius_source_interfaces:\n  - name: Management1\n    vrf: MGMT\n\n#MAC and ARP aging timers\nmac_address_table:\n  aging_time: 1800\n\narp:\n  aging:\n    timeout_default: 1500\n\n# DNS Servers\nname_servers:\n  - 10.100.100.20\n\n# DNS lookup source interface (Servers defined in 1L2P.yml)\nip_domain_lookup:\n  source_interfaces:\n    Management1:\n      vrf: MGMT\n\n# NTP Servers (source interface defined in group specific YML files (CORE, ACCESS, MGMT, INET)\nntp:\n  local_interface:\n    name: Management1\n    vrf: MGMT\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      preferred: true\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: time.google.com\n      vrf: MGMT\n\nclock:\n  timezone: \"America/Detroit\"\n</code></pre> <p><code>interface.yml</code></p> <pre><code>---\nspine1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet48:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet1:\n      desc: TO_LEAF1\n      mlag_peerlink: false\n    Ethernet2:\n      desc: TO_LEAF2\n      mlag_peerlink: false\nspine2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_LEAF1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_LEAF2'\n      mlag_peerlink: false\nleaf1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\nleaf2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\n</code></pre>"},{"location":"jinja-yaml/#jinja","title":"Jinja","text":""},{"location":"jinja-yaml/#what-is-jinja","title":"What is Jinja?","text":"<p>At a high level, Jinja is a templating engine used to create markup files such as HTML or XML as well as custom text files, such as in our instance config files. Under the hood, Jinja is an open source python library that let's you create extensible templates. One of the major benefits of Jinja is that the template files you create allow you to define both static text, as well as variables. Some of the Jinja template syntax may look familiar because Jinja is not the first templating engine, and is actually inspired by Django.</p>"},{"location":"jinja-yaml/#what-is-jinja-used-for","title":"What is Jinja Used For?","text":"<p>As it may have become apparent from the YAML section, after creating our data model of the various configuration parameters we want to automate, we need to get that data model and all its variables into a format that can be read and understood by our network devices. This is where Jinja comes in. The use of Jinja templates, along with some yet to be shown Ansible magic, allows us to render full or partial configuration files that can loaded onto network devices. The underlying purpose of this, as it relates to automation, is that with the use of various expressions and variables in the Jinja templates, we can use a single template, with single or multiple YAML variable files, and create configurations against a multitude of network devices. As far as the actual template file and its file extension, technically any file can be called as a template regardless of its extension as long as its formatted correctly, however, we typically use the <code>.j2</code> extension on all Jinja template files.</p>"},{"location":"jinja-yaml/#jinja-syntax","title":"Jinja Syntax","text":"<p>As a foreword to getting into the different tasks we can accomplish in our Jinja templates, it's important to call out that there are a few common expressions that are used throughout all Jinja templates, including those related to network devices. They are outlined below.</p> <p><code>Comments:</code></p> <p>Comments are represented as such, with our friend the pound symbol:</p> <pre><code>{# Automation Is Fun #}\n</code></pre> <p><code>Expressions/Variables</code></p> <p>Expressions or variables are represented with a pair of curly brackets:</p> <pre><code> {{ inventory_hostname }}\n</code></pre> <p><code>Statements</code></p> <p>Statements are represented with a percent symbol:</p> <pre><code>{% for items in vars_file['interfaces'] %}\n{% endfor %}\n</code></pre> Live Jinja Parser <p>Quick and easy method for testing data models and Jinja syntax: https://j2live.ttl255.com/</p>"},{"location":"jinja-yaml/#inventory-file-and-ansible-playbook","title":"Inventory File and Ansible Playbook","text":"<p>While we won't cover the inventory file or Ansible playbooks in depth in this section as it will be covered in Ansible, it is important to call out it's importance in relation to Jinja. You may wonder, when using Ansible to automate and render any configurations, how does it know what devices I want to create configurations for? This is accomplished with the Ansible inventory file. This will be covered more thoroughly, however, for the purpose of the following examples, we will assume we have an inventory file with four hosts, <code>spine1</code>, <code>spine2</code>, <code>leaf1</code>, and <code>leaf2</code>. Some examples may show output for all four devices, and some may show output for just one, depending on the complexity of the example. Also, every example we show will use the following Ansible Playbook so you understand the destination filename syntax. There are more parts to this playbook but they will be shown at the end and covered in the next presentation:</p> <pre><code>---\n# Inventory File\nfabric:\n  spines:\n    spine1:\n    spine2:\n  leafs:\n    leaf1:\n    leaf2:\n</code></pre> <pre><code>---\n#Config Playbook\n- hosts: spine1,spine2,leaf1,leaf2\n  gather_facts: false\n  - name: Register variables\n    include_vars:\n      file: \"{{lookup('env','PWD')}}/vars/global.yml\"\n      name: global\n  - name: Register variables\n    include_vars:\n      file: \"{{lookup('env','PWD')}}/vars/interface.yml\"\n      name: interface\n  - name: Create configs\n    template:\n      src: \"{{lookup('env','PWD')}}/templates/full_config.j2\"\n      dest: \"{{lookup('env','PWD')}}/configs/full_config/{{inventory_hostname}}_config.cfg\"\n</code></pre>"},{"location":"jinja-yaml/#variable-substitution","title":"Variable Substitution","text":"<p>As shown previously, we know expressions or variable substitution is performed with the double curly brackets, <code>{{ my_var }}</code>, but what does this look like in a Jinja template?</p> <p>For example, we may want to generate the hostname in our template for all the devices in our inventory file. In order to do this we can use a standard Ansible variable called <code>inventory_hostname</code>, which substitutes the name of the current inventory host the Ansible play is running against.</p> <pre><code>{# Create a file assigning the device hostname #}\n\nhostname {{ inventory_hostname }}\n</code></pre> <p>Assuming our pre-defined inventory file, running Ansible against this template with the relevant YAML file called would yield four different output files:</p> <pre><code>spine1_config.cfg\nspine2_config.cfg\nleaf1_config.cfg\nleaf2_config.cfg\n</code></pre> <p>The output of one of these files all listed below would be as follows:</p> <pre><code>#spine1_config.cfg\nhostname spine1\n\n#spine2_config.cfg\nhostname spine2\n\n#leaf1_config.cfg\nhostname leaf1\n\n#leaf2_config.cfg\nhostname leaf2\n</code></pre> <p>How about a more complex variable substitution using something from one of our data models above. This shows how to substitute for a single dictionary item:</p> <pre><code># Global.yml\naaa_authentication:\n  login:\n    default: group radius local\n\naaa_authorization:\n  exec:\n    default: group radius local\n\nclock:\n  timezone: \"America/Detroit\"\n</code></pre> <p>The Jinja template to call these variables is shown here:</p> <pre><code># Render aaa authC config line\naaa authentication login default {{ global['aaa_authentication']['login']['default'] }}\n\n# Render aaa authZ config line\naaa authorization exec default {{ global['aaa_authorization']['exec']['default'] }}\n\n# Render clock timezone config line\nclock timezone {{ global['clock']['timezone'] }}\n</code></pre> <p>As you can see, the variable has many parameters in them. Let's walk through these parameters using the <code>aaa authentication</code> config line.</p> <p><code>global</code>:  Global is the name of the vars file when we registered it in our Ansible playbook. You can register this as anything you want.</p> <p><code>aaa_authentication</code>:  This is the name of the parent or top level label or key in our dictionary. This tells the Jinja template the next level to look for the variable we substitute.</p> <p><code>login</code>: This is another label or key down the dictionary, and again tell us where to keep looking for the final variable.</p> <p><code>default</code>:  This is the key-value pair mapping in the dictionary that we want assigned as the variable. We see this is the final parameter in our variable substitution because we want the value of that key to be the result.</p> <p>Running the playbook generates the following configuration against all devices called in the inventory file:</p> Configuration Output <pre><code># AAA authC Login\naaa authentication login default group radius local\n\n# AAA authZ Login\naaa authorization exec default group radius local\n\n# Device Timezone\nclock timezone America/Detroit\n</code></pre>"},{"location":"jinja-yaml/#conditionals-and-loops","title":"Conditionals and Loops","text":""},{"location":"jinja-yaml/#conditionals","title":"Conditionals","text":"<p>Conditionals, such as <code>{% if %}</code>, <code>{% elif %}</code>, and <code>{% else %}</code>, as well as <code>{% for %}</code> loops are extremely helpful for either configurations that may apply to only a subset of devices you are generating configurations for. Additionally, for loops are a must to efficiently work through nested data structures like lists of lists or lists of dictionaries.</p> <p>Let's start our statements journey with conditionals, and where they can be helpful.</p> <p>In this first example we will look at the following portion of our <code>interfaces.yml</code> YAML vars file:</p> <pre><code>---\nspine1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet48:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet1:\n      desc: TO_LEAF1\n      mlag_peerlink: false\n    Ethernet2:\n      desc: TO_LEAF2\n      mlag_peerlink: false\nspine2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_LEAF1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_LEAF2'\n      mlag_peerlink: false\n</code></pre> <p>The following dictionary key is what we are going to pay close attention to for our Jinja template: <code>mlag_side</code></p> <p>Here we will use a Jinja template to create a partial, correct MLAG configuration per spine device, based on which side it is.</p> <pre><code>{% if interface[inventory_hostname]['mlag_side'] == 'A' %}\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n\n{% else %}\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n\n{% endif %}\n</code></pre> <p>Reviewing this template, we can see we are using a conditional based on which MLAG side the device is. If the device belongs to side A, it will apply the matching configuration. If the device belongs to side B it will apply the other configuration.</p> <p>Here is the output of running the Ansible playbook and the configs that are generated:</p> spine1 Output <pre><code>int vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n</code></pre> spine2 Output <pre><code>int vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n</code></pre> <p>Another aspect of conditionals to match on is a boolean, whether something is true or false, using the following example, also referencing our <code>interfaces.yml</code>:</p> <pre><code>{% for name, intf in interface[inventory_hostname]['interfaces'].items() %}\ninterface {{ name }}\ndescription {{ intf['desc'] }}\n{% if intf['mlag_peerlink'] == true %}\nchannel-group 2000 mode active\n{% endif %}\n\n{% endfor %}\n</code></pre> <p>Ignoring for a moment the for loop part we haven't covered yet, we can see we are checking each interface that is defined to see if the interface parameter for <code>mlag_peerlink</code> is set to true, if it is, we want to apply an extra line of configuration, <code>channel-group 2000 mode active</code>, if it's not, we just configure the specified interface with a description. The output of running this against the spines would be as follows:</p> spine1 Output <pre><code>interface Ethernet47\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet1\ndescription TO_LEAF1\n\ninterface Ethernet2\ndescription TO_LEAF2\n</code></pre> spine2 Output <pre><code>interface Ethernet47\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet1\ndescription TO_LEAF1\n\ninterface Ethernet2\ndescription TO_LEAF2\n</code></pre>"},{"location":"jinja-yaml/#loops","title":"Loops","text":"<p>Another great function that Jinja templates support is the use of <code>for</code> loops. For loops come in handy when trying to iterate through a list of dictionaries to repeat configuration lines.</p> <p>Let's start with an example using a <code>for</code> loop to iterate through a list. For example, the below list shows a list with a single DNS server:</p> <pre><code># DNS Servers\nname_servers:\n  - 10.100.100.20\n</code></pre> <p>The template with a for loop to iterate through this list would be as follows:</p> <pre><code>{% for dns in global['name_servers'] %}\nip name-server {{ dns }}\n{% endfor %}\n</code></pre> <p>Let's analyze the sections of this template.</p> <p><code>dns</code>:  DNS is a variable that we are using to represent each item in the list. This can be anything you wish.</p> <p><code>global</code>:  If you recall, this is the name of the YAML file we are registering in our playbook. This tells the template which YAML file to look at for the variable.</p> <p><code>name_servers</code>:  This is the parent label or key of the list. This says which items in the list we want to iterate through and assign to the variable we defined.</p> <p>After assigning the value in the list to our created variable, we issue our configuration line which contains static text and our variable. The output would look as follows:</p> <pre><code>ip name-server 10.100.100.20\n</code></pre> <p>What if we had multiple items in the list like this:</p> <pre><code># DNS Servers\nname_servers:\n  - 10.100.100.20\n  - 8.8.8.8\n  - 4.4.4.4\n  - 208.67.222.222\n</code></pre> <p>The for loop would run as many times as there are items in the list and the configuration file output would look as follows:</p> <pre><code>ip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n</code></pre> <p>Now let's take a look at a slightly more complex, nested data structure, such as a dictionary with a list item. We will use the following portion from our data model:</p> <pre><code># radius servers\nradius_servers:\n  - host: 192.168.1.10\n    vrf: MGMT\n    key: radiusserverkey\n</code></pre> <p>The template with a for loop to iterate through this list would be as follows:</p> <pre><code>{% for rsrv in global['radius_servers'] %}\nradius-server host {{ rsrv['host'] }} vrf {{ rsrv['vrf'] }} key {{ rsrv['key'] }}\n{% endfor %}\n</code></pre> <p>Let's analyze the sections of this template.</p> <p><code>rsrv</code>:  This variable represents an item in the dictionary <code>radius_servers</code> as the dictionary is looped over.</p> <p><code>global</code>: This tells the template which YAML file to look at for the variable.</p> <p>Looking at the configuration line we created, we see instead of walking through the dictionary via the dictionary key names, we key off our variable which represents the items in our dictionary. This can be seen with the <code>rsrv['hosts']</code> line. This means we are looking for the value of the <code>host</code> key for each server in our list that is currently assigned to the <code>rsrv</code> variable. The same holds true for the <code>rsrv['vrf']</code> and <code>rsrv['key']</code> lines.</p> <p>The for loop would run as many times as there are items in the list, which is just one, and the configuration file output would look as follows:</p> <pre><code>radius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n</code></pre> <p>While that was simple, what if we have something more complex, like a dictionary with a list of dictionaries? Let's take a look at how this would be represented in Jinja using the following YAML data model from our <code>global.yml</code> file.</p> <pre><code>ntp:\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: time.google.com\n      vrf: MGMT\n</code></pre> <p>The template with a for loop to iterate through this list would be as follows:</p> <pre><code>{% for ntps in global['ntp']['servers'] %}\nntp server vrf {{ ntps['vrf'] }} {{ ntps['name'] }}\n{% endfor %}\n</code></pre> <p>Let's analyze the sections of this template.</p> <p><code>ntps</code>:  This is another variable representing an item in the dictionary servers as the dictionary is looped over.</p> <p><code>global</code>: This tells the template which YAML file to look at for the variable.</p> <p>Looking at the configuration line we created, we see instead of walking through the dictionary via the dictionary key names, we key off our variable which represents the items in our dictionary. This can be seen with the <code>ntps['vrf']</code> line. This means we are looking for the value of the <code>vrf</code> key for each server in our list that is currently assigned to the <code>ntps</code> variable. The same holds true for the <code>ntps['name']</code> line.</p> <p>The for loop would run as many times as there are items in the list and the configuration file output would look as follows:</p> <pre><code>ntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n</code></pre> <p>In the previous examples we only covered single for loops iterating a single layer deep, however, what if we have a dictionary within a dictionary, which contains a list? In that instance we would need to have nested for loops in our templates. For this example we will introduce a new YAML data model to better illustrate nested for loops. This section shows some configuration you may need to apply related to EVPN VXLAN:</p> <pre><code>---\n# EVPN VXLAN vrfs\nRed:\n  l3vni: 10000\n  vlans:\n    101:\n      l2vni: 10001\n      anycast_gw: 10.10.10.1/24\n    102:\n      l2vni: 10002\n      anycast_gw: 10.10.20.1/24\nBlue:\n  l3vni: 20000\n  vlans:\n    201:\n      l2vni: 20001\n      anycast_gw: 10.20.10.1/24\n    202:\n      l2vni: 20002\n      anycast_gw: 10.20.20.1/24\n</code></pre> <p>Reviewing this data model we can see we have two dictionaries that represent different VRFs, <code>Red</code> and <code>Blue</code>. Within each of those dictionaries we have another dictionary called <code>vlans</code>, which defines the interfaces in each of those VRFs. When performing the configuration of a VXLAN interface we need to define both the l3vni, as well as each VLAN to VXLAN mapping. Then we need to loop through each of the parent keys followed by each of the items in <code>vlans</code> dictionary.</p> <p>The template to accomplish this would look like this:</p> <pre><code>{# Leaf evpn config #}\ninterface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789\n\n{% for vrf,vrf_values in vrf.items() %}\n\nvxlan vrf {{ vrf }} vni {{ vrf_values['l3vni'] }}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n\nvxlan vlan {{ vlan }} vni {{ vlan_values['l2vni'] }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Looking at this template, let's walk through what's happening. First, we have a for loop that is iterating through the top level dictionary keys in the YAML file itself. Those keys would be <code>Red</code> and <code>Blue</code>. Looking at the actual for loop syntax itself, we have some new parameters to look at:</p> <p><code>vrf</code>:  This is a variable the keys in the dictionary list are assigned to.</p> <p><code>vrf_values</code>:  This is another variable the corresponding values of those keys are assigned to.</p> <p><code>vrf.items()</code>: This uses the built in .items function to determine which items in the dictionary we are looping through. It includes the top level keys.</p> <p>Using those parameters, we construct our configuration line, which includes the variable <code>vrf</code>, the key, and the value for the pair <code>l3vni</code>.</p> <p>After this we repeat the same loop logic, but for the nested dictionary. In this instance we create a new set of variables for the keys and values, only this time, those are assigned the values within the <code>vlans</code> dictionary, as called by the <code>vrf_values['vlans'].items()</code> parameter.</p> <p>Before we view the final output, let's look at what the variable output actually is. This will both make it clearer what they represent based on the data model, as well as show you a nice way to troubleshoot your template while you are working on it.</p> <p>First we will look at the <code>vrf</code> variables, and we will do this by changing the template to this:</p> <pre><code>{# Leaf evpn config #}\n{# interface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789 #}\n{% for vrf,vrf_values in vrf.items() %}\n{# vxlan vrf {{ vrf }} vni {{ vrf_values['l3vni'] }} #}\n{{ vrf }}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n{# vxlan vlan {{ vlan }} vni {{ vlan_params['l2vni'] }} #}\n{{ vlan }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Reviewing the above template, we will comment out everything except our for loop and our initial key variables. The result of this template will run through the nested loops, and output the value of the variables called, <code>{{ vrf }}</code>, and <code>{{ vlan }}</code>.</p> <p>Viewing the output, we can see that these equal the top level keys of our data model:</p> <pre><code>Red\n\n101\n\n102\n\n\nBlue\n\n201\n\n202\n</code></pre> <p>The first for loop runs and you can see <code>vrf=Red</code>. Then the nested loop runs twice due to there being two VLANs, and we can see both VLAN values. This then repeats for the second top level key, <code>Blue</code>.</p> <p>Now let's take a look at the <code>_params</code> variable. Changing the template to what follows and commenting out our initial variable, and adding a substitution line for the <code>_params</code> variable:</p> <pre><code>{# Leaf evpn config #}\n{# interface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789 #}\n{% for vrf,vrf_values in vrf.items() %}\n{# vxlan vrf {{ vrf }} vni {{ vrf_values['l3vni'] }} #}\n{# {{ vrf }} #}\n{{ vrf_values }}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n{# vxlan vlan {{ vlan }} vni {{ vlan_params['l2vni'] }} #}\n{# {{ vlan }} #}\n{{ vlan_values }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Viewing the output, we can see that these equal the dictionary key-pair values of the nested <code>vlan</code> dictionary:</p> <pre><code>{'l3vni': 10000, 'vlans': {101: {'l2vni': 10001, 'anycast_gw': '10.10.10.1/24'}, 102: {'l2vni': 10002, 'anycast_gw': '10.10.20.1/24'}}}\n\n{'l2vni': 10001, 'anycast_gw': '10.10.10.1/24'}\n\n{'l2vni': 10002, 'anycast_gw': '10.10.20.1/24'}\n\n\n{'l3vni': 20000, 'vlans': {201: {'l2vni': 20001, 'anycast_gw': '10.20.10.1/24'}, 202: {'l2vni': 20002, 'anycast_gw': '10.20.20.1/24'}}}\n\n{'l2vni': 20001, 'anycast_gw': '10.20.10.1/24'}\n\n{'l2vni': 20002, 'anycast_gw': '10.20.20.1/24'}\n</code></pre> <p>Reverting back to our desired template, we can see what the template and actual output would be:</p> <pre><code>{# Leaf evpn config #}\ninterface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789\n\n{% for vrf,vrf_values in vrf.items() %}\n\nvxlan vrf {{ vrf }} vni {{ vrf_values['l3vni'] }}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n\nvxlan vlan {{ vlan }} vni {{ vlan_values['l2vni'] }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <pre><code>interface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789\nvxlan vrf Red vni 10000\nvxlan vlan 101 vni 10001\nvxlan vlan 102 vni 10002\nvxlan vrf Blue vni 20000\nvxlan vlan 201 vni 20001\nvxlan vlan 202 vni 20002\n</code></pre>"},{"location":"jinja-yaml/#filters","title":"Filters","text":"<p>The final section we will cover will be filters. While there are an enormous amount of filters available, we will just cover a very common one, the <code>ipaddr</code> filter. In a simple explanation, the <code>ipaddr</code> filter takes a full IP address and subnet mask, and strips off just the mask, with the end result being only the address. This can be helpful in an instance where you are using a full prefix and mask in your data model and don't want to create a new, duplicate, key-value pair mapping to be called.</p>"},{"location":"jinja-yaml/#ipaddr-filter","title":"IPADDR() Filter","text":"<p>In a simple explanation, the <code>ipaddr</code> filter has various operations allowing you to manipulate a prefix or network and obtain certain information about it. This can be helpful in an instance where you are using a full prefix and mask in your data model and don't want to create a new, duplicate, key-value pair mapping to be called.</p> <p>The following are some of the functions available within the ipaddr() filter:</p> <p><code>address:</code> When using a prefix in x.x.x.x/yy notation, this filter will pull only the address portion.</p> <p><code>network:</code> This filter will calculate the network ID of a given prefix.</p> <p><code>netmask:</code> This filter will generate the subnet mask from /yy prefix/CIDR notation.</p> <p><code>broadcast:</code> This filter will calculate the broadcast address of a given prefix.</p> <p><code>size:</code> This filter will calculate the size of a subnet based on the subnet mask.</p> <p><code>range_usable:</code> This filter finds the range of usable addresses within a subnet.</p> <p>To illustrate this, we will use a simple example for showing the IP info of these two interfaces run against a single device, spine1.</p> <p>The data model is as follows:</p> <pre><code>---\nspine1:\n  interfaces:\n    Ethernet1:\n      ipv4: 1.1.1.1/26\n    Ethernet2:\n      ipv4: 2.2.2.2/8\n</code></pre> <p>The Jinja template is as follows, which will use the above described filters to give us various information about the IP addresses assigned to Ethernet1 and Ethernet2:</p> <pre><code>Here is some information about some of Spine1's interfaces:\n\nEthernet1\n    IP: {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('address')  }}\n    Subnet Mask:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('netmask')  }}\n    Network ID:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('network')  }}\n    Network Size:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('size')  }}\n    Usable Range:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('range_usable')  }}\n    Broadcast Address:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('broadcast')  }}\n\nEthernet2\n    IP: {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('address')  }}\n    Subnet Mask:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('netmask')  }}\n    Network ID:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('network')  }}\n    Network Size:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('size')  }}\n    Usable Range:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('range_usable')  }}\n    Broadcast Address:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('broadcast')  }}\n</code></pre> <p>Reviewing our template, we run the playbook against the current inventory device keying in on the ipv4 address mapping. We then use the filter command, <code>|</code>, and specify the <code>address</code> keyword, meaning we only want the address part of the whole prefix.</p> <p>Running the playbook results in the following output:</p> <pre><code>Here is some information about some of Spine1's interfaces:\n\nEthernet1\n    IP: 1.1.1.1\n    Subnet Mask:  255.255.255.192\n    Network ID:  1.1.1.0\n    Network Size:  64\n    Usable Range:  1.1.1.1-1.1.1.62\n    Broadcast Address:  1.1.1.63\n\nEthernet2\n    IP: 2.2.2.2\n    Subnet Mask:  255.0.0.0\n    Network ID:  2.0.0.0\n    Network Size:  16777216\n    Usable Range:  2.0.0.1-2.255.255.254\n    Broadcast Address:  2.255.255.255\n</code></pre>"},{"location":"jinja-yaml/#join-filter","title":"JOIN() Filter","text":"<p>When working with lists in YAML, it may be necessary to concatenate list items into a single line configuration command, instead of looping through the list and creating a configuration command for each item. Some examples of this could be configuring a list of NTP or DNS servers in a single line, versus individual entries. This can be accomplished with the following filter:</p> <p><code>join(\" \"):</code> This filter joins all the items in the list. Pay close attention to the space between the double quotes. This provides spacing between the items in a list. Without this space, all items in the list would be joined into one continuous string.</p> <p>To illustrate this, we will use simple DNS server data model and Jinja template.</p> <p>The data model is as follows:</p> <pre><code>---\n# DNS Servers\nname_servers:\n  - 10.100.100.20\n  - 8.8.8.8\n  - 4.4.4.4\n  - 208.67.222.222\n</code></pre> <p>The following Jinja template will allow us to configure all four DNS servers on a single line for a single configuration command:</p> <pre><code>ip name-server {{ global['name_servers'] | join(\" \") }}\n</code></pre> <p>Running the playbook results in the following output:</p> <pre><code>ip name-server 10.100.100.20 8.8.8.8 4.4.4.4 208.67.222.222\n</code></pre>"},{"location":"jinja-yaml/#the-jinja-yaml-relationship","title":"The Jinja YAML Relationship","text":"<p>As has hopefully been explained well, the interdependent relationship between Jinja and YAML is that Jinja templates utilize the YAML vars files and their data model to generate new configurations, whether full or partial. YAML files serve no purpose without being called and their variables used, and configurations rendered from Jinja templates would be no different than static configs without the variable substitution of YAML files.</p>"},{"location":"jinja-yaml/#jinja-templates","title":"Jinja Templates","text":""},{"location":"jinja-yaml/#network-config-template-example","title":"Network Config Template Example","text":"<p>Using all the above sections we have reviewed, let's put it all together into a template called <code>full_config.j2</code>, which renders an entire device config based on the <code>global.yml</code> and <code>interface.yml</code> YAML files.</p> <p>The Jinja template would look as follows:</p> full_config.j2 <pre><code>{# Full Config #}\n\nhostname {{ inventory_hostname }}\n\nusername admin priv {{ global['local_users']['admin']['privilege'] }} role {{ global['local_users']['admin']['role'] }} secret {{ global['local_users']['admin']['secret'] }}\n\nusername noc priv {{ global['local_users']['noc']['privilege'] }} role {{ global['local_users']['noc']['role'] }} secret {{ global['local_users']['noc']['secret'] }}\n\naaa authentication login default {{ global['aaa_authentication']['login']['default'] }}\n\naaa authorization exec default {{ global['aaa_authorization']['exec']['default'] }}\n\n{% for rsrv in global['radius_servers'] %}\nradius-server host {{ rsrv['host'] }} vrf {{ rsrv['vrf'] }} key {{ rsrv['key'] }}\n{% endfor %}\n\n{% for httpsrc in global['ip_http_client_source_interfaces'] %}\nip http client local-interface {{ httpsrc['name'] }} vrf {{ httpsrc['vrf'] }}\n{% endfor %}\n\n{% for radsrc in global['ip_radius_source_interfaces'] %}\nip radius vrf {{ radsrc['vrf'] }} source-interface {{ radsrc['name'] }}\n{% endfor %}\n\nmac address-table aging-time {{ global['mac_address_table']['aging_time'] }}\n\narp aging timeout {{ global['arp']['aging']['timeout_default'] }}\n\n{% for dns in global['name_servers'] %}\nip name-server {{ dns }}\n{% endfor %}\n\nip domain lookup vrf {{ global['ip_domain_lookup']['source_interfaces']['Management1']['vrf'] }} source-interface Management1\n\n\n{% for ntps in global['ntp']['servers'] %}\nntp server vrf {{ ntps['vrf'] }} {{ ntps['name'] }}\n{% endfor %}\n\nclock timezone {{ global['clock']['timezone'] }}\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\n{% for name, intf in interface[inventory_hostname]['interfaces'].items() %}\ninterface {{ name }}\ndescription {{ intf['desc'] }}\n{% if intf['mlag_peerlink'] == true %}\nchannel-group 2000 mode active\n{% endif %}\n\n{% endfor %}\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\n{% if interface[inventory_hostname]['mlag_side'] == 'A' %}\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n{% else %}\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n{% endif %}\n</code></pre>"},{"location":"jinja-yaml/#final-output-tying-it-all-together","title":"Final Output - Tying It All Together","text":"<p>Let's put our full YAML data models below, as well as the output that is generated for <code>spine1-2</code> and <code>leaf1-2</code>:</p> global.yml <pre><code># local users\nlocal_users:\n  admin:\n    privilege: 15\n    role: network-admin\n    secret: aristaadmin\n  noc:\n    privilege: 1\n    role: network-operator\n    secret: aristaops\n\n# aaa authentication and authorization\naaa_authentication:\n  login:\n    default: group radius local\n\naaa_authorization:\n  exec:\n    default: group radius local\n\n# radius servers\nradius_servers:\n  - host: 192.168.1.10\n    vrf: MGMT\n    key: radiusserverkey\n\n# HTTP Client source interface and VRF\nip_http_client_source_interfaces:\n    - name: Management1\n      vrf: MGMT\n\n# RADIUS source interface and VRF\nip_radius_source_interfaces:\n  - name: Management1\n    vrf: MGMT\n\n#MAC and ARP aging timers\nmac_address_table:\n  aging_time: 1800\n\narp:\n  aging:\n    timeout_default: 1500\n\n# DNS Servers\nname_servers:\n  - 10.100.100.20\n  - 8.8.8.8\n  - 4.4.4.4\n  - 208.67.222.222\n\n# DNS lookup source interface (Servers defined in 1L2P.yml)\nip_domain_lookup:\n  source_interfaces:\n    Management1:\n      vrf: MGMT\n\n# NTP Servers (source interface defined in group specific YML files (CORE, ACCESS, MGMT, INET)\nntp:\n  local_interface:\n    name: Management1\n    vrf: MGMT\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: time.google.com\n      vrf: MGMT\n\nclock:\n  timezone: \"America/Detroit\"\n</code></pre> interface.yml <pre><code>---\nspine1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet48:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet1:\n      desc: TO_LEAF1\n      mlag_peerlink: false\n    Ethernet2:\n      desc: TO_LEAF2\n      mlag_peerlink: false\nspine2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_LEAF1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_LEAF2'\n      mlag_peerlink: false\nleaf1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\nleaf2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\n</code></pre> <p>And the configuration outputs:</p> spine1_config.cfg <pre><code>hostname spine1\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_LEAF1\n\n\n\ninterface Ethernet2\ndescription TO_LEAF2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n</code></pre> spine2_config.cfg <pre><code>hostname spine2\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_LEAF1\n\n\n\ninterface Ethernet2\ndescription TO_LEAF2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n</code></pre> leaf1_config.cfg <pre><code>hostname leaf1\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_LEAF2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_LEAF2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_SPINE1\n\n\n\ninterface Ethernet2\ndescription TO_SPINE2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n</code></pre> leaf2_config.cfg <pre><code>hostname leaf2\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_LEAF1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_LEAF1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_SPINE1\n\n\n\ninterface Ethernet2\ndescription TO_SPINE2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n</code></pre>"},{"location":"vscode/","title":"Welcome to VS Code","text":""},{"location":"vscode/#what-is-vs-code","title":"What is VS Code?","text":"<p>Visual Studio Code (VS Code), written by Microsoft, is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages and runtime environments (such as C++, C#, Java, Python, PHP, Go, .NET).</p> <p>VS Code integrates directly with Git and allows Network Engineers to efficiently perform the following tasks:</p> <ul> <li>Edit Files</li> <li>Drag and Drop Files into the Explorer</li> <li>Visually Compare File Diffs</li> <li>Write &amp; Test Scripts</li> <li>Open Terminals for various shells</li> <li>Execute various Git commands (such as: status, init, log, commit, switch, add, etc...)</li> <li>Connect to Remote Hosts</li> </ul> <p>VS Code is your personal Git Genie making automation workflows effortless. Once you learn to use VS Code, you will wonder why it took so long to get on the bandwagon.</p>"},{"location":"vscode/#system-requirements","title":"System Requirements","text":"<p>Visual Studio Code is a small download (&lt; 200 MB) and has a disk footprint of &lt; 500 MB. VS Code is lightweight and should run on today's hardware.</p> <p>Recommend Hardware:</p> <ul> <li>1.6 GHz or faster processor</li> <li>1 GB of RAM</li> </ul> <p>Support Platforms:</p> <ul> <li>macOS X (10.11+)</li> <li>Windows 8.0, 8.1 and 10, 11 (32-bit and 64-bit)</li> <li>Linux (Debian): Ubuntu Desktop 16.04, Debian 9</li> <li>Linux (Red Hat): Red Hat Enterprise Linux 7, CentOS 7, Fedora 34</li> </ul>"},{"location":"vscode/#download-installation","title":"Download &amp; Installation","text":"<p>VS Code downloads for Windows, Linux and Mac can be found here.</p>"},{"location":"vscode/#vs-code-gui-walk-through","title":"VS Code GUI Walk-through","text":"<p>The four main areas of the VS Code GUI are:</p> <ul> <li>Activity Bar</li> <li>Status Bar</li> <li>Editor</li> <li>Terminal Window</li> </ul> <p></p> Note <p>Your <code>Activity Bar</code> may have different icons for the extensions you have installed.</p>"},{"location":"vscode/#extensions","title":"Extensions","text":"<p>There are tons of extensions to enhance your experience with VS Code. Below is a list of recommended extensions that are commonly used. For a full list of available extensions visit the VS Code Marketplace.</p> <ul> <li>Markdown All-in-One (Yu Zhang)</li> <li>YAML (Red Hat)</li> <li>Better Jinja (Samuel Colvin)</li> <li>Peacock (John Papa)</li> <li>Remote - SSH (Microsoft)</li> <li>Development Containers (Microsoft)</li> </ul>"},{"location":"vscode/#using-git-source-control-in-vs-code","title":"Using Git source control in VS Code","text":"<p>Visual Studio Code has integrated source control management (SCM) and includes Git support out-of-the-box. In the previous section we ran all Git commands from the command-line. Now we will use VS Code to perform the same operations.</p>"},{"location":"vscode/#start-over-with-clean-slate","title":"Start over with clean slate","text":"<p>Previously we downloaded and created a repository called <code>samplefiles</code>. Reset this repository to a directory by removing the hidden sub-directory <code>.git</code>.  This will remove any version control settings for the repository.</p> <pre><code>cd /home/coder/project/labfiles/samplefiles\nrm -rf .git\n</code></pre>"},{"location":"vscode/#the-basics","title":"The Basics","text":""},{"location":"vscode/#initialize-repository","title":"Initialize Repository","text":"<p>There are multiple ways to initialize a repository from VS Code. We will explore one of the methods.</p> <p>First open the folder <code>/home/coder/project/labfiles/samplefiles</code> from within the VS Code Explorer.</p> <p></p> <p>Re-open a new terminal window.</p> <p>Next click on the <code>Source Control</code> icon in the Activity Bar, and then click <code>Initialize Repository</code>.</p> <p></p> <p>This is equivalent to running <code>git init</code> from within that directory.</p> <p>Several things just happened. VS Code gives us a visual representation of the newly created Git repository. The <code>Source Control</code> icon now shows a blue dot with a <code>6</code>. This indicates six untracked, represented by the capital <code>U</code> next to each file.</p> <p>Also, in the bottom left corner of status bar we can see we are on the <code>main</code>* branch.</p> <p></p>"},{"location":"vscode/#vs-code-workflow","title":"VS Code Workflow","text":"<p>Use VS Code to perform the following actions.</p> <ul> <li>Stage files<ul> <li>Source Control &gt; Source Control (heading) &gt; click plus icon in the section heading for staging all files</li> <li>(alternatively use the plus icon next to file name to only stage certain files)</li> </ul> </li> <li>Commit staged files<ul> <li>Source Control &gt; Source Control (heading) &gt; Commit button</li> </ul> </li> <li>Check the log - git log<ul> <li>File Explorer &gt; Timeline &gt; click on an event</li> <li>The above isn't quite the same as <code>git log</code> command from terminal</li> </ul> </li> <li>Verify current branches<ul> <li>Source Control &gt; Branches</li> </ul> </li> <li>Create and switch to new branch called <code>change-usernames</code><ul> <li>Source Control &gt; Branches &gt; click plus icon</li> </ul> </li> <li>Update username <code>arista</code> to <code>admin</code><ul> <li>Use VS Code Replace in files via Search (username arista &gt; username admin)</li> <li>Show file diff via Source Control &gt; click on a file to see its diff</li> </ul> </li> <li>Stage and commit changes to new branch</li> <li>Switch back to <code>main</code> branch<ul> <li>Source Control &gt; Branches &gt; curved arrow to <code>Switch to Branch...</code></li> </ul> </li> <li>Merge <code>change-usernames</code> into <code>main</code><ul> <li>Command-Palette Type in <code>&gt;git merge</code><ul> <li>Or Source Control &gt; Source Control (heading) &gt; ... &gt; Branch &gt; Merge Branch</li> <li>Verify files have new names in the <code>main</code> branch</li> </ul> </li> </ul> </li> <li>Be a good citizen and clean up old branch<ul> <li>Source Control &gt; Branches &gt; right click on branch name &gt; Delete Branch...</li> </ul> </li> <li>Publish repo to GitHub</li> </ul>"},{"location":"vscode/#dev-containers","title":"Dev Containers","text":"<p>The Dev Container extension allows you to use containers as a development environment. It lets you use a Docker container as a full-featured development environment. Whether you deploy to containers or not, containers make a great development environment because you can:</p> <ul> <li>Develop with a consistent, easily reproducible toolchain on the same operating system you deploy to.</li> <li>Quickly swap between different, separate development environments and safely make updates without worrying about impacting your local machine.</li> <li>Make it easy for new team members / contributors to get up and running in a consistent development environment.</li> <li>Try out new technologies or clone a copy of a code base without impacting your local setup.</li> </ul> <p>The extension starts (or attaches to) a development container running a well defined tool and runtime stack. Workspace files can be mounted into the container from the local file system, or copied or cloned into it once the container is running. Extensions are installed and run inside the container where they have full access to the tools, platform, and file system.</p>"},{"location":"vscode/#dev-container-system-requirements","title":"Dev Container System Requirements","text":"<p>Local:</p> <ul> <li>Windows: Docker Desktop 2.0+ on Windows 10 Pro/Enterprise. Windows 10 Home (2004+) requires Docker Desktop 2.2+ and the WSL2 back-end. (Docker Toolbox is not supported.)</li> <li>macOS: Docker Desktop 2.0+.</li> <li>Linux: Docker CE/EE 18.06+ and Docker Compose 1.21+. (The Ubuntu snap package is not supported.)</li> </ul> <p>Containers:</p> <ul> <li>x86_64 / ARMv7l (AArch32) / ARMv8l (AArch64) Debian 9+, Ubuntu 16.04+, CentOS / RHEL 7+</li> <li>x86_64 Alpine Linux 3.9+</li> </ul> <p>Note</p> <p>Docker must be installed on your host for Dev Containers to work.</p>"},{"location":"l2ls/l2ls-lab-guide/","title":"AVD Lab Guide","text":""},{"location":"l2ls/l2ls-lab-guide/#avd-lab-guide-overview","title":"AVD Lab Guide Overview","text":"<p>The AVD Lab Guide is a follow-along set of instructions to deploy a dual data center L2LS fabric design. The data model overview and details can be found here. In the following steps, we will explore updating the data models to add services, ports, and WAN links to our fabrics and test traffic between sites.</p> <p>In this example, the ATD lab is used to create the L2LS Dual Data Center topology below. The IP Network cloud (orange area) is pre-provisioned and is comprised of the border and core nodes in the ATD topology. Our focus will be creating the L2LS AVD data models to build and deploy configurations for Site 1 and Site 2 (blue areas) and connect them to the IP Network.</p> <p></p>"},{"location":"l2ls/l2ls-lab-guide/#host-addresses","title":"Host Addresses","text":"Host IP Address s1-host1 10.10.10.100 s1-host2 10.20.20.100 s2-host1 10.30.30.100 s2-host2 10.40.40.100"},{"location":"l2ls/l2ls-lab-guide/#step-1-prepare-lab-environment","title":"Step 1 - Prepare Lab Environment","text":""},{"location":"l2ls/l2ls-lab-guide/#access-the-atd-lab","title":"Access the ATD Lab","text":"<p>Connect to your ATD Lab and start the Programmability IDE. Next, create a new Terminal.</p>"},{"location":"l2ls/l2ls-lab-guide/#fork-and-clone-branch-to-atd-lab","title":"Fork and Clone branch to ATD Lab","text":"<p>An ATD Dual Data Center L2LS data model is posted on GitHub.</p> <ul> <li>Fork this repository to your own GitHub account.</li> <li>Next, clone your forked repo to your ATD lab instance.</li> </ul> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <pre><code>git clone &lt;your copied URL&gt;\n</code></pre> <pre><code>cd ci-workshops-avd\n</code></pre> <p>Configure your global Git settings.</p> <pre><code>git config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code>git config --global user.email \"name@example.com\"\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#update-avd","title":"Update AVD","text":"<p>AVD has been pre-installed in your lab environment. However, it may be on an older (or newer) version than intended for the lab. The following steps will update AVD and modules to the valid versions for the lab.</p> <pre><code>pip3 config set global.break-system-packages true\npip3 config set global.disable-pip-version-check true\npip3 install \"pyavd[ansible-collection]==4.10.0\"\nansible-galaxy collection install -r requirements.yml\n</code></pre> Important <p>You must run these commands when you start your lab or a new shell (terminal).</p>"},{"location":"l2ls/l2ls-lab-guide/#change-to-lab-working-directory","title":"Change To Lab Working Directory","text":"<p>Now that AVD is updated, lets move into the appropriate directory so we can access the files necessary for this L3LS Lab!</p> <pre><code>cd labs/L2LS\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#setup-lab-password-environment-variable","title":"Setup Lab Password Environment Variable","text":"<p>Each lab comes with a unique password. We set an environment variable called <code>LABPASSPHRASE</code> with the following command. The variable is later used to generate local user passwords and connect to our switches to push configs.</p> <pre><code>export LABPASSPHRASE=`awk '/password:/{print $2}' /home/coder/.config/code-server/config.yaml`\n</code></pre> <p>You can view the password is set. This is the same password displayed when you click the link to access your lab.</p> <pre><code>echo $LABPASSPHRASE\n</code></pre> IMPORTANT <p>You must run this step when you start your lab or a new shell (terminal).</p>"},{"location":"l2ls/l2ls-lab-guide/#prepare-wan-ip-network-and-test-hosts","title":"Prepare WAN IP Network and Test Hosts","text":"<p>The last step in preparing your lab is to push pre-defined configurations to the WAN IP Network (cloud) and the four hosts used to test traffic. The spines from each site will connect to the WAN IP Network with P2P links. The hosts (two per site) have port-channels to the leaf pairs and are pre-configured with an IP address and route to reach the other hosts.</p> <p>Run the following to push the configs.</p> <pre><code>make preplab\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#step-2-build-and-deploy-dual-data-center-l2ls-network","title":"Step 2 - Build and Deploy Dual Data Center L2LS Network","text":"<p>This section will review and update the existing L2LS data model. We will add features to enable VLANs, SVIs, connected endpoints, and P2P links to the WAN IP Network. After the lab, you will have enabled an L2LS dual data center network through automation with AVD. YAML data models and Ansible playbooks will be used to generate EOS CLI configurations and deploy them to each site. We will start by focusing on building out Site 1 and then repeat similar steps for Site 2. Finally, we will enable connectivity to the WAN IP Network to allow traffic to pass between sites.</p>"},{"location":"l2ls/l2ls-lab-guide/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Build and Deploy <code>Site 1</code></li> <li>Build and Deploy <code>Site 2</code></li> <li>Connect sites to WAN IP Network</li> <li>Verify routing</li> <li>Test traffic</li> </ol>"},{"location":"l2ls/l2ls-lab-guide/#step-3-site-1","title":"Step 3 - Site 1","text":""},{"location":"l2ls/l2ls-lab-guide/#build-and-deploy-initial-fabric","title":"Build and Deploy Initial Fabric","text":"<p>The initial fabric data model key/value pairs have been pre-populated in the following group_vars files in the <code>sites/site_1/group_vars/</code> directory.</p> <ul> <li>SITE1_FABRIC_PORTS.yml</li> <li>SITE1_FABRIC_SERVICES.yml</li> <li>SITE1_FABRIC.yml</li> <li>SITE1_LEAFS.yml</li> <li>SITE1_SPINES.yml</li> </ul> <p>Review these files to understand how they relate to the topology above.</p> <p>At this point, we can build and deploy our initial configurations to the topology.</p> <pre><code>make build-site-1\n</code></pre> <p>AVD creates a separate markdown and EOS configuration file per switch. In addition, you can review the files in the <code>documentation</code> and <code>intended</code> folders per site.</p> <p></p> <p>Now, deploy the configurations to Site 1 switches.</p> <pre><code>make deploy-site-1\n</code></pre> <p>Log into your switches to verify the current configs (<code>show run</code>) match the ones created in <code>intended/configs</code> folder.</p> <p>You can also check the current state for MLAG, VLANs, interfaces, and port-channels.</p> <pre><code>show mlag\n</code></pre> <pre><code>show vlan brief\n</code></pre> <pre><code>show ip interface brief\n</code></pre> <pre><code>show port-channel\n</code></pre> <p>The basic fabric with MLAG peers and port-channels between leaf and spines are now created. Next up, we will add VLAN and SVI services to the fabric.</p>"},{"location":"l2ls/l2ls-lab-guide/#add-services-to-the-fabric","title":"Add Services to the Fabric","text":"<p>The next step is to add Vlans and SVIs to the fabric. The services data model file <code>SITE1_FABRIC_SERVICES.yml</code> is pre-populated with Vlans and SVIs <code>10</code> and <code>20</code> in the default VRF.</p> <p>Open <code>SITE1_FABRIC_SERVICES.yml</code> and uncomment lines 1-28, then run the build &amp; deploy process again.</p> Tip <p> In VS Code, you can toggle comments on/off by selecting the text and pressing windows Ctrl + / or mac Cmd + /.</p> <pre><code>make build-site-1\n</code></pre> <pre><code>make deploy-site-1\n</code></pre> <p>Log into <code>s1-spine1</code> and <code>s1-spine2</code> and verify the SVIs <code>10</code> and <code>20</code> exist.</p> <pre><code>show ip interface brief\n</code></pre> <p>It should look similar to the following:</p> <pre><code>                                           Address\nInterface         IP Address            Status       Protocol            MTU    Owner\n----------------- --------------------- ------------ -------------- ----------- -------\nLoopback0         10.1.252.1/32         up           up                65535\nManagement0       192.168.0.10/24       up           up                 1500\nVlan10            10.10.10.2/24         up           up                 1500\nVlan20            10.20.20.2/24         up           up                 1500\nVlan4093          10.1.254.0/31         up           up                 1500\nVlan4094          10.1.253.0/31         up           up                 1500\n</code></pre> <p>You can verify the recent configuration session was created.</p> Info <p>When the configuration is applied via a configuration session, EOS will create a \"checkpoint\" of the configuration. This checkpoint is a snapshot of the device's running configuration as it was prior to the configuration session being committed.</p> <pre><code>show clock\n</code></pre> <pre><code>show configuration sessions detail\n</code></pre> <p>List the recent checkpoints.</p> <pre><code>show config checkpoints\n</code></pre> <p>View the contents of the latest checkpoint file.</p> <pre><code>more checkpoint:&lt; filename &gt;\n</code></pre> <p>See the difference between the running config and the latest checkpoint file.</p> Tip <p>This will show the differences between the current device configuration and the configuration before we did our <code>make deploy</code> command.</p> <pre><code>diff checkpoint:&lt; filename &gt; running-config\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#add-ports-for-hosts","title":"Add Ports for Hosts","text":"<p>Let's configure port-channels to our hosts (<code>s1-host1</code> and <code>s1-host2</code>).</p> <p>Open <code>SITE1_FABRIC_PORTS.yml</code> and uncomment lines 17-45, then run the build &amp; deploy process again.</p> <pre><code>make build-site-1\n</code></pre> <pre><code>make deploy-site-1\n</code></pre> <p>At this point, hosts should be able to ping each other across the fabric.</p> <p>From <code>s1-host1</code>, run a ping to <code>s1-host2</code>.</p> <pre><code>ping 10.20.20.100\n</code></pre> <pre><code>PING 10.20.20.100 (10.20.20.100) 72(100) bytes of data.\n80 bytes from 10.20.20.100: icmp_seq=1 ttl=63 time=30.2 ms\n80 bytes from 10.20.20.100: icmp_seq=2 ttl=63 time=29.5 ms\n80 bytes from 10.20.20.100: icmp_seq=3 ttl=63 time=28.8 ms\n80 bytes from 10.20.20.100: icmp_seq=4 ttl=63 time=24.8 ms\n80 bytes from 10.20.20.100: icmp_seq=5 ttl=63 time=26.2 ms\n</code></pre> <p>Site 1 fabric is now complete.</p>"},{"location":"l2ls/l2ls-lab-guide/#step-4-site-2","title":"Step 4 - Site 2","text":"<p>Repeat the previous three steps for Site 2.</p> <ul> <li>Add Services</li> <li>Add Ports</li> <li>Build and Deploy Configs</li> <li>Verify ping traffic between hosts <code>s2-host1</code> and <code>s2-host2</code></li> </ul> <p>At this point, you should be able to ping between hosts within a site but not between sites. For this, we need to build connectivity to the <code>WAN IP Network</code>. This is covered in the next section.</p>"},{"location":"l2ls/l2ls-lab-guide/#step-5-connect-sites-to-wan-ip-network","title":"Step 5 - Connect Sites to WAN IP Network","text":"<p>The WAN IP Network is defined by the <code>core_interfaces</code> data model. Full data model documentation is located here.</p> <p>The data model defines P2P links (<code>/31s</code>) on the spines with a stanza per link. See details in the graphic below. Each spine has two links to the WAN IP Network configured on ports <code>Ethernet7</code> and <code>Ethernet8</code>. OSPF is added to these links as well.</p> <p></p>"},{"location":"l2ls/l2ls-lab-guide/#add-p2p-links-to-wan-ip-network-for-site-1-and-2","title":"Add P2P Links to WAN IP Network for Site 1 and 2","text":"<p>Add each site's <code>core_interfaces</code> dictionary (shown below) to the bottom of the following files <code>SITE1_FABRIC.yml</code> and <code>SITE2_FABRIC.yml</code></p>"},{"location":"l2ls/l2ls-lab-guide/#site-1","title":"Site #1","text":"<p>Add the following code block to the bottom of <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <pre><code>##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#site-2","title":"Site #2","text":"<p>Add the following code block to the bottom of <code>sites/site_2/group_vars/SITE2_FABRIC.yml</code>.</p> <pre><code>##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n\n    - ip: [ 10.0.0.37/31, 10.0.0.36/31 ]\n      nodes: [ s2-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.41/31, 10.0.0.40/31 ]\n      nodes: [ s2-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.39/31, 10.0.0.38/31 ]\n      nodes: [ s2-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.43/31, 10.0.0.42/31 ]\n      nodes: [ s2-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#build-and-deploy-wan-ip-network-connectivity","title":"Build and Deploy WAN IP Network connectivity","text":"<pre><code>make build-site-1 build-site-2 deploy-site-1 deploy-site-2\n</code></pre> Tip <p>Daisy chaining \"Makesies\" is a great way to run a series of tasks with a single CLI command </p>"},{"location":"l2ls/l2ls-lab-guide/#check-routes-on-spine-nodes","title":"Check routes on spine nodes","text":"<p>From the spines, verify that they can see routes to the following networks where the hosts reside.</p> <ul> <li>10.10.10.0/24</li> <li>10.20.20.0/24</li> <li>10.30.30.0/24</li> <li>10.40.40.0/24</li> </ul> <pre><code>show ip route\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#test-traffic-between-sites","title":"Test traffic between sites","text":"<p>From <code>s1-host1</code> ping both <code>s2-host1</code> &amp; <code>s2-host2</code>.</p> <pre><code># s2-host1\nping 10.30.30.100\n</code></pre> <pre><code># s2-host2\nping 10.40.40.100\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#congratulations","title":"Congratulations!","text":"<p>You have built a multi-site L2LS network without touching the CLI on a single switch.</p>"},{"location":"l2ls/l2ls-lab-guide/#step-6-day-2-operations","title":"Step 6 - Day 2 Operations","text":"<p>Our multi-site L2LS network is working great. But, before too long, it will be time to change our configurations. Lucky for us, that time is today!</p>"},{"location":"l2ls/l2ls-lab-guide/#cleaning-up","title":"Cleaning Up","text":"<p>Before going any further, let's ensure we have a clean repo by committing the changes we've made up to this point. The CLI commands below can accomplish this, but the VS Code Source Control GUI can be used as well.</p> <pre><code>git add .\ngit commit -m 'Your message here'\n</code></pre> <p>Next, we'll want to push these changes to our forked repository on GitHub.</p> <pre><code>git push\n</code></pre> <p>If this is our first time pushing to our forked repository, then VS Code will provide us with the following sign-in prompt:</p> <p></p> <p>Choose Allow, and another prompt will come up, showing your unique login code:</p> <p></p> <p>Choose Copy &amp; Continue to GitHub, and another prompt will come up asking if it's ok to open an external website (GitHub).</p> <p></p> <p>Choose Open and then an external site (GitHub) will open, asking for your login code.</p> <p></p> <p>Paste in your login code and choose Continue. You will then be prompted to Authorize VS Code.</p> <p></p> <p>Choose Authorize Visual-Studio-Code, and you should be presented with the coveted Green Check Mark!</p> <p></p> <p>Whew! Alright. Now that we have that complete, let's keep moving...</p>"},{"location":"l2ls/l2ls-lab-guide/#branching-out","title":"Branching Out","text":"<p>Before jumping in and modifying our files, we'll create a branch named banner-syslog in our forked repository to work on our changes. We can create our branch in multiple ways, but we'll use the <code>git switch</code> command with the <code>-c</code> parameter to create our new branch.</p> <pre><code>git switch -c banner-syslog\n</code></pre> <p>After entering this command, we should see our new branch name reflected in the terminal. It will also be reflected in the status bar in the lower left-hand corner of our VS Code window (you may need to click the refresh icon before this is shown).</p> <p>Now we're ready to start working on our changes .</p>"},{"location":"l2ls/l2ls-lab-guide/#login-banner","title":"Login Banner","text":"<p>When we initially deployed our multi-site topology, we should have included a login banner on all our switches. Let's take a look at the AVD documentation site to see what the data model is for this configuration.</p> <p>The banner on all of our switches will be the same. After reviewing the AVD documentation, we know we can accomplish this by defining the <code>banners</code> input variable in our <code>global_vars/global_dc_vars.yml</code> file.</p> <p>Add the code block below to <code>global_vars/global_dc_vars.yml</code>.</p> <pre><code># Login Banner\nbanners:\n  motd: |\n    You shall not pass. Unless you are authorized. Then you shall pass.\n    EOF\n</code></pre> Yes, that \"EOF\" is important! <p>Ensure the entire code snippet above is copied; including the <code>EOF</code>. This must be present for the configuration to be considered valid</p> <p>Next, let's build out the configurations and documentation associated with this change.</p> <pre><code>make build-site-1 build-site-2\n</code></pre> <p>Please take a minute to review the results of our five lines of YAML. When finished reviewing the changes, let's commit them.</p> <p>As usual, there are a few ways of doing this, but the CLI commands below will get the job done:</p> <pre><code>git add .\ngit commit -m 'add banner'\n</code></pre> <p>So far, so good! Before we publish our branch and create a Pull Request though, we have some more work to do...</p>"},{"location":"l2ls/l2ls-lab-guide/#syslog-server","title":"Syslog Server","text":"<p>Our next Day 2 change is adding a syslog server configuration to all our switches. Once again, we'll take a look at the AVD documentation site to see the data model associated with the <code>logging</code> input variable.</p> <p>Like our banner operation, the syslog server configuration will be consistent on all our switches. Because of this, we can also put this into our <code>global_vars/global_dc_vars.yml</code> file.</p> <p>Add the code block below to <code>global_vars/global_dc_vars.yml</code>.</p> <pre><code># Syslog\nlogging:\n  vrfs:\n    - name: default\n      source_interface: Management0\n      hosts:\n        - name: 10.200.0.108\n        - name: 10.200.1.108\n</code></pre> <p>Finally, let's build out our configurations.</p> <pre><code>make build-site-1 build-site-2\n</code></pre> <p>Take a minute, using the source control feature in VS Code, to review what has changed as a result of our work.</p> <p>At this point, we have our Banner and Syslog configurations in place. The configurations look good, and we're ready to share this with our team for review. In other words, it's time to publish our branch to the remote origin (our forked repo on GitHub) and create the Pull Request (PR)!</p> <p>There are a few ways to publish the <code>banner-syslog</code> branch to our forked repository. The commands below will accomplish this via the CLI:</p> <pre><code>git add .\ngit commit -m 'add syslog'\ngit push --set-upstream origin banner-syslog\n</code></pre> <p>On our forked repository, let's create the Pull Request.</p> <p>When creating the PR, ensure that the <code>base repository</code> is the main branch of your fork. This can be selected via the dropdown as shown below:</p> <p></p> <p>Take a minute to review the contents of the PR. Assuming all looks good, let's earn the YOLO GitHub badge by approving and merging your PR!</p> Tip <p>Remember to delete the banner-syslog branch after performing the merge - Keep that repo clean!</p> <p>Once merged, let's switch back to our <code>main</code> branch and pull down our merged changes.</p> <pre><code>git switch main\ngit pull\n</code></pre> <p>Then, let's delete our now defunct banner-syslog branch.</p> <pre><code>git branch -D banner-syslog\n</code></pre> <p>Finally, let's deploy our changes.</p> <pre><code>make deploy-site-1 deploy-site-2\n</code></pre> <p>Once completed, we should see our banner when logging into any switch. The output of the <code>show logging</code> command should also have our newly defined syslog servers.</p>"},{"location":"l2ls/l2ls-lab-guide/#provisioning-new-switches","title":"Provisioning new Switches","text":"<p>Our network is gaining popularity, and it's time to add a new Leaf pair into the environment! s1-leaf5 and s1-leaf6 are ready to be provisioned, so let's get to it.</p>"},{"location":"l2ls/l2ls-lab-guide/#branch-time","title":"Branch Time","text":"<p>Before jumping in, let's create a new branch for our work. We'll call this branch add-leafs.</p> <pre><code>git switch -c add-leafs\n</code></pre> <p>Now that we have our branch created let's get to work!</p>"},{"location":"l2ls/l2ls-lab-guide/#inventory-update","title":"Inventory Update","text":"<p>First, we'll want to add our new switches, named s1-leaf5 and s1-leaf6, into our inventory file. We'll add them as members of the <code>SITE1_LEAFS</code> group.</p> <p>Add the following two lines under <code>s1-leaf4</code> in <code>sites/site_1/inventory.yml</code>.</p> <pre><code>s1-leaf5:\ns1-leaf6:\n</code></pre> <p>The <code>sites/site_1/inventory.yml</code> file should now look like the example below:</p> sites/site_1/inventory.yml <pre><code>---\nSITE1:\n  children:\n    CVP:\n      hosts:\n        cvp:\n    SITE1_FABRIC:\n      children:\n        SITE1_SPINES:\n          hosts:\n            s1-spine1:\n            s1-spine2:\n        SITE1_LEAFS:\n          hosts:\n            s1-leaf1:\n            s1-leaf2:\n            s1-leaf3:\n            s1-leaf4:\n            s1-leaf5:\n            s1-leaf6:\n    SITE1_FABRIC_SERVICES:\n      children:\n        SITE1_SPINES:\n        SITE1_LEAFS:\n    SITE1_FABRIC_PORTS:\n      children:\n        SITE1_SPINES:\n        SITE1_LEAFS:\n</code></pre> <p>Next, let's add our new Leaf switches into <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <p>These new switches will go into RACK3, leverage MLAG for multi-homing, and will have locally connected endpoints in VLANs <code>10</code> and <code>20</code>.</p> <p>Just like the other Leaf switches, interfaces <code>Ethernet2</code> and <code>Ethernet3</code> will be used to connect to the spines.</p> <p>On the spines, interface <code>Ethernet9</code> will be used to connect to s1-leaf5, while <code>Ethernet10</code> will be used to connect to s1-leaf6.</p> <p>Starting at line 69, add the following code block into <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <pre><code>- group: RACK3\n  nodes:\n    - name: s1-leaf5\n      id: 5\n      mgmt_ip: 192.168.0.28/24\n      uplink_switch_interfaces: [ Ethernet9, Ethernet9 ]\n    - name: s1-leaf6\n      id: 6\n      mgmt_ip: 192.168.0.29/24\n      uplink_switch_interfaces: [ Ethernet10, Ethernet10 ]\n</code></pre> Warning <p>Make sure the indentation of <code>RACK3</code> is the same as <code>RACK2</code>, which can be found on line 57</p> <p>The <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code> file should now look like the example below:</p> sites/site_1/group_vars/SITE1_FABRIC.yml <pre><code>---\nfabric_name: SITE1_FABRIC\n\n# Set Design Type to L2ls\ndesign:\n  type: l2ls\n\n# Spine Switches\nl3spine:\n  defaults:\n    platform: cEOS\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 4096\n    loopback_ipv4_pool: 10.1.252.0/24\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    mlag_peer_l3_ipv4_pool: 10.1.254.0/24\n    virtual_router_mac_address: 00:1c:73:00:dc:01\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: SPINES\n      nodes:\n        - name: s1-spine1\n          id: 1\n          mgmt_ip: 192.168.0.10/24\n        - name: s1-spine2\n          id: 2\n          mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nleaf:\n  defaults:\n    platform: cEOS\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 16384\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: RACK1\n      filter:\n        tags: [ \"Web\" ]\n      nodes:\n        - name: s1-leaf1\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: RACK2\n      filter:\n        tags: [ \"App\" ]\n      nodes:\n        - name: s1-leaf3\n          id: 3\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 4\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n    - group: RACK3\n      nodes:\n        - name: s1-leaf5\n          id: 5\n          mgmt_ip: 192.168.0.28/24\n          uplink_switch_interfaces: [ Ethernet9, Ethernet9 ]\n        - name: s1-leaf6\n          id: 6\n          mgmt_ip: 192.168.0.29/24\n          uplink_switch_interfaces: [ Ethernet10, Ethernet10 ]\n\n\n##################################################################\n# Underlay Routing Protocol - ran on Spines\n##################################################################\n\nunderlay_routing_protocol: OSPF\n\n##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre> Tip <p>Notice how we did not specify a <code>filter</code> or <code>tags</code> under <code>RACK3</code>. If the <code>filter</code> parameter is not defined, all VLANs/SVIs/VRFs will be provisioned on the switch. In our case, this means that VLANs <code>10</code> and <code>20</code> will both be created on our new Leaf switches. However, since they are <code>leaf</code> node types, no SVIs will be created.</p> <p>Next - Let's build the configuration!</p> <pre><code>make build-site-1\n</code></pre> Important <p>Interfaces <code>Ethernet9</code> and <code>Ethernet10</code> do not exist on the Spines. Because of this, we will not run a deploy command since it would fail.</p> <p>Please take a moment and review the results of our changes via the source control functionality in VS Code.</p> <p>Finally, we'll commit our changes and publish our branch. Again, we can use the VS Code Source Control GUI for this, or via the CLI using the commands below:</p> <pre><code>git add .\ngit commit -m 'add leafs'\ngit push --set-upstream origin add-leafs\n</code></pre>"},{"location":"l2ls/l2ls-lab-guide/#step-7-backing-out-changes","title":"Step 7 - Backing out changes","text":"<p>Ruh Roh. As it turns out, we should have added these leaf switches to an entirely new site. Oops! No worries, because we used our add-leafs branch, we can switch back to our main branch and then delete our local copy of the add-leafs branch. No harm or confusion related to this change ever hit the main branch!</p> <pre><code>git switch main\ngit branch -D add-leafs\n</code></pre> <p>Finally, we can go out to our forked copy of the repository and delete the add-leafs branch.</p> <p>All set!</p>"},{"location":"l2ls/overview/","title":"Arista CI Workshop","text":"<p>This workshop will leverage open-source tools to build a network CI pipeline for configuration development, deployment, documentation, and validation. In addition, the open-source tooling enables us to manage our network environment as code.</p> <p>This section will cover the following:</p> <ul> <li>Arista Validated Designs (AVD) Ansible Collection</li> <li>Network Data Models</li> <li>Initial Deployment (Day 0 Provisioning)</li> <li>Ongoing Operations (Day 2 and Beyond)</li> <li>Validation and Troubleshooting</li> </ul> <p>Each attendee will receive a dedicated virtual lab environment with Git, VS Code, and Ansible installed and ready to use.</p> <p>Attendees will need the following:</p> <ul> <li>A laptop</li> <li>An account on GitHub</li> <li>Familiarity with the concepts and tools covered in the previous Automation Fundamentals workshop (Git, VS Code, Jinja/YAML, Ansible)</li> </ul>"},{"location":"l2ls/overview/#lab-topology-overview","title":"Lab Topology Overview","text":"<p>Throughout this section, we will use the following dual data center topology. Click on the image to zoom in for details.</p> <p></p>"},{"location":"l2ls/overview/#basic-eos-switch-configuration","title":"Basic EOS Switch Configuration","text":"<p>Basic connectivity between the Ansible controller host and the switches must be established before Ansible can be used to deploy configurations. The following should be configured on all switches:</p> <ul> <li>Switch Hostname</li> <li>IP enabled interface</li> <li>Username and Password defined</li> <li>Management eAPI enabled</li> </ul> Info <p>In the ATD environment, cEOS virtual switches use <code>Management0</code> in the default VRF. When using actual hardware or vEOS switches, <code>Management1</code> is used. The included basic switch configurations may need to be adjusted for your environment.</p> <p>Below is an example basic configuration file for s1-spine1:</p> <pre><code>!\nno aaa root\n!\nusername admin privilege 15 role network-admin secret sha512 $6$eucN5ngreuExDgwS$xnD7T8jO..GBDX0DUlp.hn.W7yW94xTjSanqgaQGBzPIhDAsyAl9N4oScHvOMvf07uVBFI4mKMxwdVEUVKgY/.\n!\nhostname s1-spine1\n!\nmanagement api http-commands\n   no shutdown\n!\ninterface Management0\n   ip address 192.168.0.10/24\n!\nip routing\n!\nip route vrf MGMT 0.0.0.0/0 192.168.0.1\n!\n</code></pre>"},{"location":"l2ls/overview/#ansible-inventory","title":"Ansible Inventory","text":"<p>Our lab L2LS topology contains two sites, <code>Site 1</code> and <code>Site 2</code>. We need to create the Ansible inventory for each site. We have created two separate directories for each site under the <code>sites</code> sub-directory in our repo.</p> <pre><code>\u251c\u2500\u2500 sites/\n  \u251c\u2500\u2500 site_1/\n  \u251c\u2500\u2500 site_2/\n</code></pre> <p>The following is a graphical representation of the Ansible inventory groups and naming scheme used for <code>Site 1</code> in this example. This is replicated for <code>Site 2</code>.</p> <p></p>"},{"location":"l2ls/overview/#avd-fabric-variables","title":"AVD Fabric Variables","text":"<p>To apply AVD variables to the nodes in the fabric, we make use of Ansible group_vars. How and where you define the variables is your choice. The group_vars table below is one example of AVD fabric variables for <code>Site 1</code>.</p> group_vars/ Description SITE1_FABRIC.yml Fabric, Topology, and Device settings SITE1_SPINES.yml Device type for Spines SITE1_LEAFS.yml Device type for Leafs SITE1_NETWORK_SERVICES.yml VLANs, VRFs, SVIs SITE1_NETWORK_PORTS.yml Port Profiles and Connected Endpoint settings <p>Each group_vars file is listed in the following tabs.</p> SITE1_FABRICSITE1_SPINESSITE1_LEAFSSITE1_NETWORK_SERVICESSITE1_NETWORK_PORTS <p>At the Fabric level (SITE1_FABRIC), the following variables are defined in group_vars/SITE1_FABRIC.yml. The fabric name, design type (l2ls), node type defaults, interface links, and core interface P2P links are defined at this level. Other variables you must supply include:</p> <ul> <li>spanning_tree_mode</li> <li>spanning_tree_priority</li> <li>mlag_peer_ipv4_pool</li> </ul> <p>The l3spine node will need these additional variables set.</p> <ul> <li>loopback_ipv4_pool</li> <li>mlag_peer_l3_ipv4_pool</li> <li>virtual_router_mac_address</li> </ul> <p>Variables applied under the node key type (l3spine/leaf) defaults section are inherited by nodes under each type. These variables may be overwritten under the node itself.</p> <p>The spine interface used by a particular leaf is defined from the leaf's perspective with a variable called <code>uplink_switch_interfaces</code>. For example, s1-leaf1 has a unique variable <code>uplink_switch_interfaces: [Ethernet2, Ethernet2]</code> defined. This means that s1-leaf1 is connected to <code>s1-spine1</code> Ethernet2 and <code>s1-spine2</code> Ethernet2, respectively.</p> <pre><code>---\nfabric_name: SITE1_FABRIC\n\n# Set Design Type to L2ls\ndesign:\n  type: l2ls\n\n# Repeat MLAG addressing between pairs\nfabric_ip_addressing:\n  mlag:\n    algorithm: same_subnet\n\n# Spine Switches\nl3spine:\n  defaults:\n    platform: cEOS\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 4096\n    loopback_ipv4_pool: 10.1.252.0/24\n    mlag_peer_ipv4_pool: 10.1.253.0/31\n    mlag_peer_l3_ipv4_pool: 10.1.253.2/31\n    virtual_router_mac_address: 00:1c:73:00:dc:01\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: SPINES\n      nodes:\n        - name: s1-spine1\n          id: 1\n          mgmt_ip: 192.168.0.10/24\n        - name: s1-spine2\n          id: 2\n          mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nleaf:\n  defaults:\n    platform: cEOS\n    mlag_peer_ipv4_pool: 10.1.253.0/31\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 16384\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: RACK1\n      filter:\n        tags: [ \"Web\" ]\n      nodes:\n        - name: s1-leaf1\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: RACK2\n      filter:\n        tags: [ \"App\" ]\n      nodes:\n        - name: s1-leaf3\n          id: 3\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 4\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n\n##################################################################\n# Underlay Routing Protocol - ran on Spines\n##################################################################\n\nunderlay_routing_protocol: OSPF\n\n##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre> <p>In an L2LS design, there are two types of spine nodes: <code>spine</code> and <code>l3spine</code>. In AVD, the node type defines the functionality and the EOS CLI configuration to be generated. For our L2LS topology, we will use node type <code>l3spine</code> to include SVI functionality.</p> <pre><code>---\ntype: l3spine\n</code></pre> <p>In an L2LS design, we have one type of leaf node: <code>leaf</code>. This will provide L2 functionality to the leaf nodes.</p> <pre><code>---\ntype: leaf\n</code></pre> <p>You add VLANs to the Fabric by updating the group_vars/SITE1_NETWORK_SERVICES.yml. Each VLAN will be given a name and a list of tags. The tags filter the VLAN to specific Leaf Pairs. These variables are applied to spine and leaf nodes since they are a part of this nested group.</p> <pre><code>---\ntenants:\n  - name: MY_FABRIC\n    vrfs:\n      - name: default\n        svis:\n          - id: 10\n            name: 'Ten'\n            tags: [ \"Web\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.10.10.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.10.10.2/24\n              - node: s1-spine2\n                ip_address: 10.10.10.3/24\n          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2\n                ip_address: 10.20.20.3/24\n</code></pre> <p>Our fabric would only be complete by connecting some devices to it. We define connected endpoints and port profiles in group_vars/SITE1_NETWORKS_PORTS.yml. Each endpoint adapter defines which switch port and port profile to use. Our lab has two hosts connected to the <code>site 1</code> fabric. The connected endpoints keys are used for logical separation and apply to interface descriptions. These variables are applied to the spine and leaf nodes since they are a part of this nested inventory group.</p> <pre><code>---\nport_profiles:\n\n  - profile: PP-VLAN10\n    mode: \"access\"\n    vlans: \"10\"\n    spanning_tree_portfast: edge\n  - profile: PP-VLAN20\n    mode: \"access\"\n    vlans: \"20\"\n    spanning_tree_portfast: edge\n\n###########################################################\n# ---------------- Endpoint Connectivity ---------------- #\n###########################################################\n\nservers:\n\n# --------------------------------------------------------#\n# Site1 RACK1 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host1                                      # Server name\n    rack: RACK1                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf1, s1-leaf2 ]                # Switch to connect server (required)\n        profile: PP-VLAN10                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n\n# --------------------------------------------------------#\n# Site1 RACK2 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host2                                      # Server name\n    rack: RACK2                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf3, s1-leaf4 ]                # Switch to connect server (required)\n        profile: PP-VLAN20                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n</code></pre>"},{"location":"l2ls/overview/#global-variables","title":"Global Variables","text":"<p>In a multi-site environment, some variables must be applied to all sites. They include AAA, Local Users, NTP, Syslog, DNS, and TerminAttr. Instead of updating these same variables in multiple inventory group_vars, we can use a single global variable file and import the variables at playbook runtime. This allows us to make a single change applied to all sites.</p> <p>For example, in our lab, we use a global variable file <code>global_vars/global_dc-vars.yml</code>.</p> <p>AVD provides a <code>global_vars</code> plugin that enables the use of global variables.</p> <p>The <code>global_vars</code> plugin must be enabled in the <code>ansible.cfg</code> file as shown below:</p> <pre><code>#enable global vars\nvars_plugins_enabled = arista.avd.global_vars, host_group_vars\n\n#define global vars path\n[vars_global_vars]\npaths = ../../global_vars\n</code></pre> Info <p>If a folder is used as in the example above, all files in the folder will be parsed in alphabetical order.</p>"},{"location":"l2ls/overview/#example-global-vars-file","title":"Example Global Vars File","text":"global_vars/global_dc_vars.yml <pre><code>---\n\n# Credentials for CVP and EOS Switches\nansible_user: arista\nansible_password: \"{{ lookup('env', 'LABPASSPHRASE') }}\"\nansible_network_os: arista.eos.eos\n# Configure privilege escalation\nansible_become: true\nansible_become_method: enable\n# HTTPAPI configuration\nansible_connection: httpapi\nansible_httpapi_port: 443\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_python_interpreter: $(which python3)\navd_data_conversion_mode: error\navd_data_validation_mode: error\n\n# Local Users\nlocal_users:\n  - name: arista\n    privilege: 15\n    role: network-admin\n    sha512_password: \"{{ ansible_password | password_hash('sha512', salt='arista') }}\"\n\n# AAA\naaa_authorization:\n  exec:\n    default: local\n\n# OOB Management network default gateway.\nmgmt_gateway: 192.168.0.1\nmgmt_interface: Management0\nmgmt_interface_vrf: default\n\n# NTP Servers IP or DNS name, first NTP server will be preferred, and sourced from Management VRF\nntp:\n  servers:\n    - name: 192.168.0.1\n      iburst: true\n      local_interface: Management0\n\n# Domain/DNS\ndns_domain: atd.lab\n\n# TerminAttr\ndaemon_terminattr:\n  # Address of the gRPC server on CloudVision\n  # TCP 9910 is used on on-prem\n  # TCP 443 is used on CV as a Service\n  cvaddrs: # For single cluster\n    - 192.168.0.5:9910\n  # Authentication scheme used to connect to CloudVision\n  cvauth:\n    method: token\n    token_file: \"/tmp/token\"\n  # Exclude paths from Sysdb on the ingest side\n  ingestexclude: /Sysdb/cell/1/agent,/Sysdb/cell/2/agent\n  # Exclude paths from the shared memory table\n  smashexcludes: ale,flexCounter,hardware,kni,pulse,strata\n\n# Point to Point Links MTU Override for Lab\np2p_uplinks_mtu: 1500\n\n# CVP node variables\ncv_collection: v3\nexecute_tasks: true\n</code></pre>"},{"location":"l2ls/overview/#data-models","title":"Data Models","text":"<p>AVD provides a network-wide data model and is typically broken into multiple group_vars files to simplify and categorize variables with their respective functions. We break the data model into three categories: topology, services, and ports.</p>"},{"location":"l2ls/overview/#fabric-topology","title":"Fabric Topology","text":"<p>The physical fabric topology is defined by providing interface links between the spine and leaf nodes. The <code>group_vars/SITE1_FABRIC.yml</code> file defines this portion of the data model. In our lab, the spines provide layer 3 routing of SVIs and P2P links using a node type called <code>l3spines</code>. The leaf nodes are purely layer 2 and use node type <code>leaf</code>. An AVD L2LS design type provides three node type keys: l3\u00a0spine, spine, and leaf. AVD Node Type documentation can be found here.</p>"},{"location":"l2ls/overview/#spine-and-leaf-nodes","title":"Spine and Leaf Nodes","text":"<p>The example data model below defines each site's spine and leaf nodes for each site. Refer to the inline comments for variable definitions. Under each node_type_key you have key/value pairs for defaults, node_groups, and nodes. Note that key/value pairs may be overwritten with the following descending order of precedence. The key/value closest to the node will be used.</p> <p> <ol> <li>defaults</li> <li>node_groups</li> <li>nodes</li> </ol> <pre><code>################################\n# Spine Switches\n################################\n\n# node_type_key for providing L3 services\nl3spine:\n  # default key/values for all l3spine nodes\n  defaults:\n    # Platform dependent default settings for mlag timers, tcam profiles, LANZ, management interface\n    platform: cEOS\n    # Spanning Tree\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 4096\n    # Loopback0 pool, used in conjunction with value of `id:` under each node\n    loopback_ipv4_pool: 10.1.252.0/24\n    # IP Pool for MLAG peer link\n    mlag_peer_ipv4_pool: 10.1.253.0/31\n    # IP Pool for L3 peering over the MLAG peer link\n    mlag_peer_l3_ipv4_pool: 10.1.253.2/31\n    # Virtual MAC address used in vARP\n    virtual_router_mac_address: 00:1c:73:00:dc:01\n    # Default MLAG interfaces between spine nodes\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  # keyword for node groups, two nodes within a node group will form an MLAG pair\n  node_groups:\n    # User-defined node group name\n    - group: SPINES\n      # key word for nodes\n      nodes:\n        - name: s1-spine1\n          # unique identifier used for IP address calculations\n          id: 1\n          # Management address assigned to the defined management interface\n          mgmt_ip: 192.168.0.10/24\n        - name: s1-spine2\n          id: 2\n          mgmt_ip: 192.168.0.11/24\n\n################################\n# Leaf Switches\n################################\nleaf:\n  defaults:\n    platform: cEOS\n    mlag_peer_ipv4_pool: 10.1.253.0/31\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 16384\n    # Default uplink switches from leaf perspective\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    # Default uplink interfaces on leaf nodes connecting to the spines\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    # Default leaf MLAG interfaces\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    # User-defined node group name\n    - group: RACK1\n      # Filter which Vlans will be applied to the node_group, comma-separated tags supported\n      # Tags for each Vlan are defined in the SITE1_FABRIC_SERVICES.yml\n      filter:\n        tags: [ \"Web\" ]\n      nodes:\n        - name: s1-leaf1\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          # Define which interface is configured on the uplink switch\n          # In this example s1-leaf1 connects to [ s1-spine1, s1-spine2 ]\n          # on the following ports. This will be unique to each leaf\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: RACK2\n      filter:\n        tags: [ \"App\" ]\n      nodes:\n        - name: s1-leaf3\n          id: 3\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 4\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n</code></pre>"},{"location":"l2ls/overview/#core-interfaces","title":"Core Interfaces","text":"<p>Inside the same group_vars file, we define how each site is linked to the Core IP Network using point-to-point L3 links on the spine nodes. In our example, OSPF will be used to share routes between sites across the IP Network. The <code>core_interfaces</code> data model for <code>Site 1</code> follows.</p> <pre><code>core_interfaces:\n  p2p_links:\n    # s1-spine1 Ethernet7 to WANCORE\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    # s1-spine1 Ethernet8 to WANCORE\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    # s1-spine2 Ethernet7 to WANCORE\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    # s1-spine2 Ethernet8 to WANCORE\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre> <p>The following diagram shows the P2P links to the IP Network from all four spine nodes. The WAN IP Network is pre-configured in our lab with /31's running OSPF in area 0.0.0.0. The core_interfaces for the spines in <code>Site 1</code> and <code>Site 2</code> are configured and deployed with AVD.</p> <p></p>"},{"location":"l2ls/overview/#fabric-services","title":"Fabric Services","text":"<p>Fabric Services, such as VLANs, SVIs, and VRFs, are defined in this section. The following Site 1 example defines VLANs and SVIs for VLANs <code>10</code> and <code>20</code> in the default VRF. Additional VRF definitions can also be applied.</p> <pre><code>---\ntenants:\n  # User-defined Tenant/Fabric name\n  - name: MY_FABRIC\n    # key-word\n    vrfs:\n      # Default VRF\n      - name: default\n        # key-word\n        svis:\n          # Vlan ID\n          - id: 10\n            # Vlan Name\n            name: 'Ten'\n            # Tag assigned to Vlan. Used as a filter by each node_group\n            tags: [ \"Web\" ]\n            enabled: true\n            # SVI Virtual ARP address, used along with pre-defined virtual_router_mac_address\n            ip_virtual_router_addresses:\n              - 10.10.10.1\n            # Which nodes to apply physical SVI address\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.10.10.2/24\n              - node: s1-spine2\n                ip_address: 10.10.10.3/24\n          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2\n                ip_address: 10.20.20.3/24\n</code></pre>"},{"location":"l2ls/overview/#fabric-ports","title":"Fabric Ports","text":"<p>The Fabric must define ports for southbound interfaces toward connected endpoints such as servers, appliances, firewalls, and other networking devices in the data center. This section uses port profiles and connected endpoints called <code>servers</code>. Documentation for port_profiles and connected endpoints are available to see all the options available.</p> <p>The following data model defined two port profiles: PP-VLAN10 and PP-VLAN20. They define an access port profile for VLAN <code>10</code> and <code>20</code>, respectively. In addition, two server endpoints (s1-host1 and s1-host2) are created to use these port profiles. There are optional and required fields. The optional fields are used for port descriptions in the EOS intended configurations.</p> <pre><code>---\nport_profiles:\n\n  - profile: PP-VLAN10\n    mode: \"access\"\n    vlans: \"10\"\n    spanning_tree_portfast: edge\n  - profile: PP-VLAN20\n    mode: \"access\"\n    vlans: \"20\"\n    spanning_tree_portfast: edge\n\n###########################################################\n# ---------------- Endpoint Connectivity ---------------- #\n###########################################################\n\nservers:\n\n# --------------------------------------------------------#\n# Site1 RACK1 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host1                                      # Server name\n    rack: RACK1                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf1, s1-leaf2 ]                # Switch to connect server (required)\n        profile: PP-VLAN10                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n\n# --------------------------------------------------------#\n# Site1 RACK2 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host2                                      # Server name\n    rack: RACK2                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf3, s1-leaf4 ]                # Switch to connect server (required)\n        profile: PP-VLAN20                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n</code></pre>"},{"location":"l2ls/overview/#the-playbooks","title":"The Playbooks","text":"<p>Two playbooks, <code>build.yml</code> and <code>deploy.yml</code> are used in our lab. Expand the tabs below to reveal the content.</p> build.yml Playbook <pre><code>---\n- name: Build Switch configuration\n  hosts: \"{{ target_hosts }}\"\n  gather_facts: false\n\n  tasks:\n\n    - name: Generate Structured Variables per Device\n      import_role:\n        name: arista.avd.eos_designs\n\n    - name: Generate Intended Config and Documentation\n      import_role:\n        name: arista.avd.eos_cli_config_gen\n</code></pre> deploy.yml Playbook <pre><code>---\n- name: Deploy Switch configuration\n  hosts: \"{{ target_hosts }}\"\n  gather_facts: false\n\n  tasks:\n\n    - name: Deploy Configuration to Device\n      import_role:\n        name: arista.avd.eos_config_deploy_eapi\n</code></pre> <p>To make our lives easier, we use a <code>Makefile</code> to create aliases to run the playbooks and provide the needed options. This eliminates mistakes and typing long commands.</p> Makefile <pre><code>.PHONY: help\nhelp: ## Display help message\n    @grep -E '^[0-9a-zA-Z_-]+\\.*[0-9a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\n########################################################\n# Site 1\n########################################################\n\n.PHONY: build-site-1\nbuild-site-1: ## Build Configs\n    ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n# ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\" -e \"@global_vars/global_dc_vars.yml\"\n\n.PHONY: deploy-site-1\ndeploy-site-1: ## Deploy Configs via eAPI\n    ansible-playbook playbooks/deploy.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n\n########################################################\n# Site 2\n########################################################\n\n.PHONY: build-site-2\nbuild-site-2: ## Build Configs\n    ansible-playbook playbooks/build.yml -i sites/site_2/inventory.yml -e \"target_hosts=SITE2_FABRIC\"\n\n.PHONY: deploy-site-2\ndeploy-site-2: ## Deploy Configs via eAPI\n    ansible-playbook playbooks/deploy.yml -i sites/site_2/inventory.yml -e \"target_hosts=SITE2_FABRIC\"\n</code></pre> <p>For example, if we wanted to run a playbook to build configs for Site 1, we could enter the following command.</p> <pre><code>ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n</code></pre> <p>Thankfully, a convenient way to simplify the above command is with a Makefile entry like the one below.</p> <pre><code>.PHONY: build-site-1\nbuild-site-1: ## Build Configs\n  ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n</code></pre> <p>Now, you can type the following to issue the same ansible-playbook command.</p> <pre><code>make build-site-1\n</code></pre> <p>In the upcoming lab, we will use the following <code>make</code> commands several times. First, review the above <code>Makefile</code> to see what each entry does. Then, try building some custom entries.</p> <p>Build configurations</p> <pre><code># Build configs for Site 1\nmake build-site-1\n\n# Build configs for Site 2\nmake build-site-2\n</code></pre> <p>Deploy configurations</p> <pre><code># Deploy configs for Site 1\nmake deploy-site-1\n\n# Deploy configs for Site 2\nmake deploy-site-2\n</code></pre>"},{"location":"l2ls/overview/#next-steps","title":"Next Steps","text":"<p>Continue to Lab Guide</p>"},{"location":"l3ls/l3ls-lab-guide/","title":"Initial Deployment - Day 0","text":""},{"location":"l3ls/l3ls-lab-guide/#avd-lab-guide-overview","title":"AVD Lab Guide Overview","text":"<p>The AVD Lab Guide is a follow-along set of instructions to deploy a dual data center L3LS EVPN VXLAN fabric design. The data model overview and details can be found here. In the following steps, we will explore updating the data models to add services, ports, and DCI links to our fabrics and test traffic between sites.</p> <p>In this example, the ATD lab is used to create the L3LS Dual Data Center topology below. The DCI Network cloud (orange area) is pre-provisioned and is comprised of the core nodes in the ATD topology. Our focus will be creating the L3LS AVD data models to build and deploy configurations for Site 1 and Site 2 (blue areas) and connect them to the DCI Network.</p> <p></p>"},{"location":"l3ls/l3ls-lab-guide/#host-addresses","title":"Host Addresses","text":"Host IP Address s1-host1 10.10.10.100 s1-host2 10.20.20.100 s2-host1 10.10.10.200 s2-host2 10.20.20.200"},{"location":"l3ls/l3ls-lab-guide/#step-1-prepare-lab-environment","title":"Step 1 - Prepare Lab Environment","text":""},{"location":"l3ls/l3ls-lab-guide/#access-the-atd-lab","title":"Access the ATD Lab","text":"<p>Connect to your ATD Lab and start the Programmability IDE. Next, create a new Terminal.</p>"},{"location":"l3ls/l3ls-lab-guide/#fork-and-clone-branch-to-atd-lab","title":"Fork and Clone branch to ATD Lab","text":"<p>An ATD Dual Data Center L3LS data model is posted on GitHub.</p> <ul> <li>Fork this repository to your own GitHub account.</li> <li>Next, clone your forked repo to your ATD lab instance.</li> </ul> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <pre><code>git clone &lt;your copied URL&gt;\n</code></pre> <pre><code>cd ci-workshops-avd\n</code></pre> <p>Configure your global Git settings.</p> <pre><code>git config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code>git config --global user.email \"name@example.com\"\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#update-avd","title":"Update AVD","text":"<p>AVD has been pre-installed in your lab environment. However, it may be on an older version (in some cases a newer version). The following steps will update AVD and modules to the valid versions for the lab.</p> <pre><code>pip3 config set global.break-system-packages true\npip3 config set global.disable-pip-version-check true\npip3 install \"pyavd[ansible-collection]==4.10.0\"\nansible-galaxy collection install -r requirements.yml\n</code></pre> Important <p>You must run these commands when you start your lab or a new shell (terminal).</p>"},{"location":"l3ls/l3ls-lab-guide/#change-to-lab-working-directory","title":"Change To Lab Working Directory","text":"<p>Now that AVD is updated, lets move into the appropriate directory so we can access the files necessary for this L3LS Lab!</p> <pre><code>cd labs/L3LS_EVPN\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#setup-lab-password-environment-variable","title":"Setup Lab Password Environment Variable","text":"<p>Each lab comes with a unique password. We set an environment variable called <code>LABPASSPHRASE</code> with the following command. The variable is later used to generate local user passwords and connect to our switches to push configs.</p> <pre><code>export LABPASSPHRASE=`cat /home/coder/.config/code-server/config.yaml| grep \"password:\" | awk '{print $2}'`\n</code></pre> <p>You can view the password is set. This is the same password displayed when you click the link to access your lab.</p> <pre><code>echo $LABPASSPHRASE\n</code></pre> IMPORTANT <p>You must run this step when you start your lab or a new shell (terminal).</p>"},{"location":"l3ls/l3ls-lab-guide/#prepare-dci-network-and-test-hosts","title":"Prepare DCI Network and Test Hosts","text":"<p>The last step in preparing your lab is to push pre-defined configurations to the DCI Network (cloud) and the four hosts used to test traffic. The border leafs from each site will connect to their specified peer with P2P links. The hosts (two per site) have port-channels to the leaf pairs and are pre-configured with an IP address and route to reach the other hosts.</p> <p>Run the following to push the configs.</p> <pre><code>make preplab\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#step-2-build-and-deploy-dual-data-center-l3ls-network","title":"Step 2 - Build and Deploy Dual Data Center L3LS Network","text":"<p>This section will review and update the existing L3LS data model. We will add features to enable VLANs, SVIs, connected endpoints, and P2P links to the DCI Network. After the lab, you will have enabled an L3LS EVPN VXLAN dual data center network through automation with AVD. YAML data models and Ansible playbooks will be used to generate EOS CLI configurations and deploy them to each site. We will start by focusing on building out Site 1 and then repeat similar steps for Site 2. Then, we will enable connectivity to the DCI Network to allow traffic to pass between sites.  Finally, we will enable EVPN gateway functionality on the border leafs.</p>"},{"location":"l3ls/l3ls-lab-guide/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Build and Deploy <code>Site 1</code></li> <li>Build and Deploy <code>Site 2</code></li> <li>Connect sites to DCI Network</li> <li>Verify routing</li> <li>Enable EVPN gateway functionality</li> <li>Test traffic</li> </ol>"},{"location":"l3ls/l3ls-lab-guide/#step-3-site-1","title":"Step 3 - Site 1","text":""},{"location":"l3ls/l3ls-lab-guide/#build-and-deploy-initial-fabric","title":"Build and Deploy Initial Fabric","text":"<p>The initial fabric data model key/value pairs have been pre-populated in the following group_vars files in the <code>sites/site_1/group_vars/</code> directory.</p> <ul> <li>SITE1_CONNECTED_ENDPOINTS.yml</li> <li>SITE1_NETWORK_SERVICES.yml</li> <li>SITE1_FABRIC.yml</li> <li>SITE1_LEAFS.yml</li> <li>SITE1_SPINES.yml</li> </ul> <p>Review these files to understand how they relate to the topology above.</p> <p>At this point, we can build and deploy our initial configurations to the topology.</p> <pre><code>make build-site-1\n</code></pre> <p>AVD creates a separate markdown and EOS configuration file per switch. In addition, you can review the files in the <code>documentation</code> and <code>intended</code> folders per site.</p> <p></p> <p>Now, deploy the configurations to Site 1 switches.</p> <pre><code>make deploy-site-1\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#verification","title":"Verification","text":"<p>Now, lets login to some switches to verify the current configs (<code>show run</code>) match the ones created in <code>intended/configs</code> folder. We can also check the current state for MLAG, interfaces, BGP peerings for IPv4 underlay, and BGP EVPN overlay peerings.</p> <p>These outputs were taken from <code>s1-leaf1</code>:</p> <ol> <li> <p>Check MLAG status.</p> <p>Command</p> <pre><code>show mlag\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#sho mlag\nMLAG Configuration:\ndomain-id                          :            S1_RACK1\nlocal-interface                    :            Vlan4094\npeer-address                       :          10.251.1.1\npeer-link                          :       Port-Channel1\nhb-peer-address                    :             0.0.0.0\npeer-config                        :          consistent\n\nMLAG Status:\nstate                              :              Active\nnegotiation status                 :           Connected\npeer-link status                   :                  Up\nlocal-int status                   :                  Up\nsystem-id                          :   02:1c:73:c0:c6:12\ndual-primary detection             :            Disabled\ndual-primary interface errdisabled :               False\n\nMLAG Ports:\nDisabled                           :                   0\nConfigured                         :                   0\nInactive                           :                   0\nActive-partial                     :                   0\nActive-full                        :                   0\n</code></pre> </li> <li> <p>Check routed interface configurations.</p> <p>Command</p> <pre><code>show ip interface brief\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show ip interface brief\n                                                                                Address\nInterface         IP Address            Status       Protocol            MTU    Owner\n----------------- --------------------- ------------ -------------- ----------- -------\nEthernet2         172.16.1.1/31         up           up                 1500\nEthernet3         172.16.1.3/31         up           up                 1500\nLoopback0         10.250.1.3/32         up           up                65535\nLoopback1         10.255.1.3/32         up           up                65535\nManagement0       192.168.0.12/24       up           up                 1500\nVlan4093          10.252.1.0/31         up           up                 1500\nVlan4094          10.251.1.0/31         up           up                 1500\n</code></pre> </li> <li> <p>Check eBGP/iBGP peerings for IPv4 underlay routing.</p> <p>Commands</p> <pre><code>show ip bgp summary\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show ip bgp summary\nBGP summary information for VRF default\nRouter identifier 10.250.1.3, local AS number 65101\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor   V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-leaf2                 10.252.1.1 4 65101             15        14    0    0 00:02:29 Estab   10     10\n  s1-spine1_Ethernet2      172.16.1.0 4 65100             13        18    0    0 00:02:30 Estab   7      7\n  s1-spine2_Ethernet2      172.16.1.2 4 65100             15        14    0    0 00:02:29 Estab   7      7\n</code></pre> </li> <li> <p>Check eBGP/iBGP peerings for the EVPN overlay.</p> <p>Commands</p> <pre><code>show bgp evpn summary\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show ip bgp summary\nBGP summary information for VRF default\nRouter identifier 10.250.1.3, local AS number 65101\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor   V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-leaf2                 10.252.1.1 4 65101             15        14    0    0 00:02:29 Estab   10     10\n  s1-spine1_Ethernet2      172.16.1.0 4 65100             13        18    0    0 00:02:30 Estab   7      7\n  s1-spine2_Ethernet2      172.16.1.2 4 65100             15        14    0    0 00:02:29 Estab   7      7\n</code></pre> </li> </ol> Base Fabric Built <p>The basic fabric with MLAG peers, P2P routed links between leaf and spines, eBGP/iBGP IPv4 underlay peerings, and eBGP/iBGP EVPN overlay peerings is now created. Next up, we will add VLAN and SVI services to the fabric.</p>"},{"location":"l3ls/l3ls-lab-guide/#add-services-to-the-fabric","title":"Add Services to the Fabric","text":"<p>The next step is to add Vlans and SVIs to the fabric. The services data model file <code>SITE1_NETWORK_SERVICES.yml</code> is pre-populated with Vlans and SVIs <code>10</code> and <code>20</code> in the OVERLAY VRF.</p> <p>Open <code>SITE1_NETWORK_SERVICES.yml</code> and uncomment lines 1-16, then run the build &amp; deploy process again.</p> Tip <p> In VS Code, you can toggle comments on/off by selecting the text and pressing windows Ctrl + / or mac Cmd + /.</p> <p>Build the Configs</p> <pre><code>make build-site-1\n</code></pre> <p>Deploy the Configs</p> <pre><code>make deploy-site-1\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#verification_1","title":"Verification","text":"<p>Now lets go back to node <code>s1-leaf1</code> and verify the new SVIs exist, their IP addresses, any changes to the EVPN overlay and corresponding VXLAN configurations, as well as the EVPN control-plane now that we have some layer 3 data interfaces.</p> <ol> <li> <p>Verify VLAN SVIs 10 and 20 exist.</p> <p>Command</p> <pre><code>show ip interface brief\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show ip interface brief\n                                                                            Address\nInterface         IP Address            Status       Protocol            MTU    Owner\n----------------- --------------------- ------------ -------------- ----------- -------\nEthernet2         172.16.1.1/31         up           up                 1500\nEthernet3         172.16.1.3/31         up           up                 1500\nLoopback0         10.250.1.3/32         up           up                65535\nLoopback1         10.255.1.3/32         up           up                65535\nManagement0       192.168.0.12/24       up           up                 1500\nVlan10            10.10.10.1/24         up           up                 1500\nVlan20            10.20.20.1/24         up           up                 1500\nVlan1199          unassigned            up           up                 9164\nVlan3009          10.252.1.0/31         up           up                 1500\nVlan4093          10.252.1.0/31         up           up                 1500\nVlan4094          10.251.1.0/31         up           up                 1500\n</code></pre> Where did those VLANs come from? <p>You should notice some VLANs that we didn't define anywhere in the <code>_NETWORK_SERVICES.yml</code> data model which aren't related to MLAG.  Specifically, these will be VLAN SVIs Vlan1199 and Vlan3009.</p> <p>Vlan1199 is dynamically created and assigned for the OVERLAY vrf to VNI mapping under the VXLAN interface.  You can verify this by looking at the show interface vxlan 1 output.  Remember, we defined VNI 10 as the <code>vrf_vni</code> in our data model. <pre><code>Dynamic VLAN to VNI mapping for 'evpn' is\n[1199, 10]\n</code></pre></p> <p>Vlan3009 was also auto-configured by AVD for an iBGP peering between <code>s1-leaf1</code> and <code>s1-leaf2</code> in the OVERLAY vrf.  You can verify this by looking at the interface configuration, and BGP peering for that vrf.</p> <p>Interface Configuration <pre><code>s1-leaf1#show run interface vlan 3009\ninterface Vlan3009\n  description MLAG_PEER_L3_iBGP: vrf OVERLAY\n  mtu 1500\n  vrf OVERLAY\n  ip address 10.252.1.0/31\n</code></pre></p> <p>Overlay vrf BGP Peering <pre><code>s1-leaf1#show ip bgp summary vrf OVERLAY\nBGP summary information for VRF OVERLAY\nRouter identifier 10.250.1.3, local AS number 65101\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor   V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-leaf2                 10.252.1.1 4 65101             19        20    0    0 00:11:30 Estab   5      5\n</code></pre></p> </li> <li> <p>Verify VLANs 10 and 20, and vrf OVERLAY are now mapped to the appropriate VNIs under the <code>vxlan1</code> interface.</p> <p>Command</p> <pre><code>show run interface vxlan 1\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#sho run int vxlan 1\ninterface Vxlan1\n  description s1-leaf1_VTEP\n  vxlan source-interface Loopback1\n  vxlan virtual-router encapsulation mac-address mlag-system-id\n  vxlan udp-port 4789\n  vxlan vlan 10 vni 10010\n  vxlan vlan 20 vni 10020\n  vxlan vrf OVERLAY vni 10\n</code></pre> </li> <li> <p>Verify we are flooding to the correct remote VTEPs based on what we have learned across the EVPN overlay.</p> <p>Command</p> <pre><code>show interface vxlan1\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#sho int vxlan 1\nVxlan1 is up, line protocol is up (connected)\n  Hardware is Vxlan\n  Description: s1-leaf1_VTEP\n  Source interface is Loopback1 and is active with 10.255.1.3\n  Listening on UDP port 4789\n  Replication/Flood Mode is headend with Flood List Source: EVPN\n  Remote MAC learning via EVPN\n  VNI mapping to VLANs\n  Static VLAN to VNI mapping is\n    [10, 10010]       [20, 10020]\n  Dynamic VLAN to VNI mapping for 'evpn' is\n    [1199, 10]\n  Note: All Dynamic VLANs used by VCS are internal VLANs.\n        Use 'show vxlan vni' for details.\n  Static VRF to VNI mapping is\n  [OVERLAY, 10]\n  Headend replication flood vtep list is:\n    10 10.255.1.5      10.255.1.7\n    20 10.255.1.5      10.255.1.7\n  MLAG Shared Router MAC is 021c.73c0.c612\n</code></pre> </li> <li> <p>Finally, lets verify we have IMET (1) routes for each VLAN and VTEP in the EVPN overlay.</p> <ol> <li>IMET, or Type-3 routes are required for Broadcast, Unknown Unicast and Multicast (BUM) traffic delivery across EVPN networks.</li> </ol> <p>Command</p> <pre><code>show bgp evpn route-type imet\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show bgp evpn route-type imet\nBGP routing table information for VRF default\nRouter identifier 10.250.1.3, local AS number 65101\nRoute status codes: * - valid, &gt; - active, S - Stale, E - ECMP head, e - ECMP\n                    c - Contributing to ECMP, % - Pending best path selection\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n* &gt;      RD: 10.250.1.3:10010 imet 10.255.1.3\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.1.3:10020 imet 10.255.1.3\n                                -                     -       -       0       i\n* &gt;Ec    RD: 10.250.1.5:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.5:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.7:10010 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.7:10010 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n* &gt;Ec    RD: 10.250.1.7:10020 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.7:10020 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n* &gt;Ec    RD: 10.250.1.8:10010 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.8:10010 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n* &gt;Ec    RD: 10.250.1.8:10020 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.8:10020 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n</code></pre> </li> </ol> <p>You can verify the recent configuration session was created.</p> Info <p>When the configuration is applied via a configuration session, EOS will create a \"checkpoint\" of the configuration. This checkpoint is a snapshot of the device's running configuration as it was prior to the configuration session being committed.</p> <pre><code>show clock\n</code></pre> <pre><code>show configuration sessions detail\n</code></pre> <p>List the recent checkpoints.</p> <pre><code>show config checkpoints\n</code></pre> <p>View the contents of the latest checkpoint file.</p> <pre><code>more checkpoint:&lt; filename &gt;\n</code></pre> <p>See the difference between the running config and the latest checkpoint file.</p> Tip <p>This will show the differences between the current device configuration and the configuration before we did our <code>make deploy</code> command.</p> <pre><code>diff checkpoint:&lt; filename &gt; running-config\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#add-ports-for-hosts","title":"Add Ports for Hosts","text":"<p>Let's configure port-channels to our hosts (<code>s1-host1</code> and <code>s1-host2</code>).</p> <p>Open <code>SITE1_CONNECTED_ENDPOINTS.yml</code> and uncomment lines 17-45, then run the build &amp; deploy process again.</p> <pre><code>make build-site-1\n</code></pre> <pre><code>make deploy-site-1\n</code></pre> <p>At this point, hosts should be able to ping each other across the fabric.</p> <p>From <code>s1-host1</code>, run a ping to <code>s1-host2</code>.</p> <pre><code>ping 10.20.20.100\n</code></pre> <pre><code>PING 10.20.20.100 (10.20.20.100) 72(100) bytes of data.\n80 bytes from 10.20.20.100: icmp_seq=1 ttl=63 time=30.2 ms\n80 bytes from 10.20.20.100: icmp_seq=2 ttl=63 time=29.5 ms\n80 bytes from 10.20.20.100: icmp_seq=3 ttl=63 time=28.8 ms\n80 bytes from 10.20.20.100: icmp_seq=4 ttl=63 time=24.8 ms\n80 bytes from 10.20.20.100: icmp_seq=5 ttl=63 time=26.2 ms\n</code></pre> Success <p>Site 1 fabric is now complete.</p>"},{"location":"l3ls/l3ls-lab-guide/#step-4-site-2","title":"Step 4 - Site 2","text":"<p>Repeat the previous three steps for Site 2.</p> <ul> <li>Add Services</li> <li>Add Ports</li> <li>Build and Deploy Configs</li> <li>Verify ping traffic between hosts <code>s2-host1</code> and <code>s2-host2</code></li> </ul> <p>At this point, you should be able to ping between hosts within a site but not between sites. For this, we need to build connectivity to the <code>DCI Network</code>. This is covered in the next section.</p>"},{"location":"l3ls/l3ls-lab-guide/#step-5-connect-sites-to-dci-network","title":"Step 5 - Connect Sites to DCI Network","text":"<p>The DCI Network is defined by the <code>l3_edge</code> data model. Full data model documentation is located here.</p> <p>The data model defines P2P links (<code>/31s</code>) on the border leafs by using a combination of the ipv4_pool and the node id in the p2p_links section. See details in the graphic below. Each border leaf has a single link to its peer on interface <code>Ethernet4</code>.  In the data model, you will notice the parameter for include_in_underlay_protocol, which is set to true.  This tells AVD to render the appropriate BGP configurations for route peering.</p> <p></p>"},{"location":"l3ls/l3ls-lab-guide/#add-p2p-links-for-dci-connectivity-for-site-1-and-2","title":"Add P2P Links for DCI connectivity for Site 1 and 2","text":"<p>Enable the <code>l3_edge</code> dictionary (shown below) by uncommenting it from the <code>global_vars/global_dc_vars.yml</code>:</p> <pre><code># L3 Edge port definitions. This can be any port in the entire Fabric, where IP interfaces are defined.\nl3_edge:\n  # Define a new IP pool that will be used to assign IP addresses to L3 Edge interfaces.\n  p2p_links_ip_pools:\n    - name: S1_to_S2_IP_pool\n      ipv4_pool: 172.16.255.0/24\n  # Define a new link profile which will match the IP pool, the used ASNs and include the defined interface into underlay routing\n  p2p_links_profiles:\n    - name: S1_to_S2_profile\n      ip_pool: S1_to_S2_IP_pool\n      as: [ 65103, 65203 ]\n      include_in_underlay_protocol: true\n  # Define each P2P L3 link and link the nodes, the interfaces and the profile used.\n  p2p_links:\n    - id: 1\n      nodes: [ s1-brdr1, s2-brdr1 ]\n      interfaces: [ Ethernet4, Ethernet4 ]\n      profile: S1_to_S2_profile\n    - id: 2\n      nodes: [ s1-brdr2, s2-brdr2 ]\n      interfaces: [ Ethernet5, Ethernet5 ]\n      profile: S1_to_S2_profile\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#build-and-deploy-dci-network-connectivity","title":"Build and Deploy DCI Network connectivity","text":"<pre><code>make build-site-1 build-site-2 deploy-site-1 deploy-site-2\n</code></pre> Tip <p>Daisy chaining \"Makesies\" is a great way to run a series of tasks with a single CLI command </p>"},{"location":"l3ls/l3ls-lab-guide/#verification_2","title":"Verification","text":"<p>Now that we have built and deployed our configurations for our DCI IPv4 underlay connectivity, lets see what was done.  Looking at the data model above, we see we only defined a pool of IP addresses with a /24 mask which AVD will use to auto-alllocated a subnet per connection.  Additionally, we can see that <code>s1-brdr1</code> connects to its peer <code>s2-brdr1</code> via interface <code>Ethernet4</code>, and <code>s1-brdr2</code> connects to its peer <code>s2-brdr2</code> via interface <code>Ethernet5</code>.  Using that data model, here is what we expect to see configured.  You can verify this by logging into each border leaf and checking with show ip interface brief.</p> Host Interface IP Address s1-brdr1 Ethernet4 172.16.255.0/31 s2-brdr1 Ethernet4 172.16.255.1/31 s1-brdr2 Ethernet5 172.16.255.2/31 s2-brdr2 Ethernet5 172.16.255.3/31 <p>Now lets verify the underlay connectivity and routing across the DCI network.</p> <ol> <li> <p>From <code>s1-brdr1</code>, check the IPv4 underlay peering to its neighbor at Site 2, <code>s2-brdr1</code>.</p> <p>Command</p> <pre><code>show ip bgp summary\n</code></pre> <p>Expected Output</p> <pre><code>s1-brdr1#sho ip bgp summary\nBGP summary information for VRF default\nRouter identifier 10.250.1.7, local AS number 65103\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor     V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-brdr2                 10.252.1.9   4 65103            103       100    0    0 01:13:03 Estab   21     21\n  s1-spine1_Ethernet7      172.16.1.16  4 65100             98        99    0    0 01:13:03 Estab   7      7\n  s1-spine2_Ethernet7      172.16.1.18  4 65100             96        97    0    0 01:13:04 Estab   7      7\n  s2-brdr1                 172.16.255.1 4 65203             27        26    0    0 00:14:19 Estab   11     11\n</code></pre> </li> <li> <p>From <code>s1-brdr1</code>, check its routing table in the default VRF and look for any prefixes learned from peer <code>172.16.255.1</code> via interface <code>Ethernet4</code>.  We will be looking for anything with a third octet of .2 which signifies Site 2.</p> <p>Command</p> <pre><code>show ip route\n</code></pre> <p>Expected Output</p> <pre><code>s1-brdr1#show ip route\n\nVRF: default\nSource Codes:\n      C - connected, S - static, K - kernel,\n      O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1,\n      E2 - OSPF external type 2, N1 - OSPF NSSA external type 1,\n      N2 - OSPF NSSA external type2, B - Other BGP Routes,\n      B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1,\n      I L2 - IS-IS level 2, O3 - OSPFv3, A B - BGP Aggregate,\n      A O - OSPF Summary, NG - Nexthop Group Static Route,\n      V - VXLAN Control Service, M - Martian,\n      DH - DHCP client installed default route,\n      DP - Dynamic Policy Route, L - VRF Leaked,\n      G  - gRIBI, RC - Route Cache Route,\n      CL - CBF Leaked Route\n\nGateway of last resort:\nS        0.0.0.0/0 [1/0]\n          via 192.168.0.1, Management0\n\nB E      10.250.1.1/32 [200/0]\n          via 172.16.1.16, Ethernet2\nB E      10.250.1.2/32 [200/0]\n          via 172.16.1.18, Ethernet3\nB E      10.250.1.3/32 [200/0]\n          via 172.16.1.16, Ethernet2\n          via 172.16.1.18, Ethernet3\nB E      10.250.1.4/32 [200/0]\n          via 172.16.1.16, Ethernet2\n          via 172.16.1.18, Ethernet3\nB E      10.250.1.5/32 [200/0]\n          via 172.16.1.16, Ethernet2\n          via 172.16.1.18, Ethernet3\nB E      10.250.1.6/32 [200/0]\n          via 172.16.1.16, Ethernet2\n          via 172.16.1.18, Ethernet3\nC        10.250.1.7/32\n          directly connected, Loopback0\nB I      10.250.1.8/32 [200/0]\n          via 10.252.1.9, Vlan4093\nB E      10.250.2.1/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.2/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.3/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.4/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.5/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.6/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.7/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.250.2.8/32 [200/0]\n          via 172.16.255.1, Ethernet4\nC        10.251.1.8/31\n          directly connected, Vlan4094\nC        10.252.1.8/31\n          directly connected, Vlan4093\nB E      10.255.1.3/32 [200/0]\n          via 172.16.1.16, Ethernet2\n          via 172.16.1.18, Ethernet3\nB E      10.255.1.5/32 [200/0]\n          via 172.16.1.16, Ethernet2\n          via 172.16.1.18, Ethernet3\nC        10.255.1.7/32\n          directly connected, Loopback1\nB E      10.255.2.3/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.255.2.5/32 [200/0]\n          via 172.16.255.1, Ethernet4\nB E      10.255.2.7/32 [200/0]\n          via 172.16.255.1, Ethernet4\nC        172.16.1.16/31\n          directly connected, Ethernet2\nC        172.16.1.18/31\n          directly connected, Ethernet3\nC        172.16.255.0/31\n          directly connected, Ethernet4\nC        192.168.0.0/24\n          directly connected, Management0\n</code></pre> </li> <li> <p>From <code>s1-brdr2</code>, check the IPv4 underlay peering to its neighbor at Site 2, <code>s2-brdr2</code>.</p> <p>Command</p> <pre><code>show ip bgp summary\n</code></pre> <p>Expected Output</p> <pre><code>s1-brdr2#show ip bgp summary\nBGP summary information for VRF default\nRouter identifier 10.250.1.8, local AS number 65103\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor     V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-brdr1                 10.252.1.8   4 65103            110       113    0    0 01:21:29 Estab   21     21\n  s1-spine1_Ethernet8      172.16.1.20  4 65100            107       112    0    0 01:21:30 Estab   7      7\n  s1-spine2_Ethernet8      172.16.1.22  4 65100            112       108    0    0 01:21:29 Estab   7      7\n  s2-brdr2                 172.16.255.3 4 65203             36        36    0    0 00:22:45 Estab   11     11\n</code></pre> </li> </ol> DCI Underlay Complete <p>If your BGP peerings match above and you have the correct routes in your routing table, the DCI network is successfully connected!</p>"},{"location":"l3ls/l3ls-lab-guide/#step-6-enable-evpn-gateway-functionality","title":"Step 6 - Enable EVPN Gateway Functionality","text":"<p>Now that we have built and deployed our fabrics for both data centers, Site 1 and Site 2, we will need to enable EVPN gateway functionality on our border leafs so layer 2 and layer 3 traffic can move between the data centers across the EVPN overlay.</p> <p>In order to do this, we will need to uncomment the necessary data model to enable EVPN gateway for the border leafs.</p> <p>Below you will see the data model snippets from <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code> and <code>sites/site_2/group_vars/SITE2_FABRIC.yml</code>, for the <code>S1_BRDR</code> and <code>S2_BRDR</code> node groups.  To enable EVPN gateway functionality you will need to modify the <code>SITE1_FABRIC.yml</code> vars file, and uncomment the below highlighted sections.  Uncomment the same sections from the <code>SITE2_FABRIC.yml</code> vars file.</p> SITE1_FABRIC.ymlSITE2_FABRIC.yml <pre><code>- group: S1_BRDR\n  bgp_as: 65103\n  # evpn_gateway:\n  #   evpn_l2:\n  #     enabled: true\n  #   evpn_l3:\n  #     enabled: true\n  #     inter_domain: true\n  nodes:\n    - name: s1-brdr1\n      id: 5\n      mgmt_ip: 192.168.0.100/24\n      uplink_switch_interfaces: [ Ethernet7, Ethernet7 ]\n      # evpn_gateway:\n      #   remote_peers:\n      #     - hostname: s2-brdr1\n      #       bgp_as: 65203\n      #       ip_address: 10.255.2.7\n    - name: s1-brdr2\n      id: 6\n      mgmt_ip: 192.168.0.101/24\n      uplink_switch_interfaces: [ Ethernet8, Ethernet8 ]\n      # evpn_gateway:\n      #   remote_peers:\n      #     - hostname: s2-brdr2\n      #       bgp_as: 65203\n      #       ip_address: 10.255.2.8\n</code></pre> <pre><code>- group: S2_BRDR\n  bgp_as: 65203\n  # evpn_gateway:\n  #   evpn_l2:\n  #     enabled: true\n  #   evpn_l3:\n  #     enabled: true\n  #     inter_domain: true\n  nodes:\n    - name: s2-brdr1\n      id: 5\n      mgmt_ip: 192.168.0.200/24\n      uplink_switch_interfaces: [ Ethernet7, Ethernet7 ]\n      # evpn_gateway:\n      #   remote_peers:\n      #     - hostname: s1-brdr1\n      #       bgp_as: 65103\n      #       ip_address: 10.255.1.7\n    - name: s2-brdr2\n      id: 6\n      mgmt_ip: 192.168.0.201/24\n      uplink_switch_interfaces: [ Ethernet8, Ethernet8 ]\n      # evpn_gateway:\n      #   remote_peers:\n      #     - hostname: s1-brdr2\n      #       bgp_as: 65103\n      #       ip_address: 10.255.1.8\n</code></pre> Unified Fabric <p>If deploying a multi-site fabric with AVD, and using a single inventory file to contain all sites, the evpn_gateway/remote_peers vars for <code>bgp_as</code> and <code>ip_address</code> do NOT need to be populated.  Since AVD will know about all these nodes from the single inventory file, it will know those variables and be able to use them to render the configuration.  Since we have split the sites for complexity sake, we do have to define them here.</p>"},{"location":"l3ls/l3ls-lab-guide/#build-and-deploy-changes-for-evpn-gateway-functionality","title":"Build and Deploy Changes for EVPN Gateway Functionality","text":"<pre><code>make build-site-1 build-site-2 deploy-site-1 deploy-site-2\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#verification_3","title":"Verification","text":"<p>Now lets check and make sure the correct configurations were build and applied, and the EVPN gateways are functioning.</p> <p>From nodes <code>s1-brdr1</code> and <code>s1-brdr2</code>, we can check the following show commands.</p> <ol> <li> <p>Verify the new BGP configurations were rendered and applied for the remote gateways.</p> <p>Command</p> <pre><code>show run section bgp\n</code></pre> <p>Look for the below new configurations relevant to the EVPN gateways.</p> s1-brdr1s1-brdr2 <pre><code>router bgp 65103\n  ...\n  neighbor EVPN-OVERLAY-CORE peer group\n  neighbor EVPN-OVERLAY-CORE update-source Loopback0\n  neighbor EVPN-OVERLAY-CORE bfd\n  neighbor EVPN-OVERLAY-CORE ebgp-multihop 15\n  neighbor EVPN-OVERLAY-CORE send-community\n  neighbor EVPN-OVERLAY-CORE maximum-routes 0\n  ...\n  neighbor 10.255.2.7 peer group EVPN-OVERLAY-CORE\n  neighbor 10.255.2.7 remote-as 65203\n  neighbor 10.255.2.7 description s2-brdr1\n  ...\n  vlan 10\n      ...\n      route-target import export evpn domain remote 10010:10010\n      ...\n  !\n  vlan 20\n      ...\n      route-target import export evpn domain remote 10020:10020\n      ...\n  !\n  address-family evpn\n      neighbor EVPN-OVERLAY-CORE activate\n      neighbor EVPN-OVERLAY-CORE domain remote\n      ...\n      neighbor default next-hop-self received-evpn-routes route-type ip-prefix inter-domain\n</code></pre> <pre><code>router bgp 65103\n  ...\n  neighbor EVPN-OVERLAY-CORE peer group\n  neighbor EVPN-OVERLAY-CORE update-source Loopback0\n  neighbor EVPN-OVERLAY-CORE bfd\n  neighbor EVPN-OVERLAY-CORE ebgp-multihop 15\n  neighbor EVPN-OVERLAY-CORE send-community\n  neighbor EVPN-OVERLAY-CORE maximum-routes 0\n  ...\n  neighbor 10.255.2.8 peer group EVPN-OVERLAY-CORE\n  neighbor 10.255.2.8 remote-as 65203\n  neighbor 10.255.2.8 description s2-brdr2\n  ...\n  vlan 10\n      ...\n      route-target import export evpn domain remote 10010:10010\n      ...\n  !\n  vlan 20\n      ...\n      route-target import export evpn domain remote 10020:10020\n      ...\n  !\n  address-family evpn\n      neighbor EVPN-OVERLAY-CORE activate\n      neighbor EVPN-OVERLAY-CORE domain remote\n      ...\n      neighbor default next-hop-self received-evpn-routes route-type ip-prefix inter-domain\n</code></pre> </li> <li> <p>Verify the EVPN overlay peerings to Site 2.</p> <p>Command</p> <pre><code>show bgp evpn summary\n</code></pre> <p>Look for the peerings to the corresponding Site 2 node.</p> s1-brdr1s1-brdr2 <pre><code>s1-brdr1#sho bgp evpn summ\nBGP summary information for VRF default\nRouter identifier 10.250.1.7, local AS number 65103\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor   V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-spine1                10.250.1.1 4 65100            151       150    0    0 01:37:10 Estab   20     20\n  s1-spine2                10.250.1.2 4 65100            152       151    0    0 01:37:11 Estab   20     20\n  s2-brdr1                 10.250.2.7 4 65203             14        14    0    0 00:00:08 Estab   19     19\n</code></pre> <pre><code>s1-brdr2#show bgp evpn summary\nBGP summary information for VRF default\nRouter identifier 10.250.1.8, local AS number 65103\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor   V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-spine1                10.250.1.1 4 65100            158       152    0    0 01:38:37 Estab   20     20\n  s1-spine2                10.250.1.2 4 65100            156       147    0    0 01:38:37 Estab   20     20\n  s2-brdr2                 10.250.2.8 4 65203             15        15    0    0 00:01:35 Estab   19     19\n</code></pre> </li> <li> <p>Finally, lets verify which routes we are seeing in the EVPN table from Site 2.  If you recall when we checked this within each site, we had an IMET route per VLAN, from each VTEP.  In this instance, since we are using EVPN gateway functionality to summarize the routes from the other site, we should only see 1 IMET route per VLAN to the remote EVPN gateway.</p> <p>Command</p> <pre><code>show bgp evpn route-type imet\n</code></pre> <p>Expected Output</p> <pre><code>s1-brdr1#sho bgp evpn route-type imet\nBGP routing table information for VRF default\nRouter identifier 10.250.1.7, local AS number 65103\nRoute status codes: * - valid, &gt; - active, S - Stale, E - ECMP head, e - ECMP\n                    c - Contributing to ECMP, % - Pending best path selection\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n* &gt;Ec    RD: 10.250.1.3:10010 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.3:10010 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.3:10020 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.3:10020 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.4:10010 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.4:10010 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.4:10020 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.4:10020 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.5:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.5:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10010 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10020 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;      RD: 10.250.1.7:10010 imet 10.255.1.7\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.1.7:10020 imet 10.255.1.7\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.1.7:10010 imet 10.255.1.7 remote\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.1.7:10020 imet 10.255.1.7 remote\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.2.7:10010 imet 10.255.2.7 remote\n                                10.255.2.7            -       100     0       65203 i\n* &gt;      RD: 10.250.2.7:10020 imet 10.255.2.7 remote\n                                10.255.2.7            -       100     0       65203 i\n</code></pre> </li> </ol> EVPN Gateways Active <p>If all your peerings are established and you have the correct IMET routes in the EVPN table, then your EVPN gateways are functioning!  Lets move on to a final connectivity test.</p>"},{"location":"l3ls/l3ls-lab-guide/#final-fabric-test","title":"Final Fabric Test","text":"<p>At this point your full Layer 3 Leaf Spine with EVPN VXLAN and EVPN gateway functionality should be ready to go. Lets perform some final tests to verify everything is working.</p> <p>From <code>s1-host1</code> ping both <code>s2-host1</code> &amp; <code>s2-host2</code>.</p> <pre><code># s2-host1\nping 10.10.10.200\n</code></pre> <pre><code># s2-host2\nping 10.20.20.200\n</code></pre> Great Success! <p>You have built a multi-site L3LS network with an EVPN/VXLAN overlay and EVPN Gateway functionality without touching the CLI on a single switch!</p>"},{"location":"l3ls/l3ls-lab-guide/#day-2-operations","title":"Day 2 Operations","text":"<p>Our multi-site L3LS EVPN/VXLAN network is working great. But, before too long, it will be time to change our configurations. Lucky for us, that time is today!</p>"},{"location":"l3ls/l3ls-lab-guide/#cleaning-up","title":"Cleaning Up","text":"<p>Before going any further, let's ensure we have a clean repo by committing the changes we've made up to this point. The CLI commands below can accomplish this, but the VS Code Source Control GUI can be used as well.</p> <pre><code>git add .\ngit commit -m 'Your message here'\n</code></pre> <p>Next, we'll want to push these changes to our forked repository on GitHub.</p> <pre><code>git push\n</code></pre> <p>If this is our first time pushing to our forked repository, then VS Code will provide us with the following sign-in prompt:</p> <p></p> <p>Choose Allow, and another prompt will come up, showing your unique login code:</p> <p></p> <p>Choose Copy &amp; Continue to GitHub, and another prompt will come up asking if it's ok to open an external website (GitHub).</p> <p></p> <p>Choose Open and then an external site (GitHub) will open, asking for your login code.</p> <p></p> <p>Paste in your login code and choose Continue. You will then be prompted to Authorize VS Code.</p> <p></p> <p>Choose Authorize Visual-Studio-Code, and you should be presented with the coveted Green Check Mark!</p> <p></p> <p>Whew! Alright. Now that we have that complete, let's keep moving...</p>"},{"location":"l3ls/l3ls-lab-guide/#branching-out","title":"Branching Out","text":"<p>Before jumping in and modifying our files, we'll create a branch named banner-syslog in our forked repository to work on our changes. We can create our branch in multiple ways, but we'll use the <code>git switch</code> command with the <code>-c</code> parameter to create our new branch.</p> <pre><code>git switch -c banner-syslog\n</code></pre> <p>After entering this command, we should see our new branch name reflected in the terminal. It will also be reflected in the status bar in the lower left-hand corner of our VS Code window (you may need to click the refresh icon before this is shown).</p> <p>Now we're ready to start working on our changes .</p>"},{"location":"l3ls/l3ls-lab-guide/#login-banner","title":"Login Banner","text":"<p>When we initially deployed our multi-site topology, we should have included a login banner on all our switches. Let's take a look at the AVD documentation site to see what the data model is for this configuration.</p> <p>The banner on all of our switches will be the same. After reviewing the AVD documentation, we know we can accomplish this by defining the <code>banners</code> input variable in our <code>global_vars/global_dc_vars.yml</code> file.</p> <p>Add the code block below to <code>global_vars/global_dc_vars.yml</code>.</p> <pre><code># Login Banner\nbanners:\n  motd: |\n    You shall not pass. Unless you are authorized. Then you shall pass.\n    EOF\n</code></pre> Yes, that \"EOF\" is important! <p>Ensure the entire code snippet above is copied; including the <code>EOF</code>. This must be present for the configuration to be considered valid</p> <p>Next, let's build out the configurations and documentation associated with this change.</p> <pre><code>make build-site-1 build-site-2\n</code></pre> <p>Please take a minute to review the results of our five lines of YAML. When finished reviewing the changes, let's commit them.</p> <p>As usual, there are a few ways of doing this, but the CLI commands below will get the job done:</p> <pre><code>git add .\ngit commit -m 'add banner'\n</code></pre> <p>So far, so good! Before we publish our branch and create a Pull Request though, we have some more work to do...</p>"},{"location":"l3ls/l3ls-lab-guide/#syslog-server","title":"Syslog Server","text":"<p>Our next Day 2 change is adding a syslog server configuration to all our switches. Once again, we'll take a look at the AVD documentation site to see the data model associated with the <code>logging</code> input variable.</p> <p>Like our banner operation, the syslog server configuration will be consistent on all our switches. Because of this, we can also put this into our <code>global_vars/global_dc_vars.yml</code> file.</p> <p>Add the code block below to <code>global_vars/global_dc_vars.yml</code>.</p> <pre><code># Syslog\nlogging:\n  vrfs:\n    - name: default\n      source_interface: Management0\n      hosts:\n        - name: 10.200.0.108\n        - name: 10.200.1.108\n</code></pre> <p>Finally, let's build out our configurations.</p> <pre><code>make build-site-1 build-site-2\n</code></pre> <p>Take a minute, using the source control feature in VS Code, to review what has changed as a result of our work.</p> <p>At this point, we have our Banner and Syslog configurations in place. The configurations look good, and we're ready to share this with our team for review. In other words, it's time to publish our branch to the remote origin (our forked repo on GitHub) and create the Pull Request (PR)!</p> <p>There are a few ways to publish the <code>banner-syslog</code> branch to our forked repository. The commands below will accomplish this via the CLI:</p> <pre><code>git add .\ngit commit -m 'add syslog'\ngit push --set-upstream origin banner-syslog\n</code></pre> <p>On our forked repository, let's create the Pull Request.</p> <p>When creating the PR, ensure that the <code>base repository</code> is the main branch of your fork. This can be selected via the dropdown as shown below:</p> <p></p> <p>Take a minute to review the contents of the PR. Assuming all looks good, let's earn the YOLO GitHub badge by approving and merging your PR!</p> Tip <p>Remember to delete the banner-syslog branch after performing the merge - Keep that repo clean!</p> <p>Once merged, let's switch back to our <code>main</code> branch and pull down our merged changes.</p> <pre><code>git switch main\ngit pull\n</code></pre> <p>Then, let's delete our now defunct banner-syslog branch.</p> <pre><code>git branch -D banner-syslog\n</code></pre> <p>Finally, let's deploy our changes.</p> <pre><code>make deploy-site-1 deploy-site-2\n</code></pre> <p>Once completed, we should see our banner when logging into any switch. The output of the <code>show logging</code> command should also have our newly defined syslog servers.</p>"},{"location":"l3ls/l3ls-lab-guide/#adding-additional-vlans","title":"Adding additional VLANs","text":"<p>One of the many benefits of AVD is the ability to deploy new services very quickly and efficiently by modifying a small amount of data model. Lets add some new VLANs to our fabric.</p> <p>For this we will need to modify the two <code>_NETWORK_SERVICES.yml</code> data model vars files. To keep things simple we will add two new VLANs, 30 and 40.</p> <p>Copy the following pieces of data model, and paste right below the last VLAN entry, in both <code>SITE1_NETWORK_SERVICES.yml</code> and <code>SITE2_NETWORK_SERVICES.yml</code>.  Ensure the <code>-id:</code> entries all line up.</p> <pre><code>- id: 30\n  name: 'Thirty'\n  enabled: true\n  ip_address_virtual: 10.30.30.1/24\n- id: 40\n  name: 'Forty'\n  enabled: true\n  ip_address_virtual: 10.40.40.1/24\n</code></pre> <p>When complete, your vars file should look like this.</p> <pre><code>---\ntenants:\n  - name: S2_FABRIC\n    mac_vrf_vni_base: 10000\n    vrfs:\n      - name: OVERLAY\n        vrf_vni: 10\n        svis:\n          - id: 10\n            name: 'Ten'\n            enabled: true\n            ip_address_virtual: 10.10.10.1/24\n          - id: 20\n            name: 'Twenty'\n            enabled: true\n            ip_address_virtual: 10.20.20.1/24\n          - id: 30\n            name: 'Thirty'\n            enabled: true\n            ip_address_virtual: 10.30.30.1/24\n          - id: 40\n            name: 'Forty'\n            enabled: true\n            ip_address_virtual: 10.40.40.1/24\n</code></pre> <p>Finally, let's build out and deploy our configurations.</p> <pre><code>make build-site-1 build-site-2 deploy-site-1 deploy-site-2\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#verification_4","title":"Verification","text":"<p>Now lets jump into one of the nodes, <code>s1-leaf1</code>, and check that our new VLAN SVIs were configured, as well as what we see in the VXLAN interface and EVPN table for both local and remote VTEPs.</p> <ol> <li> <p>Check that the VLAN SVIs were configured.</p> <p>Command</p> <pre><code>show ip interface brief\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show ip interface brief\n                                                                                Address\nInterface         IP Address            Status       Protocol            MTU    Owner\n----------------- --------------------- ------------ -------------- ----------- -------\nEthernet2         172.16.1.1/31         up           up                 1500\nEthernet3         172.16.1.3/31         up           up                 1500\nLoopback0         10.250.1.3/32         up           up                65535\nLoopback1         10.255.1.3/32         up           up                65535\nManagement0       192.168.0.12/24       up           up                 1500\nVlan10            10.10.10.1/24         up           up                 1500\nVlan20            10.20.20.1/24         up           up                 1500\nVlan30            10.30.30.1/24         up           up                 1500\nVlan40            10.40.40.1/24         up           up                 1500\nVlan1199          unassigned            up           up                 9164\nVlan3009          10.252.1.0/31         up           up                 1500\nVlan4093          10.252.1.0/31         up           up                 1500\nVlan4094          10.251.1.0/31         up           up                 1500\n</code></pre> </li> <li> <p>Lets check the <code>VXLAN 1</code> interface and see what changes were made there.</p> <p>Command</p> <pre><code>show run interface vxlan 1\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show run interface vxlan 1\ninterface Vxlan1\n  description s1-leaf1_VTEP\n  vxlan source-interface Loopback1\n  vxlan virtual-router encapsulation mac-address mlag-system-id\n  vxlan udp-port 4789\n  vxlan vlan 10 vni 10010\n  vxlan vlan 20 vni 10020\n  vxlan vlan 30 vni 10030\n  vxlan vlan 40 vni 10040\n  vxlan vrf OVERLAY vni 10\n</code></pre> <p>Command</p> <pre><code>show interface vxlan 1\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#show interface vxlan 1\nVxlan1 is up, line protocol is up (connected)\n  Hardware is Vxlan\n  Description: s1-leaf1_VTEP\n  Source interface is Loopback1 and is active with 10.255.1.3\n  Listening on UDP port 4789\n  Replication/Flood Mode is headend with Flood List Source: EVPN\n  Remote MAC learning via EVPN\n  VNI mapping to VLANs\n  Static VLAN to VNI mapping is\n    [10, 10010]       [20, 10020]       [30, 10030]       [40, 10040]\n\n  Dynamic VLAN to VNI mapping for 'evpn' is\n    [1199, 10]\n  Note: All Dynamic VLANs used by VCS are internal VLANs.\n        Use 'show vxlan vni' for details.\n  Static VRF to VNI mapping is\n  [OVERLAY, 10]\n  Headend replication flood vtep list is:\n    10 10.255.1.5      10.255.1.7\n    20 10.255.1.5      10.255.1.7\n    30 10.255.1.5      10.255.1.7\n    40 10.255.1.5      10.255.1.7\n  MLAG Shared Router MAC is 021c.73c0.c612\n</code></pre> </li> <li> <p>Now, lets check the EVPN table. We can filter the routes to only the new VLANs by specifying the new VNIs, 10030 and 10040.</p> <p>Command</p> <pre><code>show bgp evpn vni 10030\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#sho bgp evpn vni 10030\nBGP routing table information for VRF default\nRouter identifier 10.250.1.3, local AS number 65101\nRoute status codes: * - valid, &gt; - active, S - Stale, E - ECMP head, e - ECMP\n                    c - Contributing to ECMP, % - Pending best path selection\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n* &gt;      RD: 10.250.1.3:10030 imet 10.255.1.3\n                                -                     -       -       0       i\n* &gt;Ec    RD: 10.250.1.5:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.7:10030 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.7:10030 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n* &gt;Ec    RD: 10.250.1.8:10030 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.8:10030 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n</code></pre> <p>Command</p> <pre><code>show bgp evpn vni 10040\n</code></pre> <p>Expected Output</p> <pre><code>s1-leaf1#sho bgp evpn vni 10040\nBGP routing table information for VRF default\nRouter identifier 10.250.1.3, local AS number 65101\nRoute status codes: * - valid, &gt; - active, S - Stale, E - ECMP head, e - ECMP\n                    c - Contributing to ECMP, % - Pending best path selection\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n* &gt;      RD: 10.250.1.3:10040 imet 10.255.1.3\n                                -                     -       -       0       i\n* &gt;Ec    RD: 10.250.1.5:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.7:10040 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.7:10040 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n* &gt;Ec    RD: 10.250.1.8:10040 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n*  ec    RD: 10.250.1.8:10040 imet 10.255.1.7\n                                10.255.1.7            -       100     0       65100 65103 i\n</code></pre> </li> <li> <p>Finally, lets check from <code>s1-brdr1</code> for the remote EVPN gateway.</p> <p>Command</p> <pre><code>show bgp evpn vni 10030\n</code></pre> <p>Expected Output</p> <pre><code>s1-brdr1#  sho bgp evpn vni 10030\nBGP routing table information for VRF default\nRouter identifier 10.250.1.7, local AS number 65103\nRoute status codes: * - valid, &gt; - active, S - Stale, E - ECMP head, e - ECMP\n                    c - Contributing to ECMP, % - Pending best path selection\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n* &gt;Ec    RD: 10.250.1.3:10030 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.3:10030 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.4:10030 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.4:10030 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.5:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10030 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;      RD: 10.250.1.7:10030 imet 10.255.1.7\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.1.7:10030 imet 10.255.1.7 remote\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.2.7:10030 imet 10.255.2.7 remote\n                                10.255.2.7            -       100     0       65203 i\n</code></pre> <p>Command</p> <pre><code>show bgp evpn vni 10040\n</code></pre> <p>Expected Output</p> <pre><code>s1-brdr1#  sho bgp evpn vni 10040\nBGP routing table information for VRF default\nRouter identifier 10.250.1.7, local AS number 65103\nRoute status codes: * - valid, &gt; - active, S - Stale, E - ECMP head, e - ECMP\n                    c - Contributing to ECMP, % - Pending best path selection\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n* &gt;Ec    RD: 10.250.1.3:10040 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.3:10040 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.4:10040 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n*  ec    RD: 10.250.1.4:10040 imet 10.255.1.3\n                                10.255.1.3            -       100     0       65100 65101 i\n* &gt;Ec    RD: 10.250.1.5:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.5:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;Ec    RD: 10.250.1.6:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n*  ec    RD: 10.250.1.6:10040 imet 10.255.1.5\n                                10.255.1.5            -       100     0       65100 65102 i\n* &gt;      RD: 10.250.1.7:10040 imet 10.255.1.7\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.1.7:10040 imet 10.255.1.7 remote\n                                -                     -       -       0       i\n* &gt;      RD: 10.250.2.7:10040 imet 10.255.2.7 remote\n                                10.255.2.7            -       100     0       65203 i\n</code></pre> </li> </ol>"},{"location":"l3ls/l3ls-lab-guide/#provisioning-new-switches","title":"Provisioning new Switches","text":"<p>Our network is gaining popularity, and it's time to add a new Leaf pair into the environment! s1-leaf5 and s1-leaf6 are ready to be provisioned, so let's get to it.</p>"},{"location":"l3ls/l3ls-lab-guide/#branch-time","title":"Branch Time","text":"<p>Before jumping in, let's create a new branch for our work. We'll call this branch add-leafs.</p> <pre><code>git switch -c add-leafs\n</code></pre> <p>Now that we have our branch created let's get to work!</p>"},{"location":"l3ls/l3ls-lab-guide/#inventory-update","title":"Inventory Update","text":"<p>First, we'll want to add our new switches, named s1-leaf5 and s1-leaf6, into our inventory file. We'll add them as members of the <code>SITE1_LEAFS</code> group.</p> <p>Add the following two lines under <code>s1-leaf4</code> in <code>sites/site_1/inventory.yml</code>.</p> <pre><code>s1-leaf5:\ns1-leaf6:\n</code></pre> <p>The <code>sites/site_1/inventory.yml</code> file should now look like the example below:</p> sites/site_1/inventory.yml <pre><code>---\nSITE1:\n  children:\n    CVP:\n      hosts:\n        cvp:\n    SITE1_FABRIC:\n      children:\n        SITE1_SPINES:\n          hosts:\n            s1-spine1:\n            s1-spine2:\n        SITE1_LEAFS:\n          hosts:\n            s1-leaf1:\n            s1-leaf2:\n            s1-leaf3:\n            s1-leaf4:\n            s1-brdr1:\n            s1-brdr2:\n            s1-leaf5:\n            s1-leaf6:\n    SITE1_NETWORK_SERVICES:\n      children:\n        SITE1_SPINES:\n        SITE1_LEAFS:\n    SITE1_CONNECTED_ENDPOINTS:\n      children:\n        SITE1_SPINES:\n        SITE1_LEAFS:\n</code></pre> <p>Next, let's add our new Leaf switches into <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <p>These new switches will go into S1_RACK4, leverage MLAG for multi-homing and use BGP ASN# 65104.</p> <p>Just like the other Leaf switches, interfaces <code>Ethernet2</code> and <code>Ethernet3</code> will be used to connect to the spines.</p> <p>On the spines, interface <code>Ethernet9</code> will be used to connect to <code>s1-leaf5</code>, while <code>Ethernet10</code> will be used to connect to <code>s1-leaf6</code>.</p> <p>Starting at line 64, add the following code block into <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <pre><code>- group: S1_RACK4\n  bgp_as: 65104\n  nodes:\n    - name: s1-leaf5\n      id: 5\n      mgmt_ip: 192.168.0.28/24\n      uplink_switch_interfaces: [ Ethernet9, Ethernet9 ]\n    - name: s1-leaf6\n      id: 6\n      mgmt_ip: 192.168.0.29/24\n      uplink_switch_interfaces: [ Ethernet10, Ethernet10 ]\n</code></pre> Warning <p>Make sure the indentation of <code>RACK4</code> is the same as <code>RACK3</code>, which can be found on line 52</p> <p>The <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code> file should now look like the example below:</p> sites/site_1/group_vars/SITE1_FABRIC.yml <pre><code>---\nfabric_name: SITE1_FABRIC\n\n# Set Design Type to L2ls\ndesign:\n  type: l3ls-evpn\n\n# Spine Switches\nspine:\n  defaults:\n    platform: cEOS\n    loopback_ipv4_pool: 10.250.1.0/24\n    bgp_as: 65100\n  nodes:\n    - name: s1-spine1\n      id: 1\n      mgmt_ip: 192.168.0.10/24\n    - name: s1-spine2\n      id: 2\n      mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nl3leaf:\n  defaults:\n    platform: cEOS\n    spanning_tree_priority: 4096\n    spanning_tree_mode: mstp\n    loopback_ipv4_pool: 10.250.1.0/24\n    loopback_ipv4_offset: 2\n    vtep_loopback_ipv4_pool: 10.255.1.0/24\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    uplink_ipv4_pool: 172.16.1.0/24\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n    mlag_peer_ipv4_pool: 10.251.1.0/24\n    mlag_peer_l3_ipv4_pool: 10.252.1.0/24\n    virtual_router_mac_address: 00:1c:73:00:00:99\n  node_groups:\n    - group: S1_RACK1\n      bgp_as: 65101\n      nodes:\n        - name: s1-leaf1\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: S1_RACK2\n      bgp_as: 65102\n      nodes:\n        - name: s1-leaf3\n          id: 3\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 4\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n    - group: S1_BRDR\n      bgp_as: 65103\n      evpn_gateway:\n        evpn_l2:\n          enabled: true\n        evpn_l3:\n          enabled: true\n          inter_domain: true\n      nodes:\n        - name: s1-brdr1\n          id: 5\n          mgmt_ip: 192.168.0.100/24\n          uplink_switch_interfaces: [ Ethernet7, Ethernet7 ]\n          evpn_gateway:\n            remote_peers:\n              - hostname: s2-brdr1\n                bgp_as: 65203\n                ip_address: 10.255.2.7\n        - name: s1-brdr2\n          id: 6\n          mgmt_ip: 192.168.0.101/24\n          uplink_switch_interfaces: [ Ethernet8, Ethernet8 ]\n          evpn_gateway:\n            remote_peers:\n              - hostname: s2-brdr2\n                bgp_as: 65203\n                ip_address: 10.255.2.8\n    - group: S1_RACK4\n      bgp_as: 65104\n      nodes:\n        - name: s1-leaf5\n          id: 5\n          mgmt_ip: 192.168.0.28/24\n          uplink_switch_interfaces: [ Ethernet9, Ethernet9 ]\n        - name: s1-leaf6\n          id: 6\n          mgmt_ip: 192.168.0.29/24\n          uplink_switch_interfaces: [ Ethernet10, Ethernet10 ]\n</code></pre> <p>Next - Let's build the configuration!</p> <pre><code>make build-site-1\n</code></pre> Important <p>Interfaces <code>Ethernet9</code> and <code>Ethernet10</code> do not exist on the Spines. Because of this, we will not run a deploy command since it would fail.</p> <p>Please take a moment and review the results of our changes via the source control functionality in VS Code.</p> <p>Finally, we'll commit our changes and publish our branch. Again, we can use the VS Code Source Control GUI for this, or via the CLI using the commands below:</p> <pre><code>git add .\ngit commit -m 'add leafs'\ngit push --set-upstream origin add-leafs\n</code></pre>"},{"location":"l3ls/l3ls-lab-guide/#backing-out-changes","title":"Backing out changes","text":"<p>Ruh Roh. As it turns out, we should have added these leaf switches to an entirely new site. Oops! No worries, because we used our add-leafs branch, we can switch back to our main branch and then delete our local copy of the add-leafs branch. No harm or confusion related to this change ever hit the main branch!</p> <pre><code>git switch main\ngit branch -D add-leafs\n</code></pre> <p>Finally, we can go out to our forked copy of the repository and delete the add-leafs branch.</p> Great Success! <p>Congratulations. You have now successfully completed initial fabric builds and day 2 operational changes without interacting with any switch CLI!</p>"},{"location":"l3ls/overview/","title":"Arista CI Workshop - Layer 3 Leaf Spine with EVPN/VXLAN","text":"<p>This workshop will leverage open-source tools for configuration development, deployment, and documentation of a Layer 3 Leaf Spine network with EVPN and VXLAN. In addition, the open-source tooling enables us to manage our network environment as code.</p> <p>This section will cover the following:</p> <ul> <li>Arista Validated Designs (AVD) Ansible Collection</li> <li>Network Data Models</li> <li>Initial Deployment (Day 0 Provisioning)</li> <li>Ongoing Operations (Day 2 and Beyond)</li> </ul> <p>Each attendee will receive a dedicated virtual lab environment with Git, VS Code, and Ansible installed and ready to use.</p> <p>Attendees will need the following:</p> <ul> <li>A laptop</li> <li>An account on GitHub</li> <li>Familiarity with the concepts and tools covered in the previous Automation Fundamentals workshop (Git, VS Code, Jinja/YAML, Ansible)</li> </ul>"},{"location":"l3ls/overview/#lab-topology-overview","title":"Lab Topology Overview","text":"<p>Throughout this section, we will use the following dual data center topology. Click on the image to zoom in for details.</p> <p></p>"},{"location":"l3ls/overview/#basic-eos-switch-configuration","title":"Basic EOS Switch Configuration","text":"<p>Basic connectivity between the Ansible controller host and the switches must be established before Ansible can be used to deploy configurations. The following should be configured on all switches:</p> <ul> <li>Switch Hostname</li> <li>IP enabled interface</li> <li>Username and Password defined</li> <li>Management eAPI enabled</li> </ul> Info <p>In the ATD environment, cEOS virtual switches use <code>Management0</code> in the default VRF. When using actual hardware or vEOS switches, <code>Management1</code> is used. The included basic switch configurations may need to be adjusted for your environment.</p> <p>Below is an example basic configuration file for s1-spine1:</p> <pre><code>!\nno aaa root\n!\nusername admin privilege 15 role network-admin secret sha512 $6$eucN5ngreuExDgwS$xnD7T8jO..GBDX0DUlp.hn.W7yW94xTjSanqgaQGBzPIhDAsyAl9N4oScHvOMvf07uVBFI4mKMxwdVEUVKgY/.\n!\nhostname s1-spine1\n!\nmanagement api http-commands\n   no shutdown\n!\ninterface Management0\n   ip address 192.168.0.10/24\n!\nip routing\n!\nip route vrf MGMT 0.0.0.0/0 192.168.0.1\n!\n</code></pre>"},{"location":"l3ls/overview/#ansible-inventory","title":"Ansible Inventory","text":"<p>Our lab L3LS topology contains two sites, <code>Site 1</code> and <code>Site 2</code>. We need to create the Ansible inventory for each site. We have created two separate directories for each site under the <code>sites</code> sub-directory in our repo.</p> <pre><code>\u251c\u2500\u2500 sites/\n  \u251c\u2500\u2500 site_1/\n  \u251c\u2500\u2500 site_2/\n</code></pre> <p>The following is a graphical representation of the Ansible inventory groups and naming scheme used for <code>Site 1</code> in this example. This is replicated for <code>Site 2</code>.</p> <p></p>"},{"location":"l3ls/overview/#avd-fabric-variables","title":"AVD Fabric Variables","text":"<p>To apply AVD variables to the nodes in the fabric, we make use of Ansible group_vars. How and where you define the variables is your choice. The group_vars table below is one example of AVD fabric variables for <code>Site 1</code>.</p> group_vars/ Description SITE1_FABRIC.yml Fabric, Topology, and Device settings SITE1_SPINES.yml Device type for Spines SITE1_LEAFS.yml Device type for Leafs SITE1_NETWORK_SERVICES.yml VLANs, VRFs, SVIs SITE1_CONNECTED_ENDPOINTS.yml Port Profiles and Connected Endpoint settings <p>Each group_vars file is listed in the following tabs.</p> SITE1_FABRICSITE1_SPINESSITE1_LEAFSSITE1_NETWORK_SERVICESSITE1_CONNECTED_ENDPOINTS <p>At the Fabric level (SITE1_FABRIC), the following variables are defined in group_vars/SITE1_FABRIC.yml. The fabric name, design type (l3ls-evpn), node type defaults, interface links, and EVPN gateway functionality are defined at this level. Being a Layer 3 Leaf Spine topology, the leaf nodes will require more variables than the spines. The variables needed for the spines include:</p> <ul> <li>loopback_ipv4_pool</li> <li>bgp_as</li> </ul> <p>The leaf nodes will need the following variables set:</p> <ul> <li>spanning_tree_priority</li> <li>spanning_tree_mode</li> <li>loopback_ipv4_pool</li> <li>loopback_ipv4_offset</li> <li>vtep_loopback_ipv4_pool</li> <li>uplink_switches</li> <li>uplink_interfaces</li> <li>uplink_ipv4_pool</li> <li>mlag_interfaces</li> <li>mlag_peer_ipv4_pool</li> <li>mlag_peer_l3_ipv4_pool</li> <li>virtual_router_mac_address</li> </ul> <p>Variables applied under the node key type (spine/l3leaf) defaults section are inherited by nodes under each type. These variables may be overwritten under the node itself.</p> <p>The spine interface used by a particular leaf is defined from the leaf's perspective with a variable called <code>uplink_switch_interfaces</code>. For example, s1-leaf1 has a unique variable <code>uplink_switch_interfaces: [Ethernet2, Ethernet2]</code> defined. This means that s1-leaf1 is connected to <code>s1-spine1</code> Ethernet2 and <code>s1-spine2</code> Ethernet2, respectively.</p> <pre><code>---\nfabric_name: SITE1_FABRIC\n\n# Set Design Type to L2ls\ndesign:\n  type: l3ls-evpn\n\n# Spine Switches\nspine:\n  defaults:\n    platform: cEOS\n    loopback_ipv4_pool: 10.250.1.0/24\n    bgp_as: 65100\n  nodes:\n    - name: s1-spine1\n      id: 1\n      mgmt_ip: 192.168.0.10/24\n    - name: s1-spine2\n      id: 2\n      mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nl3leaf:\n  defaults:\n    platform: cEOS\n    spanning_tree_priority: 4096\n    spanning_tree_mode: mstp\n    loopback_ipv4_pool: 10.250.1.0/24\n    loopback_ipv4_offset: 2\n    vtep_loopback_ipv4_pool: 10.255.1.0/24\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    uplink_ipv4_pool: 172.16.1.0/24\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n    mlag_peer_ipv4_pool: 10.251.1.0/24\n    mlag_peer_l3_ipv4_pool: 10.252.1.0/24\n    virtual_router_mac_address: 00:1c:73:00:00:99\n  node_groups:\n    - group: S1_RACK1\n      bgp_as: 65101\n      nodes:\n        - name: s1-leaf1\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: S1_RACK2\n      bgp_as: 65102\n      nodes:\n        - name: s1-leaf3\n          id: 3\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 4\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n    - group: S1_BRDR\n      bgp_as: 65103\n      evpn_gateway:\n         evpn_l2:\n           enabled: true\n         evpn_l3:\n           enabled: true\n           inter_domain: true\n      nodes:\n        - name: s1-brdr1\n          id: 5\n          mgmt_ip: 192.168.0.100/24\n          uplink_switch_interfaces: [ Ethernet7, Ethernet7 ]\n          evpn_gateway:\n             remote_peers:\n               - hostname: s2-brdr1\n                 bgp_as: 65203\n                 ip_address: 10.255.2.7\n        - name: s1-brdr2\n          id: 6\n          mgmt_ip: 192.168.0.101/24\n          uplink_switch_interfaces: [ Ethernet8, Ethernet8 ]\n          evpn_gateway:\n             remote_peers:\n               - hostname: s2-brdr2\n                 bgp_as: 65203\n                 ip_address: 10.255.2.8\n</code></pre> <p>In an L3LS design, there are two types of spine nodes: <code>spine</code> and <code>super_spine</code>. In AVD, the node type defines the functionality and the EOS CLI configuration to be generated. For our L3LS topology, we will use node type <code>spine</code>, as we do not have any inter-DC or inter-pod spines.</p> <pre><code>---\ntype: spine\n</code></pre> <p>In an L3LS design, we have two types of leaf nodes: <code>l3leaf</code> or <code>l2leaf</code>. This deployment will utilize <code>l3leaf</code> for all leafs as this sets the L3 network services and VTEP functionality for EVPN/VXLAN.</p> <pre><code>---\ntype: l3leaf\n</code></pre> <p>You add VLANs, VRFS, and EVPN specific parameters to the Fabric by updating the group_vars/SITE1_NETWORK_SERVICES.yml. Within the main tenant we will be configuring, we will supply a mac_vrf_vni_base value, which will be used for the VLAN to VNI mapping under the VXLAN interface. We will then define a VRF our VLANs will be part of and give that a VNI value for the VRF to VNI mapping. Finally, each VLAN SVI will be configured, given a name, and a single virtual IP address which will end up being configured on all <code>l3leaf</code> nodes.</p> <pre><code>---\ntenants:\n  - name: S1_FABRIC\n    mac_vrf_vni_base: 10000\n    vrfs:\n      - name: OVERLAY\n        vrf_vni: 10\n        svis:\n          - id: 10\n            name: 'Ten'\n            enabled: true\n            ip_address_virtual: 10.10.10.1/24\n          - id: 20\n            name: 'Twenty'\n            enabled: true\n            ip_address_virtual: 10.20.20.1/24\n</code></pre> <p>Our fabric would only be complete by connecting some devices to it. We define connected endpoints and port profiles in group_vars/SITE1_CONNECTED_ENDPOINTS.yml. Each endpoint adapter defines which switch port and port profile to use. Our lab has two hosts connected to the <code>site 1</code> fabric. The connected endpoints keys are used for logical separation and apply to interface descriptions. These variables are applied to the spine and leaf nodes since they are a part of this nested inventory group.</p> <pre><code>---\nport_profiles:\n\n  - profile: PP-VLAN10\n    mode: \"access\"\n    vlans: \"10\"\n    spanning_tree_portfast: edge\n  - profile: PP-VLAN20\n    mode: \"access\"\n    vlans: \"20\"\n    spanning_tree_portfast: edge\n\n###########################################################\n# ---------------- Endpoint Connectivity ---------------- #\n###########################################################\n\nservers:\n\n# --------------------------------------------------------#\n# Site1 RACK1 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host1                                      # Server name\n    rack: RACK1                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf1, s1-leaf2 ]                # Switch to connect server (required)\n        profile: PP-VLAN10                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n\n# --------------------------------------------------------#\n# Site1 RACK2 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host2                                      # Server name\n    rack: RACK2                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf3, s1-leaf4 ]                # Switch to connect server (required)\n        profile: PP-VLAN20                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n</code></pre>"},{"location":"l3ls/overview/#global-variables","title":"Global Variables","text":"<p>In a multi-site environment, some variables can be applied to all sites. They include AAA, Local Users, NTP, Syslog, DNS, and TerminAttr. Instead of updating these same variables in multiple inventory group_vars, we can use a single global variable file and import the variables at playbook runtime. This allows us to make a single change applied to all sites.</p> <p>For example, in our lab, we use a global variable file <code>global_vars/global_dc-vars.yml</code>.</p> <p>AVD provides a <code>global_vars</code> plugin that enables the use of global variables.</p> <p>The <code>global_vars</code> plugin must be enabled in the <code>ansible.cfg</code> file as shown below:</p> <pre><code>#enable global vars\nvars_plugins_enabled = arista.avd.global_vars, host_group_vars\n\n#define global vars path\n[vars_global_vars]\npaths = ../../global_vars\n</code></pre> Info <p>If a folder is used as in the example above, all files in the folder will be parsed in alphabetical order.</p>"},{"location":"l3ls/overview/#example-global-vars-file","title":"Example Global Vars File","text":"global_vars/global_dc_vars.yml <pre><code>---\n# Credentials for CVP and EOS Switches\nansible_user: arista\nansible_password: \"{{ lookup('env', 'LABPASSPHRASE') }}\"\nansible_network_os: arista.eos.eos\n# Configure privilege escalation\nansible_become: true\nansible_become_method: enable\n# HTTPAPI configuration\nansible_connection: httpapi\nansible_httpapi_port: 443\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_python_interpreter: $(which python3)\navd_data_conversion_mode: error\navd_data_validation_mode: error\n\n# CVP node variables\ncv_collection: v3\nexecute_tasks: true\n\n# Local Users\nlocal_users:\n  - name: arista\n    privilege: 15\n    role: network-admin\n    sha512_password: \"{{ ansible_password | password_hash(salt='workshop') }}\"\n\n# AAA\naaa_authorization:\n  exec:\n    default: local\n\n# OOB Management network default gateway.\nmgmt_gateway: 192.168.0.1\nmgmt_interface_vrf: default\n\n# NTP Servers IP or DNS name, first NTP server will be preferred, and sourced from Management VRF\nntp:\n  servers:\n    - name: 192.168.0.1\n      iburst: true\n      local_interface: Management0\n\n# Domain/DNS\ndns_domain: atd.lab\n\n# TerminAttr\ndaemon_terminattr:\n  # Address of the gRPC server on CloudVision\n  # TCP 9910 is used on on-prem\n  # TCP 443 is used on CV as a Service\n  cvaddrs: # For single cluster\n    - 192.168.0.5:9910\n  # Authentication scheme used to connect to CloudVision\n  cvauth:\n    method: token\n    token_file: \"/tmp/token\"\n  # Exclude paths from Sysdb on the ingest side\n  ingestexclude: /Sysdb/cell/1/agent,/Sysdb/cell/2/agent\n  # Exclude paths from the shared memory table\n  smashexcludes: ale,flexCounter,hardware,kni,pulse,strata\n\n# Point to Point Links MTU Override for Lab\np2p_uplinks_mtu: 1500\n\n# Set IPv4 Underlay Routing and EVPN Overlay Routing to use eBGP\nunderlay_routing_protocol: ebgp\noverlay_routing_protocol: ebgp\n\n# Configure password authentication for BGP peerings\nbgp_peer_groups:\n  evpn_overlay_peers:\n    password: Q4fqtbqcZ7oQuKfuWtNGRQ==\n  ipv4_underlay_peers:\n    password: 7x4B4rnJhZB438m9+BrBfQ==\n  mlag_ipv4_underlay_peer:\n    password: 4b21pAdCvWeAqpcKDFMdWw==\n\n# L3 Edge port definitions. This can be any port in the entire Fabric, where IP interfaces are defined.\nl3_edge:\n  # Define a new IP pool that will be used to assign IP addresses to L3 Edge interfaces.\n  p2p_links_ip_pools:\n    - name: S1_to_S2_IP_pool\n      ipv4_pool: 172.16.255.0/24\n  # Define a new link profile which will match the IP pool, the used ASNs and include the defined interface into underlay routing\n  p2p_links_profiles:\n    - name: S1_to_S2_profile\n      ip_pool: S1_to_S2_IP_pool\n      as: [ 65103, 65203 ]\n      include_in_underlay_protocol: true\n  # Define each P2P L3 link and link the nodes, the interfaces and the profile used.\n  p2p_links:\n    - id: 1\n      nodes: [ s1-brdr1, s2-brdr1 ]\n      interfaces: [ Ethernet4, Ethernet4 ]\n      profile: S1_to_S2_profile\n    - id: 2\n      nodes: [ s1-brdr2, s2-brdr2 ]\n      interfaces: [ Ethernet5, Ethernet5 ]\n      profile: S1_to_S2_profile\n</code></pre>"},{"location":"l3ls/overview/#data-models","title":"Data Models","text":"<p>AVD provides a network-wide data model and is typically broken into multiple group_vars files to simplify and categorize variables with their respective functions. We break the data model into three categories: topology, services, and ports.</p>"},{"location":"l3ls/overview/#fabric-topology","title":"Fabric Topology","text":"<p>The physical fabric topology is defined by providing interface links between the spine and leaf nodes. The <code>group_vars/SITE1_FABRIC.yml</code> file defines this portion of the data model. In our lab, the spines provide layer 3 routing of SVIs and P2P links using a node type called <code>l3spines</code>. The leaf nodes are purely layer 2 and use node type <code>leaf</code>. An AVD L2LS design type provides three node type keys: l3 spine, spine, and leaf. AVD Node Type documentation can be found here.</p>"},{"location":"l3ls/overview/#spine-and-leaf-nodes","title":"Spine and Leaf Nodes","text":"<p>The example data model below defines each site's spine and leaf nodes for each site. Refer to the inline comments for variable definitions. Under each node_type_key you have key/value pairs for defaults, node_groups, and nodes. Note that key/value pairs may be overwritten with the following descending order of precedence. The key/value closest to the node will be used.</p> <p> <ol> <li>defaults</li> <li>node_groups</li> <li>nodes</li> </ol> <pre><code># Spine Switches\nspine:\n  defaults:\n    platform: cEOS\n    loopback_ipv4_pool: 10.250.1.0/24\n    bgp_as: 65100\n  nodes:\n    - name: s1-spine1\n      id: 1\n      mgmt_ip: 192.168.0.10/24\n    - name: s1-spine2\n      id: 2\n      mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nl3leaf:\n  defaults:\n    platform: cEOS\n    spanning_tree_priority: 4096\n    spanning_tree_mode: mstp\n    loopback_ipv4_pool: 10.250.1.0/24\n    loopback_ipv4_offset: 2\n    vtep_loopback_ipv4_pool: 10.255.1.0/24\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    uplink_ipv4_pool: 172.16.1.0/24\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n    mlag_peer_ipv4_pool: 10.251.1.0/24\n    mlag_peer_l3_ipv4_pool: 10.252.1.0/24\n    virtual_router_mac_address: 00:1c:73:00:00:99\n  node_groups:\n    - group: S1_RACK1\n      bgp_as: 65101\n      nodes:\n        - name: s1-leaf1\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: S1_RACK2\n      bgp_as: 65102\n      nodes:\n        - name: s1-leaf3\n          id: 3\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 4\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n    - group: S1_BRDR\n      bgp_as: 65103\n      evpn_gateway:\n        evpn_l2:\n          enabled: true\n        evpn_l3:\n          enabled: true\n          inter_domain: true\n      nodes:\n        - name: s1-brdr1\n          id: 5\n          mgmt_ip: 192.168.0.100/24\n          uplink_switch_interfaces: [ Ethernet7, Ethernet7 ]\n          evpn_gateway:\n            remote_peers:\n              - hostname: s2-brdr1\n                bgp_as: 65203\n                ip_address: 10.255.2.7\n        - name: s1-brdr2\n          id: 6\n          mgmt_ip: 192.168.0.101/24\n          uplink_switch_interfaces: [ Ethernet8, Ethernet8 ]\n          evpn_gateway:\n            remote_peers:\n              - hostname: s2-brdr2\n                bgp_as: 65203\n                ip_address: 10.255.2.8\n</code></pre>"},{"location":"l3ls/overview/#l3-edge-interfaces","title":"L3 Edge Interfaces","text":"<p>Inside the same global_dc_vars file, we define how each site is linked to each other through their respective border leafs. In our example, the <code>core</code> switches are simply configured with layer 2 interfaces so each border leaf connects to its peer in a point to point manner. The <code>l3_edge</code> data model for both sites follows.</p> <pre><code># L3 Edge port definitions. This can be any port in the entire Fabric, where IP interfaces are defined.\nl3_edge:\n  # Define a new IP pool that will be used to assign IP addresses to L3 Edge interfaces.\n  p2p_links_ip_pools:\n    - name: S1_to_S2_IP_pool\n      ipv4_pool: 172.16.255.0/24\n  # Define a new link profile which will match the IP pool, the used ASNs and include the defined interface into underlay routing\n  p2p_links_profiles:\n    - name: S1_to_S2_profile\n      ip_pool: S1_to_S2_IP_pool\n      as: [ 65103, 65203 ]\n      include_in_underlay_protocol: true\n  # Define each P2P L3 link and link the nodes, the interfaces and the profile used.\n  p2p_links:\n    - id: 1\n      nodes: [ s1-brdr1, s2-brdr1 ]\n      interfaces: [ Ethernet4, Ethernet4 ]\n      profile: S1_to_S2_profile\n    - id: 2\n      nodes: [ s1-brdr2, s2-brdr2 ]\n      interfaces: [ Ethernet5, Ethernet5 ]\n      profile: S1_to_S2_profile\n</code></pre> <p>The following diagram shows the P2P links between the four border leafs. The DCI Network is pre-configured in our lab with the interfaces facing the border leafs as access ports in VLAN 1000. The l3_edge interfaces for the border leafs in <code>Site 1</code> and <code>Site 2</code> are configured and deployed with AVD.</p> <p></p>"},{"location":"l3ls/overview/#network-services","title":"Network Services","text":"<p>Fabric Services, such as VLANs, SVIs, and VRFs, are defined in this section. The following Site 1 example defines VLANs and SVIs for VLANs <code>10</code> and <code>20</code> in the OVERLAY VRF. We also have specified a mac VRF VNI base mapping of 10000.  This will add the base mapping to the VLAN ID to come up with the VNI for the VLAN to VNI mapping under the VXLAN interface. Since we have the same VLANs stretched across to Site 2, the network services data model will be exactly the same:</p> <pre><code>---\ntenants:\n  - name: S1_FABRIC\n    mac_vrf_vni_base: 10000\n    vrfs:\n      - name: OVERLAY\n        vrf_vni: 10\n        svis:\n          - id: 10\n            name: 'Ten'\n            enabled: true\n            ip_address_virtual: 10.10.10.1/24\n          - id: 20\n            name: 'Twenty'\n            enabled: true\n            ip_address_virtual: 10.20.20.1/24\n</code></pre>"},{"location":"l3ls/overview/#connected-endpoints","title":"Connected Endpoints","text":"<p>The Fabric must define ports for southbound interfaces toward connected endpoints such as servers, appliances, firewalls, and other networking devices in the data center. This section uses port profiles and connected endpoints called <code>servers</code>. Documentation for port_profiles and connected endpoints are available to see all the options available.</p> <p>The following data model defined two port profiles: PP-VLAN10 and PP-VLAN20. They define an access port profile for VLAN <code>10</code> and <code>20</code>, respectively. In addition, two server endpoints (s1-host1 and s1-host2) are created to use these port profiles. There are optional and required fields. The optional fields are used for port descriptions in the EOS intended configurations.</p> <pre><code>---\nport_profiles:\n\n  - profile: PP-VLAN10\n    mode: \"access\"\n    vlans: \"10\"\n    spanning_tree_portfast: edge\n  - profile: PP-VLAN20\n    mode: \"access\"\n    vlans: \"20\"\n    spanning_tree_portfast: edge\n\n###########################################################\n# ---------------- Endpoint Connectivity ---------------- #\n###########################################################\n\nservers:\n\n# --------------------------------------------------------#\n# Site1 RACK1 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host1                                      # Server name\n    rack: RACK1                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf1, s1-leaf2 ]                # Switch to connect server (required)\n        profile: PP-VLAN10                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n\n# --------------------------------------------------------#\n# Site1 RACK2 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host2                                      # Server name\n    rack: RACK2                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf3, s1-leaf4 ]                # Switch to connect server (required)\n        profile: PP-VLAN20                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n</code></pre>"},{"location":"l3ls/overview/#the-playbooks","title":"The Playbooks","text":"<p>Two playbooks, <code>build.yml</code> and <code>deploy.yml</code> are used in our lab. Expand the tabs below to reveal the content.</p> build.yml Playbook <pre><code>---\n- name: Build Switch configuration\n  hosts: \"{{ target_hosts }}\"\n  gather_facts: false\n\n  tasks:\n\n    - name: Generate Structured Variables per Device\n      import_role:\n        name: arista.avd.eos_designs\n\n    - name: Generate Intended Config and Documentation\n      import_role:\n        name: arista.avd.eos_cli_config_gen\n</code></pre> deploy.yml Playbook <pre><code>---\n- name: Deploy Switch configuration\n  hosts: \"{{ target_hosts }}\"\n  gather_facts: false\n\n  tasks:\n\n    - name: Deploy Configuration to Device\n      import_role:\n        name: arista.avd.eos_config_deploy_eapi\n</code></pre> <p>To make our lives easier, we use a <code>Makefile</code> to create aliases to run the playbooks and provide the needed options. This eliminates mistakes and typing long commands.</p> Makefile <pre><code>.PHONY: help\nhelp: ## Display help message\n    @grep -E '^[0-9a-zA-Z_-]+\\.*[0-9a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\n########################################################\n# Site 1\n########################################################\n\n.PHONY: build-site-1\nbuild-site-1: ## Build Configs\n    ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n# ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\" -e \"@global_vars/global_dc_vars.yml\"\n\n.PHONY: deploy-site-1\ndeploy-site-1: ## Deploy Configs via eAPI\n    ansible-playbook playbooks/deploy.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n\n########################################################\n# Site 2\n########################################################\n\n.PHONY: build-site-2\nbuild-site-2: ## Build Configs\n    ansible-playbook playbooks/build.yml -i sites/site_2/inventory.yml -e \"target_hosts=SITE2_FABRIC\"\n\n.PHONY: deploy-site-2\ndeploy-site-2: ## Deploy Configs via eAPI\n    ansible-playbook playbooks/deploy.yml -i sites/site_2/inventory.yml -e \"target_hosts=SITE2_FABRIC\"\n</code></pre> <p>For example, if we wanted to run a playbook to build configs for Site 1, we could enter the following command.</p> <pre><code>ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n</code></pre> <p>Thankfully, a convenient way to simplify the above command is with a Makefile entry like the one below.</p> <pre><code>.PHONY: build-site-1\nbuild-site-1: ## Build Configs\n  ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n</code></pre> <p>Now, you can type the following to issue the same ansible-playbook command.</p> <pre><code>make build-site-1\n</code></pre> <p>In the upcoming lab, we will use the following <code>make</code> commands several times. First, review the above <code>Makefile</code> to see what each entry does. Then, try building some custom entries.</p> <p>Build configurations</p> <pre><code># Build configs for Site 1\nmake build-site-1\n\n# Build configs for Site 2\nmake build-site-2\n</code></pre> <p>Deploy configurations</p> <pre><code># Deploy configs for Site 1\nmake deploy-site-1\n\n# Deploy configs for Site 2\nmake deploy-site-2\n</code></pre>"},{"location":"l3ls/overview/#next-steps","title":"Next Steps","text":"<p>Continue to Lab Guide</p>"},{"location":"validation/anta_validate/","title":"Getting Started with ANTA Validation","text":""},{"location":"validation/anta_validate/#anta-lab-guide-overview","title":"ANTA Lab Guide Overview","text":"<p>The ANTA Lab Guide is a follow-along set of instructions on leveraging the available network testing modules as either part of CI/AVD labs or independent of those labs. For this lab, you're focus is discovering how Arista Network Test Automation (ANTA) can work to help you validate network state:</p> <ul> <li> Prepare your Lab</li> <li> Build your ANTA inventory</li> <li> Build your ANTA test catalog</li> <li> Run ANTA for the first time</li> <li> Explore ANTA execution options</li> </ul> <p>ANTA can be run against any Arista architecture (L2LS, L3LS, etc). For this lab we will deploy a fabric and use our testing tools to validate it's working. You can either run this lab as a continuation of the available CI/AVD workshops OR start this lab from scratch.</p> <ul> <li> <p> Option 1 | CI/AVD Continued</p> <p>If you have just completed any of the CI/AVD labs (L2LS, L3LS EVPN/VXLAN), you can continue using that environment and leverage this material to test your newly deployed network! You can skip to Step 3 and move straight to using ANTA.</p> <p> Skip to Step 3</p> </li> <li> <p> Option 2 | Starting From Scratch</p> <p>If you have not completed the CI/AVD labs, no worries! This lab contains all the steps necessary to test a working fabric using ANTA and AVD. Continue on and prepare your environment! </p> <p> Prepare Your Lab!</p> </li> </ul>"},{"location":"validation/anta_validate/#step-1-prepare-lab-environment","title":"Step 1 - Prepare Lab Environment","text":""},{"location":"validation/anta_validate/#access-the-atd-lab","title":"Access the ATD Lab","text":"<p>Connect to your ATD Lab and start the Programmability IDE. Next, create a new Terminal.</p>"},{"location":"validation/anta_validate/#fork-and-clone-branch-to-atd-lab","title":"Fork and Clone Branch to ATD Lab","text":"<p>An ATD Dual Data Center L3LS data model is posted on GitHub.</p> <ul> <li>Fork this repository to your own GitHub account.</li> <li>Next, clone your forked repo to your ATD lab instance.</li> </ul> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <pre><code>git clone &lt;your copied HTTPS URL&gt;\n</code></pre> <pre><code>cd ci-workshops-avd\n</code></pre> <p>Configure your global Git settings.</p> <pre><code>git config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code>git config --global user.email \"name@example.com\"\n</code></pre>"},{"location":"validation/anta_validate/#update-avd","title":"Update AVD","text":"<p>AVD has been pre-installed in your lab environment. However, it may be on an older version (in some cases a newer version). The following steps will update AVD and modules to the valid versions for the lab.</p> <pre><code>pip3 config set global.break-system-packages true\npip3 config set global.disable-pip-version-check true\npip3 install \"pyavd[ansible-collection]==4.10.0\"\nansible-galaxy collection install -r requirements.yml\n</code></pre> Important <p>You must run these commands when you start your lab or a new shell (terminal).</p>"},{"location":"validation/anta_validate/#setup-lab-password-environment-variable","title":"Setup Lab Password Environment Variable","text":"<p>Each lab comes with a unique password. We set an environment variable called <code>LABPASSPHRASE</code> with the following command. The variable is later used to generate local user passwords and connect to our switches to push configs.</p> <pre><code>export LABPASSPHRASE=`cat /home/coder/.config/code-server/config.yaml| grep \"password:\" | awk '{print $2}'`\n</code></pre> <p>You can view the password is set. This is the same password displayed when you click the link to access your lab.</p> <pre><code>echo $LABPASSPHRASE\n</code></pre> Important <p>You must run this step when you start your lab or a new shell (terminal).</p>"},{"location":"validation/anta_validate/#anta-environment-variable","title":"ANTA Environment Variable","text":"<p>Remove the environment variable set for ANTA if you see it there</p> <pre><code>env | grep ANTA_VERSION\n</code></pre> <p>Remove it using the following</p> <pre><code>unset ANTA_VERSION\n</code></pre>"},{"location":"validation/anta_validate/#step-2-build-the-fabric","title":"Step 2 - Build the Fabric","text":"<p>If you are starting this workshop without completing the previous CI/AVD workshops, lets get the fabric built</p> <ol> <li> <p>Navigate to the Network Testing lab</p> <pre><code>cd labs/NET_TESTING\n</code></pre> </li> <li> <p>Use AVD to build and deploy the Layer 3 Leaf/Spine fabric</p> <pre><code>make preplab build-site-1 build-site-2 deploy-site-1 deploy-site-2\n</code></pre> </li> </ol> <p>This should deploy a Layer 3 Leaf Spine with EVPN to your lab and will provide some more configuration to work with.</p> <p>What does this all do!?</p> <p>The <code>make</code> command above is a combination of AVD playbooks that build the entire environment from scratch! If you're interested in how that all worked, go check out the Automation Fundamentals (Git, VS Code, Jinja/YAML, Ansible) and L3LS EVPN/VXLAN Workshops, it will walk you through how this lab was built using Arista Validated Designs (AVD).</p>"},{"location":"validation/anta_validate/#step-3-running-anta","title":"Step 3 - Running ANTA","text":"<p>Ok, you've just deployed a dual data center, Layer 3 VXLAN/EVPN Spine/Leaf fabric in that time using AVD. Of course, visit the CI/AVD labs to learn more about how it all works, but for now we have a supposedly working fabric... lets validate that's true!</p>"},{"location":"validation/anta_validate/#summary-of-steps","title":"Summary of Steps","text":"<ul> <li> Review ANTA Inventory and Test Catalog</li> <li> Run ANTA Network Ready For Use (NRFU)</li> <li> Review Reports</li> <li> Run Additional Tests</li> <li> Review Additional Reports</li> </ul>"},{"location":"validation/anta_validate/#anta-inventory","title":"ANTA Inventory","text":"<p>ANTA depends on an inventory, like Ansible; it needs a source of target devices against which to run the validation tests. Let's generate that ANTA inventory from your Ansible inventory in your repository!</p> <ol> <li> <p>Open the inventory files</p> <pre><code>sites/site_1/inventory.yml\nsites/site_2/inventory.yml\n</code></pre> </li> <li> <p>These are Ansible inventory files used for AVD; let's generate ANTA-specific inventory from these</p> <pre><code>anta get from-ansible \\\n--ansible-inventory sites/site_1/inventory.yml \\\n--ansible-group SITE1_FABRIC \\\n--output sites/site_1/anta_inventory.yml \\\n--overwrite\n</code></pre> </li> <li> <p>Let's navigate to our new inventory file and see what that looks like under <code>sites/site_1/anta_inventory.yml</code>:</p> <pre><code>anta_inventory:\n    hosts:\n    - host: 192.168.0.10\n      name: s1-spine1\n    - host: 192.168.0.11\n      name: s1-spine2\n    - host: 192.168.0.12\n      name: s1-leaf1\n    - host: 192.168.0.13\n      name: s1-leaf2\n    - host: 192.168.0.14\n      name: s1-leaf3\n    - host: 192.168.0.15\n      name: s1-leaf4\n    - host: 192.168.0.100\n      name: s1-brdr1\n    - host: 192.168.0.101\n      name: s1-brdr2\n</code></pre> </li> <li> <p>Great, we've created an inventory file based on our AVD ansible inventory</p> </li> <li> <p>Let's do the same for <code>site 2</code>, this time let's use a Make shortcut to do this same thing</p> <pre><code>make anta-inv-site-2\n</code></pre> </li> <li> <p>Now let's get to building some tests to run against this inventory</p> </li> </ol>"},{"location":"validation/anta_validate/#anta-test-catalog","title":"ANTA Test Catalog","text":"<p>We have a set of target devices, we now need to define the set of tests we intend to run against our inventory. Remember, the ANTA test catalog is like a buffet of options, we choose what tests we need to prove our fabric is working as expected!</p> <ol> <li> <p>Navigate to the ANTA test catalog provided under <code>tests/all.yml</code></p> <p>CI/AVD Lab Extension</p> <p>If you are extending one of the CI/AVD labs, copy this directory to your working AVD directory. You can skip this otherwise.</p> <pre><code>cp ~/project/labfiles/ci-workshops-avd/labs/NET_TESTING/tests/all.yml &lt;your_avd_proj&gt;\n</code></pre> </li> <li> <p>Within that file, we should have three test categories defined</p> <ol> <li><code>VerifyUptime</code></li> <li><code>VerifyEnvironmentCooling</code></li> <li><code>VerifyTerminAttrVersion</code></li> </ol> </li> <li> <p>To run our first ANTA test, target our new <code>sites/site_1/anta_inventory.yml</code> with the <code>tests/all.yml</code> test catalog. We must pass our username and password as part of the run, so let's explore the options.</p> </li> <li> <p>Not many tests, but let's use this for our first ANTA NRFU run!</p> </li> </ol>"},{"location":"validation/anta_validate/#running-nrfu-tests","title":"Running NRFU Tests","text":"<p>To run our first ANTA test, target our new <code>sites/site_1/anta_inventory.yml</code> with the <code>tests/all.yml</code> test catalog. We must pass our username and password as part of the run, so let's explore the options.</p> Explore the options<pre><code>anta nrfu  \\\n    -u arista \\\n    -p ${LABPASSPHRASE} \\ #(1)!\n    -i sites/site_1/anta_inventory.yml \\ #(2)!\n    -c tests/all.yml \\ #(3)!\n    table #(4)!\n</code></pre> <ol> <li>This is your lab password; we set this environment variable at the beginning of this lab</li> <li>Our new ANTA inventory we generated from our Ansible inventory should target the <code>SITE1_FABRIC</code> group</li> <li>We haven't explored this yet, but this describes what we want to test.</li> <li>Print out the results in a table format</li> </ol> <p>Now, let's run the NRFU test</p> Copy/Paste and run<pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml table\n</code></pre> <p>Great! We should have our first set of results, let's take a look at the output.</p> <p></p> <p>You should see similar results, there are two things happening here:</p> <ol> <li>ANTA ran each of the tests against your list of inventory</li> <li>ANTA generated a report (in table format) of the success, pass, or skipped tests</li> </ol> <p>Might be wondering, this doesn't look like everything is ok with the lab and you would be right. Let's look at what those results are telling us.</p> <p>Success: VerifyCPUUtilization</p> <p>All your instances of cEOS should be running for more than 10 minutes (600 seconds) at this point, great success!</p> <p>Skipped: VerifyEnvironmentCooling</p> <p>ANTA skipped tests validating the health of device fans, we are not running physical hardware, so this is expected!</p> <p>Failure: VerifyTerminAttrVersion</p> <p>It looks like our test failed to validate the TerminAttr version, the lab is running a new version of TerminAttr since this test was last updated. Let's address this below.</p> <p>We got our first set of test results, let's adjust our testing to fit our lab instance.</p> <p>So let's address our failures and skipped tests</p> <ol> <li> <p>You should have seen a message similar to this in our test results</p> <pre><code>device is running TerminAttr version v1.34.0 and is not in the allowed list: ['v1.32.0']\n</code></pre> </li> <li> <p>Let's add that TerminAttr version detected to our test catalog under <code>tests/all.yml</code></p> <pre><code>anta.tests.software:\n    - VerifyTerminAttrVersion:\n        versions:\n        - v1.32.0\n        - v1.34.0 #(1)!\n</code></pre> <ol> <li>Add the version of TerminAttr used in your Arista Test Drive instance</li> </ol> </li> <li> <p>You've now instructed ANTA that TerminAttr v1.34.0 is an acceptable version, we should now pass this test. Feel free to run the test at this point</p> <pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml table\n</code></pre> </li> <li> <p>We should see we've fixed the failed tasks, if not make sure to revisit Step 2!</p> </li> <li>Now for the skipped tests, ANTA has the intelligence to understand any hardware tests should be skipped when run against virtual instances of EOS. Makes a lot of sense!</li> <li> <p>While there really isn't an issue here, for now we're going to simply skip the tests to make this report easier to read</p> <pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml --hide skipped table\n</code></pre> </li> <li> <p>You should have some results that look similar to this now!</p> <p></p> </li> </ol>"},{"location":"validation/anta_validate/#step-4-adding-anta-tests","title":"Step 4 - Adding ANTA Tests","text":"<p>You've run your first ANTA NRFU test; we've only scratched the surface of ANTA. Let's take a minute to explore what ANTA is capable of when testing different scenarios.</p> <p>When operating a real-world network, we will curate our tests to an ever-changing environment. Let's build on our current test catalog and provide additional validation.</p> <ol> <li>We are testing against a Layer 3 Spine/Leaf using EVPN VXLAN, lets do a basic check and ensure all our devices are configured for multiprotocol BGP.</li> <li>Let's navigate to anta.arista.com and find the test in the documentation</li> <li>You can search at the top for <code>multi-agent</code>, but lets open <code>Tests Documentation &gt; Routing &gt; Generic</code></li> <li>We should find there is an ANTA test built for us to verify exactly this using the <code>VerifyRoutingProtocolModel</code> under this testing class</li> <li> <p>Let's add this test to our test catalog under <code>labs/NET_TESTING/tests/all.yml</code></p> <pre><code>anta.tests.routing:\n    generic:\n        - VerifyRoutingProtocolModel: #(1)!\n            model: multi-agent\n</code></pre> <ol> <li>The test will pass if the configured routing protocol model is the one we expect, in this case <code>multi-agent</code>, the alternative is <code>ribd</code>.</li> </ol> </li> <li> <p>Save the file and re-run the tests</p> <pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml --hide skipped table\n</code></pre> </li> <li> <p>You should find that all devices are configured with <code>multi-agent</code>, let's verify <code>site2</code> now</p> <pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_2/anta_inventory.yml -c tests/all.yml --hide skipped table\n</code></pre> </li> <li> <p>Great! Just like that, we verified all 16 devices are configured for MP-BGP</p> </li> <li> <p>Let's take this further and ensure BGP is peering across the address families and VRFs. If we think about it, there should be</p> <ol> <li>Default VRF: 3 Peers, 2 peers to each of the spines and iBGP peer across the MLAG link</li> <li>OVERLAY VRF: 1 Peer, the iBGP peer across the MLAG link</li> <li>EVPN Address Family: 2 peers, both spines are EVPN route servers</li> </ol> </li> <li> <p>Ok, back on ANTA Testing Documentation there is a Routing BGP set of sets</p> </li> <li> <p>Let's find the <code>VerifyBGPPeerCount</code> and verify this test's configuration.</p> </li> <li> <p>Let's represent our criteria in the format of the test structure within our catalog under <code>anta.tests.routing</code>, our routing tests should look like this now.</p> <p>Reminder</p> <p>This test is a part of the same test category <code>anta.tests.routing</code> we just used for our <code>multi-agent</code> test. Be sure to add the sub category under the same key!</p> <pre><code>anta.tests.routing:\n  generic: #(1)!\n    - VerifyRoutingProtocolModel:\n        model: multi-agent\n  bgp:\n    - VerifyBGPPeerCount: #(2)!\n        address_families:\n        - afi: \"evpn\" #(3)!\n          num_peers: 2 #(4)!\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"OVERLAY\"\n          num_peers: 1 #(5)!\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"default\"\n          num_peers: 3 #(6)!\n</code></pre> <ol> <li>Our previous test was under the <code>generic</code> sub-category, this new test is under <code>bgp</code></li> <li>This test will succeed for each device if the count of BGP peers matches the expected count for each address family and VRF.</li> <li>Identify the address family, in this case we want to address the EVPN address family</li> <li>Verify we have BGP peers to both our EVPN route servers</li> <li>Verify the <code>OVERLAY</code> VRF should have 1 peer, this is our designated MLAG peer for this VRF</li> <li>Verify the underlay is peering to both spines and our MLAG peer</li> </ol> </li> <li> <p>Save the file and re-run the tests</p> <pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml --hide skipped table\n</code></pre> </li> <li> <p>Alright, now that went to plan... sort of. Looks like there are some failures, let's re-run this test but use the <code>--group-by</code> flag to make this easier to visualize what failed.</p> <pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml table --group-by device\n</code></pre> Skipped tests are coming back <p>We'll remove the <code>--hide skipped</code>, we can visualize all tests much easier using <code>--group-by</code>. You can group by both <code>device</code> and <code>test</code>, check out the documentation for more information ANTA Command Overview</p> </li> <li> <p>Much better, looks like the <code>spine</code> and <code>brdr</code> devices are failing the BGP count tests, can you think of why? \ud83e\udd14</p> <p> We have failures!!! </p> <p>YES! We have failures and THAT'S OK... at least here. In an attempt to simulate a fictitious world where networks are flawed, we are starting with failures we will fix! \ud83d\ude09</p> <p></p> </li> <li> <p>Before continuing to the next lab, run all these tests against <code>site2</code> if you have not already. We will address these failures in the next step!</p> </li> </ol> <p>Great, in this section, we covered the following:</p> <ul> <li> How to find new tests on anta.arista.com</li> <li> Adding tests to our catalog</li> <li> Additional flag to group test results</li> <li> Verifying test results</li> </ul>"},{"location":"validation/anta_validate/#step-5-tagging-tests","title":"Step 5 - Tagging Tests","text":"<p>We added some tests in the last step that worked great, albeit it failed on the spines and borders. Let's validate what's on the spines and borders and troubleshoot why our tests failed.</p> <ol> <li>Login to <code>s1-spine</code></li> <li> <p>Let's check the default BGP peers we defined as part of our <code>VerifyBGPPeerCount</code> test</p> <pre><code>show ip bgp summary\n\n# This command will give you the count of Established peers\nshow ip bgp summary | grep \"Estab\" | wc -l\n</code></pre> <pre><code>s1-spine1#sh ip bgp summary\nBGP summary information for VRF default\nRouter identifier 10.250.1.1, local AS number 65100\nNeighbor Status Codes: m - Under maintenance\n  Description              Neighbor    V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  s1-leaf1_Ethernet2       172.16.1.1  4 65101            110       111    0    0 01:17:54 Estab   3      3\n  s1-leaf2_Ethernet2       172.16.1.5  4 65101            107       111    0    0 01:17:55 Estab   3      3\n  s1-leaf3_Ethernet2       172.16.1.9  4 65102            105       113    0    0 01:17:54 Estab   3      3\n  s1-leaf4_Ethernet2       172.16.1.13 4 65102            104       112    0    0 01:17:54 Estab   3      3\n  s1-brdr1_Ethernet2       172.16.1.17 4 65103              0         0    0    0 01:17:58 Active\n  s1-brdr2_Ethernet2       172.16.1.21 4 65103            111       100    0    0 01:17:54 Estab   14     14\n\ns1-spine1#show ip bgp summary | grep \"Estab\" | wc -l\n5\n</code></pre> </li> <li> <p>First, let's take note of the fact we have an Active BGP peer (not Established), we will troubleshoot that in a bit.</p> </li> <li> <p>More concerning, the count of BGP peers we specified as part of our test is not correct. Reminder this is the test we supplied for the default vrf</p> <pre><code>anta.tests.routing:\n    bgp:\n        - VerifyBGPPeerCount:\n            - afi: \"ipv4\"\n              safi: \"unicast\"\n              vrf: \"default\"\n              num_peers: 3 #(1)!\n</code></pre> <ol> <li>We have more than 3 peers, more like 6 peers! (well 1 active \ud83d\ude03)</li> </ol> </li> <li> <p>Ok, let's check our other two tests, verify the number of BGP peers for our <code>vrf OVERLAY</code> and <code>address-family evpn</code>:</p> <pre><code>show ip bgp summary vrf OVERLAY\nshow bgp evpn summary\n</code></pre> </li> <li> <p>So you should notice a few things</p> <ol> <li><code>vrf OVERLAY</code>: The vrf doesn't exist on the spines, remember spines are simple L3 transports for VXLAN/EVPN and thus don't participate in VRF routing.</li> <li><code>address-family evpn</code>: The leafs have 2 EVPN peers, these are spines acting as route servers. The spines themselves are peered to all leaves/borders, so we get <code>2 * 3 leaf pairs = 6 peers</code></li> </ol> </li> <li> <p>So now know why this test is failing on <code>s1-spine1</code>, run the same for <code>s1-spine2</code>, <code>s1-border1</code>, and <code>s1-border2</code> to note the differences.</p> </li> <li>We've discovered an issue on the spines, address the failure of peer counts!</li> <li>We are going to use ANTA device and test tags to curate some tests to our device roles.</li> <li>Let's first update our <code>sites/site_1/anta_inventory.yml</code> file and add some tags to our devices.</li> <li> <p>Add a tag for each device role: <code>spine</code>, <code>leaf</code>, and <code>border</code> as such</p> <pre><code>anta_inventory:\n  hosts:\n  - host: 192.168.0.10\n    name: s1-spine1\n    tags: ['spine']\n  - host: 192.168.0.11\n    name: s1-spine2\n    tags: ['spine']\n  - host: 192.168.0.12\n    name: s1-leaf1\n    tags: ['leaf']\n  - host: 192.168.0.13\n    name: s1-leaf2\n    tags: ['leaf']\n  - host: 192.168.0.14\n    name: s1-leaf3\n    tags: ['leaf']\n  - host: 192.168.0.15\n    name: s1-leaf4\n    tags: ['leaf']\n  - host: 192.168.0.100\n    name: s1-brdr1\n    tags: ['border']\n  - host: 192.168.0.101\n    name: s1-brdr2\n    tags: ['border']\n</code></pre> </li> <li> <p>Let's now create some additional tests in our test catalog and tag them for each device type</p> </li> <li> <p>Open your test catalog in <code>tests/all.yml</code> and add tag our existing tests for <code>leaf</code></p> <pre><code>bgp:\n- VerifyBGPPeerCount:\n    address_families:\n    - afi: \"evpn\"\n      num_peers: 2\n    - afi: \"ipv4\"\n      safi: \"unicast\"\n      vrf: \"OVERLAY\"\n      num_peers: 1\n    - afi: \"ipv4\"\n      safi: \"unicast\"\n      vrf: \"default\"\n      num_peers: 3\n    filters:\n        tags: ['leaf']\n</code></pre> </li> <li> <p>Before we add our new tests, let's re-run our tests and see how that has changed our results. Feel free to run with and without the <code>--group-by</code></p> Run and hide skipped tests<pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml --hide skipped table\n</code></pre> Run and group by device<pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml table --group-by device\n</code></pre> </li> <li> <p>We should see there are no more failed tests?! That's right, we are no longer running these tests against the borders or spines, we have a gap now, let's fix that.</p> </li> <li> <p>Let's add another test under <code>bgp:</code> that matches the expected spine peers like we saw in a previous validation step</p> <pre><code>- VerifyBGPPeerCount:\n    address_families:\n    - afi: \"evpn\"\n      num_peers: 6\n    - afi: \"ipv4\"\n      safi: \"unicast\"\n      vrf: \"default\"\n      num_peers: 6\n    filters:\n      tags: ['spine']\n</code></pre> </li> <li> <p>Let's add the same for the border.</p> <pre><code>- VerifyBGPPeerCount:\n    address_families:\n    - afi: \"evpn\"\n      num_peers: 2\n    - afi: \"ipv4\"\n      safi: \"unicast\"\n      vrf: \"OVERLAY\"\n      num_peers: 1\n    - afi: \"ipv4\"\n      safi: \"unicast\"\n      vrf: \"default\"\n      num_peers: 3\n    filters:\n      tags: ['border']\n</code></pre> </li> <li> <p>If formatting is a bit of a problem, copy the following into your <code>tests/all.yml</code> file.</p> All ANTA Tests Available for Copy <pre><code>anta.tests.system:\n  - VerifyUptime:\n      minimum: 600\n\nanta.tests.hardware:\n  # Verifies the status of power supply fans and all fan trays.\n  - VerifyEnvironmentCooling:\n      states:\n        - ok\n\nanta.tests.software:\n\n    # Verifies the device is running one of the allowed TerminAttr version.\n    - VerifyTerminAttrVersion:\n        versions:\n        - v1.32.0\n        - v1.34.0\n\nanta.tests.routing:\n  generic:\n    - VerifyRoutingProtocolModel:\n        model: multi-agent\n  bgp:\n    - VerifyBGPPeerCount:\n        address_families:\n        - afi: \"evpn\"\n          num_peers: 2\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"OVERLAY\"\n          num_peers: 1\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"default\"\n          num_peers: 3\n        filters:\n            tags: ['leaf']\n    - VerifyBGPPeerCount:\n        address_families:\n        - afi: \"evpn\"\n          num_peers: 6\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"default\"\n          num_peers: 6\n        filters:\n          tags: ['spine']\n    - VerifyBGPPeerCount:\n        address_families:\n        - afi: \"evpn\"\n          num_peers: 2\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"OVERLAY\"\n          num_peers: 1\n        - afi: \"ipv4\"\n          safi: \"unicast\"\n          vrf: \"default\"\n          num_peers: 3\n        filters:\n          tags: ['border']\n</code></pre> </li> <li> <p>Re-run the ANTA tests, we should see both <code>s1-spine1</code> and <code>s1-spine2</code> are passing now! (4 tests pass per device)</p> Run and group by device<pre><code>anta nrfu  -u arista -p ${LABPASSPHRASE} -i sites/site_1/anta_inventory.yml -c tests/all.yml table --group-by device\n</code></pre> <p>My Borders Are Still Failing</p> <p>Remember the steps above, we caught a BGP peer in <code>Active</code> state on the spines. We will address this soon!</p> <p>My Spines Are Still Failing</p> <p>If your tests are failing for some reason, run this command without the <code>--group-by</code> and you will get the <code>Message(s)</code> column that will give you a hint as to why!</p> </li> <li> <p>Let's pause here, make sure all your tests are successful. If not, ensure your tags and peer counts are correct, otherwise CONGRATS, you did it!</p> <p></p> </li> </ol>"},{"location":"validation/anta_validate/#anta-custom-tests","title":"ANTA Custom Tests","text":"<p>You might think yourself at this point, that's cool, the testing ideas start flowing and you wonder if a specific test might exist in ANTA. Answer is probably yes, to start, but you might start finding yourself in a situation where ANTA doesn't provide a built in test that validates exactly what you want. Well, ANTA is a framework, and while there are built in tests, it was built on top of a framework you can customize.</p> <p>We will not explore the depths of customizing ANTA, but like we've explored before the ANTA documentation for creating custom tests provides some great guidance! As an example, recall our hardware test we added for <code>VerifyEnvironmentCooling</code>, well here's that source code and some insight in how to ANTA works under the hood!</p> <pre><code>class VerifyEnvironmentCooling(AntaTest): #(1)!\n    \"\"\"Verifies the status of power supply fans and all fan trays.\"\"\"\n\n    name = \"VerifyEnvironmentCooling\" #(2)!\n    description = \"Verifies the status of power supply fans and all fan trays.\"\n    categories: ClassVar[list[str]] = [\"hardware\"] #(3)!\n    commands: ClassVar[list[AntaCommand | AntaTemplate]] = [AntaCommand(command=\"show system environment cooling\", revision=1)] #(4)!\n\n    class Input(AntaTest.Input):\n        states: list[str] #(5)!\n\n    @skip_on_platforms([\"cEOSLab\", \"vEOS-lab\", \"cEOSCloudLab\"]) #(6)!\n    @AntaTest.anta_test\n    def test(self) -&gt; None:\n        command_output = self.instance_commands[0].json_output #(7)!\n        self.result.is_success()\n        # First go through power supplies fans\n        for power_supply in command_output.get(\"powerSupplySlots\", []):\n            for fan in power_supply.get(\"fans\", []):\n                if (state := fan[\"status\"]) not in self.inputs.states: #(8)\n                    self.result.is_failure(f\"Fan {fan['label']} on PowerSupply {power_supply['label']} is: '{state}'\") #(9)\n        # Then go through fan trays\n        for fan_tray in command_output.get(\"fanTraySlots\", []):\n            for fan in fan_tray.get(\"fans\", []):\n                if (state := fan[\"status\"]) not in self.inputs.states:\n                    self.result.is_failure(f\"Fan {fan['label']} on Fan Tray {fan_tray['label']} is: '{state}'\")\n</code></pre> <ol> <li>This is the ANTA class provided to write your own tests, you could write a <code>VerifyMyVeryOwnTest(AntaTest)</code> as an example</li> <li>This should look familiar, this is the name of the test we called out in our test catalog</li> <li>This test will fall under the <code>hardware</code> category, just like we saw in our test catalog: <code>anta.tests.hardware:</code></li> <li>Here we provide the EOS CLI command to be run on the device</li> <li>Remember, we provided states we wanted to validate, in this case <code>ok</code> was the only state given</li> <li>Remember our skipped tests, this is how ANTA is instructed to skip this test on specific platforms</li> <li>The output from the command in #4 is stored here as JSON data, we use that to evaluate the response below (like power supply state)</li> <li>If any of the power supply states don't match <code>ok</code>, then we fail!</li> <li>We set the result to <code>is_failure()</code> and pass it a message, this is the message you see on failure.</li> </ol>"},{"location":"validation/anta_validate/#step-6-lets-move-to-avd","title":"Step 6 - Let's Move to AVD","text":"<p>We've now built onto the concepts, specifically using tags to group devices to specific tests. Of course, we've done this all manually, and, as you might imagine, the overhead it would take to maintain an ever-changing real-world network. That is precisely where Arista Validated Designs helps you build your validation based on your data models! Let's go check it out.</p> <p>AVD Validate Lab </p>"},{"location":"validation/avd_validate/","title":"AVD Validation","text":"<p>You should have completed the Getting Started with ANTA Validation lab. If not, it's highly suggested that you complete that before continuing. This lab is intended to build on your understanding of ANTA and Arista Validated Designs (AVD) and give you the tools to test and validate a network fabric through automation.</p> <p>While these are not hard requirements, these are the recommended prerequisites to gain the most from this lab:</p> <ul> <li> Completed the Automation Fundamentals Lab</li> <li> Completed either the CI/AVD L2LS or the CI/AVD L3LS EVPN/VXLAN labs</li> <li> Completed the Getting Started with ANTA Validation</li> </ul>"},{"location":"validation/avd_validate/#review-of-anta","title":"Review of ANTA","text":"<p>At this point, we've explored the ANTA Framework and the capabilities to test our network fabric. You should also be somewhat familiar with the AVD constructs, it has been used to deploy this lab if you are coming from the Getting Start with ANTA Validation. So what does AVD and ANTA have to do with each other and how does this work.</p> <p>Reminder, we are operating within the <code>labs/NET_TESTING/</code> directory for this lab, please change to this directory if you haven't already</p> <pre><code>cd ~/project/labfiles/ci-workshops-avd/labs/NET_TESTING\n</code></pre>"},{"location":"validation/avd_validate/#step-1-avd-validate-state","title":"Step 1 - AVD Validate State","text":"<p>Before we dive into ANTA, lets look at AVD validate state module called <code>eos_validate_state</code>.</p> <ol> <li>Lets look at the validation playbook in <code>playbooks/validate.yml</code></li> <li> <p>This playbook used the AVD <code>eos_validate_state</code> module; click the annotations below to discover what the critical lines do:</p> <pre><code>---\n- name: Validate Network State\n  hosts: \"{{ target_hosts }}\" #(1)!\n  connection: httpapi\n  gather_facts: false\n\n  tasks:\n\n    - name: validate states on EOS devices\n      ansible.builtin.import_role:\n        name: arista.avd.eos_validate_state #(2)!\n</code></pre> <ol> <li>We will pass the <code>SITE1_FABRIC</code> as an extra variable to tell this playbook to validate SITE 1</li> <li>Here we import the role <code>eos_validate_state</code> which will run validations based on the AVD data models</li> </ol> </li> <li> <p>Before we run this, lets summarize what this role is performing</p> <ol> <li>First, this role requires an AVD build to have been completed. Those intended configurations generated within <code>sites/site_1/intended/</code> are used to understand what to validate</li> <li>Second, the role will rely heavily on Ansible to gather device details and validate the device state is healthy based on AVD</li> </ol> </li> <li> <p>Ok, lets run this playbook, we can use the <code>make</code> shortcut</p> <pre><code>make validate-site-1\n</code></pre> </li> <li> <p>So a lot happened! If you look at the Ansible play recap, we see there are a number of tests it performs</p> <pre><code>Wednesday 04 September 2024  19:43:14 +0000 (0:00:00.964)       0:00:38.051 ***\n===============================================================================\neos_validate_state ----------------------------------------------------- 37.93s\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntotal ------------------------------------------------------------------ 37.93s\nWednesday 04 September 2024  19:43:14 +0000 (0:00:00.964)       0:00:38.051 ***\n===============================================================================\narista.avd.eos_validate_state : Gather ip reachability state between devices (loopback0 &lt;-&gt; loopback0) ----------------- 6.09s\narista.avd.eos_validate_state : Run show ip route lo0 ------------------------------------------------------------------ 5.67s\narista.avd.eos_validate_state : Gather ip reachability state (directly connected interfaces) --------------------------- 4.63s\narista.avd.eos_validate_state : Run show ip route VTEP IP -------------------------------------------------------------- 2.20s\narista.avd.eos_validate_state : Create required output directories if not present -------------------------------------- 1.38s\narista.avd.eos_validate_state : Gather EOS platform and version details ------------------------------------------------ 1.26s\narista.avd.eos_validate_state : Create Validation report - Markdown ---------------------------------------------------- 0.96s\narista.avd.eos_validate_state : Gather bgp summary (ip and evpn) ------------------------------------------------------- 0.95s\narista.avd.eos_validate_state : Generate variables for testing --------------------------------------------------------- 0.93s\narista.avd.eos_validate_state : Create Validation report - CSV --------------------------------------------------------- 0.90s\narista.avd.eos_validate_state : Gather ntp status ---------------------------------------------------------------------- 0.89s\narista.avd.eos_validate_state : Gather mlag status --------------------------------------------------------------------- 0.87s\narista.avd.eos_validate_state : Gather ip route summary and ArBGP state ------------------------------------------------ 0.86s\narista.avd.eos_validate_state : Gather lldp topology ------------------------------------------------------------------- 0.83s\narista.avd.eos_validate_state : Gather interfaces state ---------------------------------------------------------------- 0.83s\narista.avd.eos_validate_state : Generate Results (Set eos_validate_state_report) --------------------------------------- 0.73s\narista.avd.eos_validate_state : Validate ip bgp neighbors peer state --------------------------------------------------- 0.33s\narista.avd.eos_validate_state : Validate bgp evpn neighbors peer state ------------------------------------------------- 0.33s\narista.avd.eos_validate_state : Validate loopback reachability --------------------------------------------------------- 0.30s\narista.avd.eos_validate_state : Validate lldp topology when there is a domain name ------------------------------------- 0.30s\nPlaybook run took 0 days, 0 hours, 0 minutes, 38 seconds\n</code></pre> </li> <li> <p>The play recap is great, but this role produces a report as a part of the run. This report details all tests that succeeded or failed.</p> </li> <li> <p>Before we dig into the reports, run the validation against <code>site2</code> as well.</p> <pre><code>make validate-site-2\n</code></pre> </li> </ol>"},{"location":"validation/avd_validate/#step-2-analyzing-results","title":"Step 2 - Analyzing Results","text":"<p>Ok, we have run validation, what just happened?! So remember:</p> <ol> <li>The AVD data models in this lab have described how you intend to configure this network, from physical connectivity to protocols used.</li> <li>AVD Validate state generated tests based on what it understands from an AVD build of the topology and configuration to generate tests</li> </ol> <p>YES, AVD just generated tests for you, lets take a look!</p> <ol> <li> <p>Open <code>sites/site_1/reports/SITE1_FABRIC-state.md</code> and take a look at what the first part of this report shows</p> Validation ReportValidation Report Failures <p></p> <p></p> </li> <li> <p>So first off note it generated over 300+ tests for a single 8 node site!</p> </li> <li>If you haven't already, click the \"Validation Report Failures\" tab above, we can note a few things here<ol> <li>Quickly we realize there are test failures in the Summary Totals</li> <li>In the device summary, we can quickly discern that we have failures on the <code>s1-brdr1</code> and both <code>spine</code> devices. The categories even give us hints as to what might be wrong here.</li> <li>Last we the tests grouped by category, verifying what we seeing failing in our environment: LLDP issues + BGP issues = IP Reachability </li> </ol> </li> <li>We could analyze more of this report, but first, let's run this report again using ANTA!</li> </ol>"},{"location":"validation/avd_validate/#step-3-avd-validate-state-anta","title":"Step 3 - AVD Validate State (ANTA)","text":"<p>Ok, we just ran AVD validate state, we have produced a report, and looks like we've identified an issue already. Validate state heavily relies on Ansible modules (like the assert module) and while it works great, it can prove challenging to maintain performance while trying to achieve more complex and larger scale testing.</p> <p>AVD 5.0.0+</p> <p>Moving forward, AVD 5.0.0 and beyond will use ANTA as it's default testing framework. ANTA is developed in python and gives AVD the ability to scale while maintaining performance as testing catalogs grow.</p> <p>Let's go ahead and enable ANTA and see it in action:</p> <ol> <li>Open your validation playbook <code>playbooks/validate.yml</code></li> <li> <p>Uncomment the <code>vars</code> below, we will enable ANTA and instruct the role to save the generated test catalogs</p> <pre><code>- name: validate states on EOS devices\n  ansible.builtin.import_role:\n    name: arista.avd.eos_validate_state\n  vars:\n    use_anta: true\n    save_catalog: true\n    eos_validate_state_md_report_path: \"{{ eos_validate_state_dir }}/{{ fabric_name }}-state-anta.md\"\n</code></pre> <p>ANTA Report</p> <p>We are saving the report to a new file, that way you can go back and review the differences between the two reports!</p> </li> <li> <p>Make sure the file is saved and run our validation again</p> <pre><code>make validate-site-1\n</code></pre> </li> <li> <p>Ok, you may have noticed this play was not nearly as verbose in output, and it ran much faster! These runs were done for the creation of this lab (as of AVD 4.10.0) ANTA was over 30 seconds faster.</p> <pre><code>Wednesday 04 September 2024  20:10:08 +0000 (0:00:00.168)       0:00:06.034 ***\n===============================================================================\neos_validate_state ------------------------------------------------------ 5.93s\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntotal ------------------------------------------------------------------- 5.93s\nWednesday 04 September 2024  20:10:08 +0000 (0:00:00.168)       0:00:06.034 ***\n===============================================================================\narista.avd.eos_validate_state : Run eos_validate_state_runner leveraging ANTA ------------------------------------------ 3.47s\narista.avd.eos_validate_state : Create required output directories if not present -------------------------------------- 1.21s\narista.avd.eos_validate_state : Include ANTA tasks --------------------------------------------------------------------- 0.67s\narista.avd.eos_validate_state : Include Ansible assert tests report tasks ---------------------------------------------- 0.17s\narista.avd.eos_validate_state : Include device intended structured configuration variables ----------------------------- 0.13s\narista.avd.eos_validate_state : Include Ansible assert tests ----------------------------------------------------------- 0.11s\narista.avd.eos_validate_state : Create validation reports from ANTA tests ---------------------------------------------- 0.09s\narista.avd.eos_validate_state : Verify Requirements -------------------------------------------------------------------- 0.08s\nPlaybook run took 0 days, 0 hours, 0 minutes, 6 seconds\n</code></pre> </li> <li> <p>Remember creating an ANTA test catalog in the ANTA lab, well we enabled <code>save_catalog</code>, AVD generated and saved that catalog for you to look at! It creates a catalog per device under the <code>sites/site_1/intended/test_catalogs/</code> directory.</p> <p>That's a large catalog!</p> <p>AVD generated over 2700 lines of test catalog configuration for you! If you look closely, many of those tests are specific peer to peer connectivity or neighborship tests.</p> </li> <li> <p>Let's open the new ANTA report, remember we changed the name for this report, it should be <code>sites/site_1/reports/SITE1_FABRIC-state-anta.md</code></p> Validation ReportValidation Report FailuresOld Validation Report Failures <p></p> <p></p> <p></p> </li> <li> <p>Click the \"Validation Report Failures\" tab above, we can see we get very similar summaries of the failures. We note we still have an issue with <code>s1-brdr1</code> and both <code>spine</code> devices. One key difference is the test categories, but it still points us to a BGP/Connectivity issue!</p> </li> <li>If you haven't already, you can compare the failure results of the two reports by clicking the \"Old Validation Report Failures\" above.</li> <li> <p>Let's review the section called \"Failed Test Results Summary,\" we get a list of all failed tests with a bit more verbosity     </p> </li> <li> <p>There seems to be three tests within the BGP and Connectivity ANTA test categories failing</p> <ol> <li><code>VerifyBGPSpecificPeers</code></li> <li><code>VerifyReachability</code></li> <li><code>VerifyLLDPNeighbors</code></li> </ol> </li> <li> <p>Let's start with layer 1 and look at this <code>VerifyLLDPNeighbors</code> test failure, lets focus on the Inputs and Messages         1.  Inputs (Intended): this is what AVD told the test framework what our fabric SHOULD look like, this is supposed to be source of truth         2.  Messages (Actual): what ANTA has told AVD ACTUALLY exists in the network by querying the device</p> </li> <li> <p>So that being said, looks like <code>s1-brdr1:Ethernet2</code> should be connected to <code>s1-spine1:Ethernet6</code>, but it detected it's actually detected <code>s1-spine1:Ethernet7</code>. Remember AVD would have configured <code>s1-spine1:Ethernet6</code> for peer connectivity, something isn't right.</p> </li> <li> <p>Looks like we have one of two scenarios         1. AVD Typo: We have a typo in what we consider our intended configuration in AVD         2. Cabling Issue: We've cabled the links between <code>s1-brdr1</code> and our <code>spine</code> devices wrong</p> </li> <li> <p>In a real-world scenario, this would have caught a use case of the links being transposed or a typo in the AVD repository! This is a virtual lab, so mistakes in cabling are most likely not the cause. Let's go fix AVD </p> </li> <li> <p>Under <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code> let's fix our border node <code>s1-brdr1</code> up link interfaces, update the <code>uplink_switch_inerfaces</code> key.</p> <pre><code>  nodes:\n    - name: s1-brdr1\n      id: 5\n      mgmt_ip: 192.168.0.100/24\n      uplink_switch_interfaces: [ Ethernet6, Ethernet6 ] #(1)!\n      evpn_gateway:\n        remote_peers:\n          - hostname: s2-brdr1\n            bgp_as: 65203\n            ip_address: 10.250.2.7\n</code></pre> <ol> <li>Update this to <code>Ethernet7</code>, we are going to trust the border is cabled correctly and we just made a mistake here.</li> <li>Ok, we're updated. We need to push this change into our environment now. Let's do that!</li> </ol> <pre><code>make build-site-1 deploy-site-1\n</code></pre> </li> <li> <p>Perfect, give it a second for things to converge and let's try to validate! We should expect our validation to verify this peering is healthy now that we've fixed the layer 1 mix up \ud83d\udc4d</p> <pre><code>make validate-site-1\n</code></pre> </li> <li> <p>CONGRATS! You've now validated the fabric, caught an issue, resolved it, and run over 1000 tests in this time with a click of a button (or several \ud83d\ude00)</p> </li> </ol>"},{"location":"validation/avd_validate/#step-4-day-2-validation","title":"Step 4 - Day 2 Validation","text":"<p>Well you've come a long way at this point, AVD is now generating all our tests for us and we can validate our fabric with a single playbook, A SINGLE TASK! That's great, we have confidence now, but we always want more testing! Let's dive into adding new tests to our environment and how AVD behaves with what we'll consider Day 2 Changes.</p>"},{"location":"validation/avd_validate/#day-2-dynamic-tests","title":"Day 2 - Dynamic Tests","text":"<p>The first scenario we'll address is simply adding a VLAN. Adding VLANs/SVIs across a fabric can be automated, as you've seen with AVD, but ensuring that all those leaf switches take the change and/or that VLAN is actually up is another story. Let's try this out:</p> <ol> <li>Let's navigate to <code>sites/site_1/group_vars/SITE1_NETWORK_SERVICES.yml</code></li> <li>Go ahead and uncomment VLAN 30; this will add the VLAN to the EVPN/VXLAN environment. That means each leaf should get this new VLAN/SVI</li> <li> <p>First, let's validate our network is still healthy; validation is great after a change, but it's just as important before a change!</p> <pre><code>make validate-site-1\n</code></pre> </li> <li> <p>Great, we shouldn't see any changes.</p> </li> <li> <p>Lets run our AVD build only and validate</p> <pre><code>make build-site-1 validate-site-1\n</code></pre> </li> <li> <p>Let's check our validation report <code>sites/site_1/reports/SITE1_FABRIC-state-anta.md</code></p> </li> <li> <p>There's a new test for VLAN 30! Yes, AVD generated a test for you based on your AVD change. Yes, it failed because we told our validation to make sure the network has VLAN 30 configured and up... well we haven't deployed yet!</p> <p></p> </li> <li> <p>Let's fix that, let's deploy the changes and re-run our validation</p> <pre><code>make deploy-site-1 validate-site-1\n</code></pre> </li> <li> <p>Awesome, you changed 4 lines of configuration and look at that testing at work for day 2 operations!</p> </li> </ol>"},{"location":"validation/avd_validate/#day-2-add-tests","title":"Day 2 - Add Tests","text":"<p>Ok, lets take a new scenario that we've got an ask from our enterprise monitoring and compliance teams to add a new logging server to our environment. Compliance would like us to prove all our switches are pointed to this logging server and are using the Management interface.</p> <p>Great, let's solve this:</p> <ol> <li>First, let's take our first action, add the new logging server to the fabric</li> <li>Open <code>sites/global_vars/global_dc_vars.yml</code> and uncomment the logging server. We are using global vars to enable the logging server globally, across both SITE1 and SITE2</li> <li> <p>Ok, like before, let's build and validate</p> <pre><code>make build-site-1 validate-site-1\n</code></pre> </li> <li> <p>Let's check our validation report <code>sites/site_1/reports/SITE1_FABRIC-state-anta.md</code></p> </li> <li> <p>Interesting, we don't see a new test like we did with Vlan 30. We would expect a failure based on the fact we defined a new logging server but haven't deployed it yet. Let's fix this</p> <p>AVD and ANTA Tests</p> <p>It's fair to have maybe expected a test and if you check the ANTA Documentation there is a number of logging tests available! AVD is always introducing new AVD tests that can be generated based on structured configuration. You can always search open feature requests (or open your own) on the AVD repository to see if it's been requested or staged for the next release!</p> <p>While it's not generated, we can still add it!</p> </li> <li> <p>Let's fix this lack of test, we're going to leverage an AVD construct to provide our own tests.</p> </li> <li>Navigate to <code>sites/site_1/custom_anta_catalogs/SITE1_FABRIC.yml</code> and let's look at the contents here</li> <li>This directory is where you can place files to add additional tests to your existing generated tests. The file names themselves behave the same way <code>group_vars</code> files behave; the file name maps to an ansible inventory group, and those tests in that file will be run against those devices. In this case, <code>SITE1_FABRIC.yml</code> is used, so every device in the fabric will get this test!</li> <li> <p>Uncomment the logging tests for now and let's re-run our validation</p> <p>No AVD Build?</p> <p>We haven't changed anything that changes configuration, simply adding tests, so no need to build again!</p> <pre><code>make validate-site-1\n</code></pre> </li> <li> <p>Ok, now we're seeing the failures, because we have not deployed the logging servers to our switches just yet. However, we know we have the test now.</p> <p></p> </li> <li> <p>Let's deploy and validate again!</p> <pre><code>make deploy-site-1 validate-site-1\n</code></pre> </li> <li> <p>Awesome, you should no longer see any failures, we deployed the change and our test validated our entire SITE1 fabric is configured correctly. We can hand this automation and report to prove we've configured logging servers correctly!</p> </li> </ol>"},{"location":"validation/avd_validate/#day-2-leaf-tests","title":"Day 2 - Leaf Tests","text":"<p>You've successfully added tests to prove global changes, but what if you want to target some tests for possible areas of interest. Management understand our spines are serving as route reflectors for our entire EVPN/VXLAN fabric and there is growing concern this could result in an outage. How might we address this?</p> <p>Well let's see what we can do to build some confidence we're not only monitoring this, but ensuring that before any change to the spines they are healthy.</p> <p>Monitoring should have caught this</p> <p>As we go through this, of course monitoring should catch the tests we're about to present. We are simply double checking nothing was missed and of course ANTA can do some unique checks that aren't always caught in logs or traps!</p> <ol> <li> <p>Before we begin, let's run our validation and check our report on the number of total tests per device</p> <pre><code>make validate-site-1\ncat sites/site_1/reports/SITE1_FABRIC-state-anta.md | grep -EA 12 \"Summary.*Under Test\"\n</code></pre> </li> <li> <p>Take note of the number of spine tests, we should see 3 additional tests during our next run</p> </li> <li>Let's navigate back to our custom ANTA catalog and open the <code>SITE1_SPINES.yml</code> file</li> <li>In there, we have several system tests we can uncomment</li> <li>We can add these to ONLY the spines to ensure their role as a route server hasn't caused any adverse effects. Critically we want to know before and after a change if anything changes here</li> <li> <p>Let's go ahead and run our new tests</p> <pre><code>make validate-site-1\ncat sites/site_1/reports/SITE1_FABRIC-state-anta.md | grep -EA 12 \"Summary.*Under Test\"\n</code></pre> </li> <li> <p>Great! You've added tests to only the spines; you may even see your tests catch the virtual environment running high on CPU and Memory! That's what Management wants to avoid \ud83d\ude04</p> </li> </ol>"},{"location":"validation/avd_validate/#step-5-avd-adjustments","title":"Step 5 - AVD Adjustments","text":"<p>Last thing we're going to do is explore some options available to us and documented within the AVD Validation Documentation.</p> <p>We are not going to go too deep into this topic, however it's important to note you can customize aspects of the validation framework, it can be molded to your operational workflows and needs.</p>"},{"location":"validation/avd_validate/#skip-tests","title":"Skip Tests","text":"<p>Let's explore a use case where you need to pull specific tests out of a report to deliver to Management. For example, they are uninterested in test results related to hardware. You can skip the AVD Test Categories using the <code>skip_tests</code> flag</p> <ol> <li>Open the playbook <code>playbooks/validate.yml</code></li> <li> <p>Replace your <code>vars</code> section with the following</p> <pre><code>  vars:\n    use_anta: true\n    save_catalog: true\n    eos_validate_state_md_report_path: \"{{ eos_validate_state_dir }}/{{ fabric_name }}-no-harwdare.md\" #(1)!\n    skip_tests:\n        - category: AvdTestHardware #(2)!\n</code></pre> <ol> <li>Save to a new unique report file</li> <li>Skip the AVD hardware test category</li> </ol> </li> <li> <p>This will skip all hardware tests!</p> </li> </ol>"},{"location":"validation/avd_validate/#test-tags","title":"Test Tags","text":"<p>Let's take the opposite and there is a recent project that has just finished a large cabling exercise. Your project manager wants evidence the network team has signed off on all the cabling. Well good news, you can run specific tests using the tags!</p> <p>We will actually just manipulate the way we run our validation playbook and add a tag at the end</p> <pre><code>ansible-playbook playbooks/validate.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\" --tags lldp_topology\n</code></pre> <p>This will produce a report of the LLDP topology as understood by AVD!</p> <p>Great Success</p> <p>Congratulations. You have now successfully completed the ANTA and AVD Network Testing without interacting with any switch CLI!</p>"},{"location":"validation/overview/","title":"Getting Started with Network Testing","text":""},{"location":"validation/overview/#introduction","title":"Introduction","text":"<p>Designing, developing, and deploying a network fabric through automation can be extremely powerful, but how do you ensure that the product of your work actually does what you designed it to do? In the software world, we write tests. Well, what tests do we write, and where should these tests be run?</p> <p>This workshop will leverage Arista's open-source testing framework, Arista Network Test Automation (ANTA), to build a network testing strategy.</p> <p>This section will cover the following:</p> <ul> <li>Software Testing</li> <li>Arista Network Test Automation (ANTA)</li> <li>Arista Validated Designs (AVD) Validate State</li> </ul> <p>Each attendee will receive a dedicated virtual lab environment with Git, VS Code, Ansible/AVD, and ANTA installed and ready to use.</p> <p>Attendees will need the following:</p> <ul> <li> A laptop</li> <li> An account on GitHub</li> <li> Familiarity with some of the concepts and tools covered in both the<ul> <li> Automation Fundamentals workshop (Git, VS Code, Jinja/YAML, Ansible)</li> <li> CI/AVD L2LS or CI/AVD L3LS EVPN/VXLAN workshops</li> </ul> </li> </ul>"},{"location":"validation/overview/#lab-topology","title":"Lab Topology","text":"<p>We will be testing against the same topology provided in CI/AVD L3LS EVPN/VXLAN. While this is specific to an L3LS, the ANTA testing framework concepts used throughout this lab applies to any fabric or Arista Validated Design.</p>"},{"location":"validation/overview/#software-testing","title":"Software Testing","text":"<p>When building, deploying, or operating a network, we consistently go through a degree of testing. Whether that testing is effective or scales with our network is debatable, but we try our best to test our networks. If you've walked through the automation fundamentals labs, you may have learned networks can be represented as code. For this upcoming lab, this code for all intents and purposes, is software and will be tested as such.</p> <p>We are not going to dive too deep into software testing frameworks, methodologies and techniques in this lab. However, it is important to get familiar with some of the software testing vocabulary. Our focus will be on the fundamental building blocks used in software testing: Unit, Integration, and System Testing.</p> <ul> <li> <p> Unit Testing</p> <p>When testing the most atomic components of a piece of software, think of it as units of code. Test software functions work as intended. For example, the <code>valid_ip_address()</code> function actually validates IP address formats and will throw clear messaging if it fails.</p> </li> <li> <p> Integration Testing</p> <p>Testing the different units of code together as a combined entity, the integration between these units can be validated. For example, all the units of code properly integrate together to generate a valid network configuration.</p> </li> <li> <p> System Testing</p> <p>Testing the software as whole, assessing the system's functionality and performance meets expectations. For example, this could be thought of as my software (automation) generates and deploys configuration that builds a working EVPN/VXLAN fabric!</p> </li> </ul> <p>Let's apply these concepts to our network and furthermore into our network automation. These workshops have focused on using software to define our network configuration using a combination of YAML files, Jinja templates, and Ansible. We can and should test these!</p> <p>Well good news, all these pieces of the puzzle have been wrapped into Arista Validated Designs (AVD) Ansible collection and it's all tested! The data models, templates, and logic to generate EOS configuration go through hundreds of unit and integration tests.</p> <p>So what does it mean that AVD has been tested?! At a high level, that means testing ensures:</p> <ul> <li>AVD <code>arista.avd.eos_designs</code> generates valid data models for our configuration generation</li> <li>AVD <code>arista.avd.eos_cli_config_gen</code> renders <code>eos_designs</code> data models to produce valid EOS configuration</li> <li>AVD filter plugins (like <code>arista.avd.range_expand</code>) is tested that both user input and resulting EOS interface ranges are valid</li> </ul> <p></p> <p>This suite of unit and integration tests result in the AVD Ansible Collection producing valid EOS configuration you can trust to readily deploy to your network!</p> <p>Testing is the MOST important thing</p> <p>If you have yet to hear it, you certainly will at some point. Testing Arista's Extensible Operating System (EOS) uses the same fundamental building blocks. Hundreds of thousands of tests run against EOS every day!</p>"},{"location":"validation/overview/#testing-the-network","title":"Testing the Network","text":"<p>Generating and applying configuration to the network is only half of the problem. We want to test that the physical (or virtual) network is actually behaving as expected. Automated testing frameworks are nothing new to the network industry. Tools like Batfish and Robot are two of the more popular open-source options used in the industry today.</p> <p>ANTA is similar, but catered to Arista EOS! All these testing frameworks approach the problem slightly differently, but all are trying to test whether a network is doing what it's intended to do. Let's explore some of the differences between these options.</p> <ul> <li> <p> ANTA</p> <p>Arista's open-source automation framework for acceptance testing, supported as a part of Arista Validated Designs (AVD). ANTA, similar to Robot, relies on a test library (or catalog) that instructs ANTA on what tests to run against what set of devices, what thresholds evaluate a pass/fail and deliver a report.</p> <p>Python/YAML</p> <p> Arista Network Test Automation</p> </li> <li> <p> Batfish</p> <p>An open-source configuration analysis tool entirely based on modeling network operating systems, it relies on predicting network state based on network configuration files before it is deployed to a network. It requires a Batfish server to run the modeling software and evaluate configuration files from that vendor.</p> <p>Python/Pandas Query</p> <p> Batfish</p> </li> <li> <p> Robot Framework</p> <p>Open-source automation testing framework for test automation and robotic process automation (RPA). Robot relies on a test library that includes a collection of files on what commands to run against what set of network devices, how to interpret those results for pass/fail, and deliver a report.</p> <p>Python/Robot Syntax</p> <p> Robot Framework</p> </li> </ul> <p>These testing frameworks offer solutions to the problem of testing network state, either through modeling or direct device interrogation. However, integration with any network operating system still needs work.</p>"},{"location":"validation/overview/#arista-network-testing-automation-anta","title":"Arista Network Testing Automation (ANTA)","text":"<p>Arista Network Test Automation (ANTA) is a Python framework that automates the testing of your network and natively understands how to interact and leverage the output of any Arista EOS device. In this section, we will explore the key components of ANTA and how to get started.</p> <p>As we walk through executing ANTA for the first time, there will be references back to ANTA documentation. If you would like to explore any of the topics deeper, visit anta.arista.com.</p>"},{"location":"validation/overview/#components","title":"Components","text":"<p>ANTA is easy to get started with because a lot of functionality is built into the framework upon installation. To begin, you only need to provide two YAML files: one describing the devices you want to test and another specifying the tests to run on those devices.</p> <ul> <li> <p> Inventory</p> <p>The minimum requirements for the ANTA inventory include the <code>host</code> and <code>name</code> of the device.</p> inventory.yml<pre><code>anta_inventory:\n  hosts:\n  - host: 192.168.0.10 #(1)!\n      name: s1-spine1  #(2)!\n\n  - host: 192.168.0.11\n      name: s1-spine2\n\n  - host: 192.168.0.12\n      name: s1-leaf1\n\n  - host: 192.168.0.13\n      name: s1-leaf2\n</code></pre> <ol> <li>IP address or hostname</li> <li>Recommended to use the device hostname, however this can be any string to represent the device</li> </ol> </li> <li> <p> Test Catalog</p> <p>Instruct ANTA to run a set of tests against a specific set of inventory using an ANTA Test Catalog.</p> tests.yml<pre><code>anta.tests.system:\n- VerifyUptime:\n    minimum: 86400 #(1)!\n\nanta.tests.software:\n- VerifyEOSVersion: #(2)!\n    versions:\n        - 4.31.3M\n\nanta.tests.mlag: #(3)!\n- VerifyMlagStatus:\n- VerifyMlagInterfaces:\n- VerifyMlagConfigSanity:\n</code></pre> <ol> <li>Has the device been up for over 24 hours?</li> <li>Verifies the device is running one of the allowed EOS versions.</li> <li>Validate MLAG state is healthy</li> </ol> </li> </ul>"},{"location":"validation/overview/#testing-catalog","title":"Testing Catalog","text":"<p>The ANTA test catalog has several built-in tests that understand Arista EOS. Pick the tests you want, add them to a file using familiar YAML syntax, and run them against your list of devices!</p> <p>While the framework gives you a lot upfront, these labs will explore the flexibility in altering the tests for your network, including how custom tests can be written. All these tests can be grouped into some high level classifications:</p> <ul> <li>Hardware: physical hardware state is reporting healthy within user-defined thresholds</li> <li>Software Compliance: software versions meet user criteria</li> <li>Configuration: specific features or functions are enabled or disabled</li> <li>Protocol: configuration results in features or functions are behaving as expected</li> </ul>"},{"location":"validation/overview/#network-ready-for-use-nrfu","title":"Network Ready For Use (NRFU)","text":"<p>Once both the ANTA inventory and test catalog have been defined, ANTA provides the command \"Network Ready For Use\" or <code>anta nrfu</code> to execute the defined tests against the inventory. That's it! This is all it takes to get started with running your first tests against the network.</p> <p></p> terminal<pre><code>anta nrfu -u admin -p arista123 -i inventory.yaml -c tests.yaml table\n</code></pre> <p></p> <p>The output of any ANTA test run is straight forward, the test is either a pass, fail, or skipped all together. That's what we want from any testing so we can address those failures.</p> <p>Success</p> <p>The test passed test criteria, this includes user defined thresholds if applicable.</p> <p>Skipped</p> <p>The test was skipped, because the platform may not support the feature (for example, vEOS does not contain physical temperature sensors)</p> <p>Failure</p> <p>The test failed the test criteria, this includes user defined thresholds if applicable.</p> <p>ANTA is simple in it's most basic form, but like anything there is much more to the framework. You will dive into some of the powerful tools built within ANTA!</p>"},{"location":"validation/overview/#avd-validate-state","title":"AVD Validate State","text":"<p>If you've completed any of the CI/AVD workshops leading up to this, you'll understand AVD at it's core is a set of data modeling that describe your network. This data model doesn't give us a lot of value until its transformed into something useful, like configuration files or documentation. Even more important, those configuration files need to make it onto our switches! So what else can we use the data models for? Really anything, but what if we could use these to generate the two ANTA components we need: an inventory and a catalog of tests.</p> <p>If you can describe your network as code (via AVD data models), why shouldn't you be able to drive network testing as code. Simple example, in AVD we define the ports used between the spines and each leaf, we're describing a physical topology.</p> SITE1_FABRIC.yml<pre><code>l3leaf: #(1)!\n  defaults:\n    platform: cEOS\n    uplink_switches: [ s1-spine1, s1-spine2 ] #(2)!\n    uplink_interfaces: [ Ethernet2, Ethernet3 ] #(3)!\n    ...\n  node_groups:\n    - group: S1_RACK1\n      bgp_as: 65101\n      nodes:\n        - name: s1-leaf1 #(4)!\n          id: 1\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ] #(5)!\n        - name: s1-leaf2\n          id: 2\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n</code></pre> <ol> <li>Everything we're configuring below is in context to our leaf switch</li> <li>Any of these leaf switches defined should up link to <code>s1-spine1</code> and <code>s1-spine2</code></li> <li>The leaf should use <code>Ethernet2</code> and <code>Ethernet3</code> to up link to these spines</li> <li>Defining our leaf <code>s1-leaf1</code></li> <li>Defining the destination port that should be used, in this case <code>Ethernet2</code> on <code>s1-spine1</code> and <code>s1-spine2</code></li> </ol> <p>Similar to the way AVD converts these data models to Arista EOS configuration, AVD has the ability to transform this data into an ANTA test catalog via the AVD Validate State role. We understand what we intend the network to looks like, we deploy our configurations to our network, now we want to test our actual network looks this way. Like you run the AVD EOS configuration build and generation, AVD provides the <code>eos_validate_state</code>!</p> playbooks/validate.yml<pre><code>---\n- name: Validate Network State\n  hosts: \"{{ target_hosts }}\" #(1)!\n  connection: httpapi\n  gather_facts: false\n\n  tasks:\n\n    - name: validate states on EOS devices\n      ansible.builtin.import_role:\n        name: arista.avd.eos_validate_state #(2)!\n      vars:\n        use_anta: true\n</code></pre> <ol> <li>Supply ANTA our inventory via our target hosts</li> <li>This role will convert the AVD data models into an ANTA test catalog</li> </ol> <p>We would prefer to not write tests, we'd rather focus our efforts to configure our network. Running the <code>eos_validate_state</code> role will write those tests for you, so you can focus on building your AVD configuration! Just as described in the AVD data model above, AVD has now converted our layer 1 connectivity into a</p> s1-leaf1s1-leaf2 s1-leaf1-catalog.yml<pre><code>anta.tests.connectivity:\n   - VerifyLLDPNeighbors:\n       neighbors:\n       - neighbor_device: s1-spine1.atd.lab\n       neighbor_port: Ethernet2\n       port: Ethernet2\n       result_overwrite:\n          custom_field: 'Local: Ethernet2 - Remote: s1-spine1 Ethernet2'\n   - VerifyLLDPNeighbors:\n       neighbors:\n       - neighbor_device: s1-spine2.atd.lab\n       neighbor_port: Ethernet2\n       port: Ethernet3\n       result_overwrite:\n          custom_field: 'Local: Ethernet3 - Remote: s1-spine2 Ethernet2'\n</code></pre> s1-leaf2-catalog.yml<pre><code>anta.tests.connectivity:\n   - VerifyLLDPNeighbors:\n       neighbors:\n       - neighbor_device: s1-spine1.atd.lab\n       neighbor_port: Ethernet3\n       port: Ethernet2\n       result_overwrite:\n          custom_field: 'Local: Ethernet2 - Remote: s1-spine1 Ethernet3'\n   - VerifyLLDPNeighbors:\n       neighbors:\n       - neighbor_device: s1-spine2.atd.lab\n       neighbor_port: Ethernet3\n       port: Ethernet3\n       result_overwrite:\n          custom_field: 'Local: Ethernet3 - Remote: s1-spine2 Ethernet3'\n</code></pre>"},{"location":"validation/overview/#start-the-labs","title":"Start the Labs","text":"<p>Hopefully you now know a bit more about how ANTA and AVD play a big role in the MOST important thing we commonly neglect, TESTING!! Don't stop here, get started with using ANTA in the lab and see it in action!</p> <p>Getting Started with ANTA</p>"}]}